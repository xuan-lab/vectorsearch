#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»é¢˜å»ºæ¨¡æ•™è‚²åº”ç”¨
Topic Modeling Educational Application

è¿™ä¸ªåº”ç”¨å±•ç¤ºäº†ä¸»é¢˜å»ºæ¨¡çš„å„ç§æŠ€æœ¯ï¼š
- LDA (Latent Dirichlet Allocation)
- NMF (Non-negative Matrix Factorization)
- LSA (Latent Semantic Analysis)
- ä¸»é¢˜å¯è§†åŒ–
- æ–‡æ¡£ä¸»é¢˜åˆ†æ

This application demonstrates various topic modeling techniques:
- LDA (Latent Dirichlet Allocation)
- NMF (Non-negative Matrix Factorization)
- LSA (Latent Semantic Analysis)
- Topic visualization
- Document topic analysis
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
import re
import time
from collections import defaultdict, Counter
import json
from wordcloud import WordCloud

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from src.text_vectorizer import TextVectorizer
from src.utils import load_documents

# å°è¯•å¯¼å…¥æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
    from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD
    from sklearn.metrics import coherence_score
    from sklearn.cluster import KMeans
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("æç¤º: å®‰è£…scikit-learnåº“ä»¥ä½¿ç”¨ä¸»é¢˜å»ºæ¨¡åŠŸèƒ½: pip install scikit-learn")

# å°è¯•å¯¼å…¥gensim
try:
    import gensim
    from gensim import corpora, models
    from gensim.models import LdaModel, HdpModel
    from gensim.models.coherencemodel import CoherenceModel
    HAS_GENSIM = True
except ImportError:
    HAS_GENSIM = False
    print("æç¤º: å®‰è£…gensimåº“ä»¥è·å¾—æ›´å¥½çš„ä¸»é¢˜å»ºæ¨¡: pip install gensim")

# å°è¯•å¯¼å…¥pyLDAvis
try:
    import pyLDAvis
    import pyLDAvis.gensim_models as gensimvis
    HAS_PYLDAVIS = True
except ImportError:
    HAS_PYLDAVIS = False
    print("æç¤º: å®‰è£…pyLDAvisåº“ä»¥è¿›è¡Œä¸»é¢˜å¯è§†åŒ–: pip install pyldavis")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class Topic:
    """ä¸»é¢˜æ•°æ®ç»“æ„"""
    id: int
    words: List[Tuple[str, float]]  # (è¯, æƒé‡)
    description: str = ""
    coherence_score: float = 0.0

@dataclass
class DocumentTopic:
    """æ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ"""
    document_id: int
    text: str
    topics: List[Tuple[int, float]]  # (ä¸»é¢˜ID, æ¦‚ç‡)
    dominant_topic: int
    dominant_probability: float

class LDATopicModeler:
    """LDAä¸»é¢˜å»ºæ¨¡å™¨"""
    
    def __init__(self, n_topics: int = 5, random_state: int = 42):
        if not HAS_SKLEARN:
            raise ImportError("éœ€è¦å®‰è£…scikit-learnåº“")
        
        self.n_topics = n_topics
        self.random_state = random_state
        self.vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 2), 
                                         stop_words='english')
        self.lda_model = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=random_state,
            max_iter=20
        )
        self.is_fitted = False
        self.feature_names = None
        self.doc_topic_dist = None
    
    def fit(self, documents: List[str]) -> List[Topic]:
        """è®­ç»ƒLDAæ¨¡å‹"""
        # å‘é‡åŒ–æ–‡æ¡£
        doc_term_matrix = self.vectorizer.fit_transform(documents)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        # è®­ç»ƒLDAæ¨¡å‹
        self.lda_model.fit(doc_term_matrix)
        self.doc_topic_dist = self.lda_model.transform(doc_term_matrix)
        self.is_fitted = True
        
        # æå–ä¸»é¢˜
        topics = self._extract_topics()
        
        print(f"LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯†åˆ«å‡º {len(topics)} ä¸ªä¸»é¢˜")
        return topics
    
    def _extract_topics(self) -> List[Topic]:
        """æå–ä¸»é¢˜"""
        topics = []
        
        for topic_idx, topic in enumerate(self.lda_model.components_):
            # è·å–ä¸»é¢˜ä¸­æƒé‡æœ€é«˜çš„è¯
            top_word_indices = topic.argsort()[-10:][::-1]
            topic_words = [(self.feature_names[i], topic[i]) for i in top_word_indices]
            
            # ç”Ÿæˆä¸»é¢˜æè¿°
            top_words = [word for word, _ in topic_words[:3]]
            description = f"ä¸»é¢˜_{topic_idx+1}: {', '.join(top_words)}"
            
            topics.append(Topic(
                id=topic_idx,
                words=topic_words,
                description=description
            ))
        
        return topics
    
    def get_document_topics(self, documents: List[str]) -> List[DocumentTopic]:
        """è·å–æ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        doc_topics = []
        
        for doc_idx, (doc, topic_dist) in enumerate(zip(documents, self.doc_topic_dist)):
            # è·å–ä¸»é¢˜åˆ†å¸ƒ
            topics = [(i, prob) for i, prob in enumerate(topic_dist)]
            topics.sort(key=lambda x: x[1], reverse=True)
            
            # ä¸»å¯¼ä¸»é¢˜
            dominant_topic = topics[0][0]
            dominant_prob = topics[0][1]
            
            doc_topics.append(DocumentTopic(
                document_id=doc_idx,
                text=doc,
                topics=topics,
                dominant_topic=dominant_topic,
                dominant_probability=dominant_prob
            ))
        
        return doc_topics

class NMFTopicModeler:
    """NMFä¸»é¢˜å»ºæ¨¡å™¨"""
    
    def __init__(self, n_topics: int = 5, random_state: int = 42):
        if not HAS_SKLEARN:
            raise ImportError("éœ€è¦å®‰è£…scikit-learnåº“")
        
        self.n_topics = n_topics
        self.random_state = random_state
        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.nmf_model = NMF(n_components=n_topics, random_state=random_state)
        self.is_fitted = False
        self.feature_names = None
        self.doc_topic_dist = None
    
    def fit(self, documents: List[str]) -> List[Topic]:
        """è®­ç»ƒNMFæ¨¡å‹"""
        # å‘é‡åŒ–æ–‡æ¡£
        doc_term_matrix = self.vectorizer.fit_transform(documents)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        # è®­ç»ƒNMFæ¨¡å‹
        self.doc_topic_dist = self.nmf_model.fit_transform(doc_term_matrix)
        self.is_fitted = True
        
        # æå–ä¸»é¢˜
        topics = self._extract_topics()
        
        print(f"NMFæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯†åˆ«å‡º {len(topics)} ä¸ªä¸»é¢˜")
        return topics
    
    def _extract_topics(self) -> List[Topic]:
        """æå–ä¸»é¢˜"""
        topics = []
        
        for topic_idx, topic in enumerate(self.nmf_model.components_):
            # è·å–ä¸»é¢˜ä¸­æƒé‡æœ€é«˜çš„è¯
            top_word_indices = topic.argsort()[-10:][::-1]
            topic_words = [(self.feature_names[i], topic[i]) for i in top_word_indices]
            
            # ç”Ÿæˆä¸»é¢˜æè¿°
            top_words = [word for word, _ in topic_words[:3]]
            description = f"ä¸»é¢˜_{topic_idx+1}: {', '.join(top_words)}"
            
            topics.append(Topic(
                id=topic_idx,
                words=topic_words,
                description=description
            ))
        
        return topics
    
    def get_document_topics(self, documents: List[str]) -> List[DocumentTopic]:
        """è·å–æ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        doc_topics = []
        
        for doc_idx, (doc, topic_dist) in enumerate(zip(documents, self.doc_topic_dist)):
            # æ ‡å‡†åŒ–ä¸»é¢˜åˆ†å¸ƒ
            total = np.sum(topic_dist)
            if total > 0:
                topic_dist = topic_dist / total
            
            # è·å–ä¸»é¢˜åˆ†å¸ƒ
            topics = [(i, prob) for i, prob in enumerate(topic_dist)]
            topics.sort(key=lambda x: x[1], reverse=True)
            
            # ä¸»å¯¼ä¸»é¢˜
            dominant_topic = topics[0][0]
            dominant_prob = topics[0][1]
            
            doc_topics.append(DocumentTopic(
                document_id=doc_idx,
                text=doc,
                topics=topics,
                dominant_topic=dominant_topic,
                dominant_probability=dominant_prob
            ))
        
        return doc_topics

class LSATopicModeler:
    """LSAä¸»é¢˜å»ºæ¨¡å™¨"""
    
    def __init__(self, n_topics: int = 5, random_state: int = 42):
        if not HAS_SKLEARN:
            raise ImportError("éœ€è¦å®‰è£…scikit-learnåº“")
        
        self.n_topics = n_topics
        self.random_state = random_state
        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.lsa_model = TruncatedSVD(n_components=n_topics, random_state=random_state)
        self.is_fitted = False
        self.feature_names = None
        self.doc_topic_dist = None
    
    def fit(self, documents: List[str]) -> List[Topic]:
        """è®­ç»ƒLSAæ¨¡å‹"""
        # å‘é‡åŒ–æ–‡æ¡£
        doc_term_matrix = self.vectorizer.fit_transform(documents)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        # è®­ç»ƒLSAæ¨¡å‹
        self.doc_topic_dist = self.lsa_model.fit_transform(doc_term_matrix)
        self.is_fitted = True
        
        # æå–ä¸»é¢˜
        topics = self._extract_topics()
        
        print(f"LSAæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯†åˆ«å‡º {len(topics)} ä¸ªä¸»é¢˜")
        return topics
    
    def _extract_topics(self) -> List[Topic]:
        """æå–ä¸»é¢˜"""
        topics = []
        
        for topic_idx, topic in enumerate(self.lsa_model.components_):
            # è·å–ä¸»é¢˜ä¸­æƒé‡æœ€é«˜çš„è¯ï¼ˆæ³¨æ„LSAå¯èƒ½æœ‰è´Ÿå€¼ï¼‰
            top_word_indices = np.argsort(np.abs(topic))[-10:][::-1]
            topic_words = [(self.feature_names[i], abs(topic[i])) for i in top_word_indices]
            
            # ç”Ÿæˆä¸»é¢˜æè¿°
            top_words = [word for word, _ in topic_words[:3]]
            description = f"ä¸»é¢˜_{topic_idx+1}: {', '.join(top_words)}"
            
            topics.append(Topic(
                id=topic_idx,
                words=topic_words,
                description=description
            ))
        
        return topics
    
    def get_document_topics(self, documents: List[str]) -> List[DocumentTopic]:
        """è·å–æ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        doc_topics = []
        
        for doc_idx, (doc, topic_dist) in enumerate(zip(documents, self.doc_topic_dist)):
            # LSAå¯èƒ½äº§ç”Ÿè´Ÿå€¼ï¼Œå–ç»å¯¹å€¼å¹¶æ ‡å‡†åŒ–
            topic_dist = np.abs(topic_dist)
            total = np.sum(topic_dist)
            if total > 0:
                topic_dist = topic_dist / total
            
            # è·å–ä¸»é¢˜åˆ†å¸ƒ
            topics = [(i, prob) for i, prob in enumerate(topic_dist)]
            topics.sort(key=lambda x: x[1], reverse=True)
            
            # ä¸»å¯¼ä¸»é¢˜
            dominant_topic = topics[0][0]
            dominant_prob = topics[0][1]
            
            doc_topics.append(DocumentTopic(
                document_id=doc_idx,
                text=doc,
                topics=topics,
                dominant_topic=dominant_topic,
                dominant_probability=dominant_prob
            ))
        
        return doc_topics

class TopicModelingApp:
    """ä¸»é¢˜å»ºæ¨¡æ•™è‚²åº”ç”¨"""
    
    def __init__(self):
        self.modelers = {}
        self.documents = []
        self.topics = {}
        self.document_topics = {}
        
        # åˆå§‹åŒ–ä¸»é¢˜å»ºæ¨¡å™¨
        if HAS_SKLEARN:
            self.modelers['lda'] = LDATopicModeler()
            self.modelers['nmf'] = NMFTopicModeler()
            self.modelers['lsa'] = LSATopicModeler()
    
    def load_documents(self, documents: List[str] = None):
        """åŠ è½½æ–‡æ¡£"""
        if documents is None:
            # ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
            documents = self._create_sample_documents()
        
        self.documents = documents
        print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    def _create_sample_documents(self) -> List[str]:
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        return [
            "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†é‡å¤§çªç ´ã€‚",
            "è¶³çƒæ˜¯ä¸–ç•Œä¸Šæœ€å—æ¬¢è¿çš„è¿åŠ¨ä¹‹ä¸€ã€‚ä¸–ç•Œæ¯æ¯å››å¹´ä¸¾åŠä¸€æ¬¡ï¼Œå¸å¼•äº†å…¨çƒæ•°åäº¿è§‚ä¼—è§‚çœ‹ã€‚",
            "ç”µå½±å·¥ä¸šä¸æ–­å‘å±•ï¼Œç‰¹æ•ˆæŠ€æœ¯è¶Šæ¥è¶Šå…ˆè¿›ã€‚å¥½è±åå¤§ç‰‡åœ¨å…¨çƒç¥¨æˆ¿å¸‚åœºå æ®é‡è¦åœ°ä½ã€‚",
            "æ–°å† ç–«æƒ…å¯¹å…¨çƒç»æµäº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚å„å›½æ”¿åºœé‡‡å–äº†ä¸åŒçš„åº”å¯¹æªæ–½æ¥æ§åˆ¶ç—…æ¯’ä¼ æ’­ã€‚",
            "äº‘è®¡ç®—æŠ€æœ¯ä¸ºä¼ä¸šæä¾›äº†çµæ´»çš„ITè§£å†³æ–¹æ¡ˆã€‚äºšé©¬é€Šã€å¾®è½¯å’Œè°·æ­Œæ˜¯äº‘æœåŠ¡çš„ä¸»è¦æä¾›å•†ã€‚",
            "ç¯å¢ƒä¿æŠ¤æ˜¯å½“ä»Šä¸–ç•Œé¢ä¸´çš„é‡è¦æŒ‘æˆ˜ã€‚æ°”å€™å˜åŒ–å’Œæ±¡æŸ“é—®é¢˜éœ€è¦å…¨çƒåˆä½œæ¥è§£å†³ã€‚",
            "æ•™è‚²æŠ€æœ¯çš„å‘å±•ä½¿åœ¨çº¿å­¦ä¹ æˆä¸ºå¯èƒ½ã€‚ç–«æƒ…æœŸé—´ï¼Œè¿œç¨‹æ•™è‚²å¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚",
            "ç”µå­å•†åŠ¡æ”¹å˜äº†äººä»¬çš„è´­ç‰©æ–¹å¼ã€‚ç§»åŠ¨æ”¯ä»˜å’Œç‰©æµé…é€ç³»ç»Ÿçš„å®Œå–„æ¨åŠ¨äº†åœ¨çº¿è´­ç‰©çš„æ™®åŠã€‚",
            "åŒ»ç–—æŠ€æœ¯çš„è¿›æ­¥æé«˜äº†ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—çš„å‡†ç¡®æ€§ã€‚åŸºå› æ²»ç–—å’Œç²¾å‡†åŒ»å­¦ä¸ºæ‚£è€…å¸¦æ¥äº†æ–°å¸Œæœ›ã€‚",
            "æ•°å­—è´§å¸å’ŒåŒºå—é“¾æŠ€æœ¯æ­£åœ¨é‡å¡‘é‡‘èè¡Œä¸šã€‚æ¯”ç‰¹å¸å’Œå…¶ä»–åŠ å¯†è´§å¸çš„ä»·å€¼æ³¢åŠ¨å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚"
        ]
    
    def train_models(self, n_topics: int = 5):
        """è®­ç»ƒæ‰€æœ‰ä¸»é¢˜å»ºæ¨¡å™¨"""
        if not self.documents:
            self.load_documents()
        
        print(f"\nå¼€å§‹è®­ç»ƒä¸»é¢˜æ¨¡å‹ï¼Œä¸»é¢˜æ•°é‡: {n_topics}")
        
        for name, modeler in self.modelers.items():
            try:
                print(f"\næ­£åœ¨è®­ç»ƒ {name.upper()} æ¨¡å‹...")
                modeler.n_topics = n_topics
                if hasattr(modeler, 'lda_model'):
                    modeler.lda_model.n_components = n_topics
                elif hasattr(modeler, 'nmf_model'):
                    modeler.nmf_model.n_components = n_topics
                elif hasattr(modeler, 'lsa_model'):
                    modeler.lsa_model.n_components = n_topics
                
                topics = modeler.fit(self.documents)
                self.topics[name] = topics
                
                doc_topics = modeler.get_document_topics(self.documents)
                self.document_topics[name] = doc_topics
                
            except Exception as e:
                print(f"è®­ç»ƒ {name} æ¨¡å‹å¤±è´¥: {e}")
    
    def display_topics(self, method: str = 'lda', n_words: int = 10):
        """æ˜¾ç¤ºä¸»é¢˜"""
        if method not in self.topics:
            print(f"æ–¹æ³• {method} çš„ä¸»é¢˜å°šæœªç”Ÿæˆ")
            return
        
        topics = self.topics[method]
        
        print(f"\n{method.upper()} ä¸»é¢˜åˆ†æç»“æœ:")
        print("=" * 60)
        
        for topic in topics:
            print(f"\n{topic.description}")
            print("å…³é”®è¯:")
            for word, weight in topic.words[:n_words]:
                print(f"  {word}: {weight:.4f}")
    
    def compare_methods(self):
        """æ¯”è¾ƒä¸åŒæ–¹æ³•çš„ä¸»é¢˜"""
        if not self.topics:
            print("æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹")
            return
        
        print("\nä¸»é¢˜å»ºæ¨¡æ–¹æ³•æ¯”è¾ƒ:")
        print("=" * 80)
        
        for method, topics in self.topics.items():
            print(f"\n{method.upper()} æ–¹æ³•è¯†åˆ«çš„ä¸»é¢˜:")
            for topic in topics:
                top_words = [word for word, _ in topic.words[:5]]
                print(f"  ä¸»é¢˜ {topic.id + 1}: {', '.join(top_words)}")
    
    def analyze_document_topics(self, method: str = 'lda', doc_index: int = None):
        """åˆ†ææ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ"""
        if method not in self.document_topics:
            print(f"æ–¹æ³• {method} çš„æ–‡æ¡£ä¸»é¢˜å°šæœªç”Ÿæˆ")
            return
        
        doc_topics = self.document_topics[method]
        
        if doc_index is not None:
            # åˆ†æç‰¹å®šæ–‡æ¡£
            if 0 <= doc_index < len(doc_topics):
                doc_topic = doc_topics[doc_index]
                print(f"\næ–‡æ¡£ {doc_index + 1}: {doc_topic.text[:100]}...")
                print(f"ä¸»å¯¼ä¸»é¢˜: ä¸»é¢˜ {doc_topic.dominant_topic + 1} (æ¦‚ç‡: {doc_topic.dominant_probability:.3f})")
                print("ä¸»é¢˜åˆ†å¸ƒ:")
                for topic_id, prob in doc_topic.topics:
                    if prob > 0.05:  # åªæ˜¾ç¤ºæ¦‚ç‡å¤§äº5%çš„ä¸»é¢˜
                        print(f"  ä¸»é¢˜ {topic_id + 1}: {prob:.3f}")
            else:
                print("æ–‡æ¡£ç´¢å¼•è¶…å‡ºèŒƒå›´")
        else:
            # åˆ†ææ‰€æœ‰æ–‡æ¡£
            print(f"\n{method.upper()} æ–‡æ¡£ä¸»é¢˜åˆ†æ:")
            print("=" * 60)
            
            for doc_topic in doc_topics:
                print(f"æ–‡æ¡£ {doc_topic.document_id + 1}: ä¸»å¯¼ä¸»é¢˜ {doc_topic.dominant_topic + 1} "
                      f"(æ¦‚ç‡: {doc_topic.dominant_probability:.3f})")
    
    def visualize_topics(self, method: str = 'lda'):
        """å¯è§†åŒ–ä¸»é¢˜"""
        if method not in self.topics or method not in self.document_topics:
            print(f"æ–¹æ³• {method} çš„ç»“æœå°šæœªç”Ÿæˆ")
            return
        
        topics = self.topics[method]
        doc_topics = self.document_topics[method]
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ä¸»é¢˜å…³é”®è¯æƒé‡
        n_topics_to_show = min(len(topics), 4)
        for i in range(n_topics_to_show):
            row = i // 2
            col = i % 2
            
            topic = topics[i]
            words = [word for word, _ in topic.words[:10]]
            weights = [weight for _, weight in topic.words[:10]]
            
            if row < 2 and col < 2:
                axes[row, col].barh(range(len(words)), weights)
                axes[row, col].set_yticks(range(len(words)))
                axes[row, col].set_yticklabels(words)
                axes[row, col].set_title(f'ä¸»é¢˜ {i + 1} å…³é”®è¯')
                axes[row, col].set_xlabel('æƒé‡')
        
        # å¦‚æœä¸»é¢˜æ•°å°‘äº4ä¸ªï¼Œéšè—å¤šä½™çš„å­å›¾
        for i in range(n_topics_to_show, 4):
            row = i // 2
            col = i % 2
            axes[row, col].set_visible(False)
        
        plt.tight_layout()
        plt.show()
        
        # 2. æ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ
        if len(doc_topics) > 0:
            # ç»Ÿè®¡æ¯ä¸ªä¸»é¢˜çš„æ–‡æ¡£æ•°é‡
            topic_doc_counts = Counter()
            for doc_topic in doc_topics:
                topic_doc_counts[doc_topic.dominant_topic] += 1
            
            # ç»˜åˆ¶ä¸»é¢˜åˆ†å¸ƒ
            plt.figure(figsize=(10, 6))
            topics_ids = list(range(len(topics)))
            counts = [topic_doc_counts.get(i, 0) for i in topics_ids]
            
            plt.bar(topics_ids, counts)
            plt.xlabel('ä¸»é¢˜ID')
            plt.ylabel('æ–‡æ¡£æ•°é‡')
            plt.title(f'{method.upper()} ä¸»é¢˜æ–‡æ¡£åˆ†å¸ƒ')
            plt.xticks(topics_ids, [f'ä¸»é¢˜{i+1}' for i in topics_ids])
            plt.show()
    
    def create_topic_wordclouds(self, method: str = 'lda'):
        """åˆ›å»ºä¸»é¢˜è¯äº‘"""
        if method not in self.topics:
            print(f"æ–¹æ³• {method} çš„ä¸»é¢˜å°šæœªç”Ÿæˆ")
            return
        
        topics = self.topics[method]
        
        n_topics = len(topics)
        cols = min(3, n_topics)
        rows = (n_topics + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
        if rows == 1:
            axes = [axes] if n_topics == 1 else axes
        else:
            axes = axes.flatten()
        
        for i, topic in enumerate(topics):
            # å‡†å¤‡è¯é¢‘æ•°æ®
            word_freq = {word: weight for word, weight in topic.words[:20]}
            
            if word_freq:
                # åˆ›å»ºè¯äº‘
                wordcloud = WordCloud(
                    width=400, height=300,
                    background_color='white',
                    font_path='simhei.ttf',  # æ”¯æŒä¸­æ–‡
                    max_words=50
                ).generate_from_frequencies(word_freq)
                
                if n_topics == 1:
                    axes.imshow(wordcloud, interpolation='bilinear')
                    axes.set_title(f'ä¸»é¢˜ {i + 1} è¯äº‘')
                    axes.axis('off')
                else:
                    axes[i].imshow(wordcloud, interpolation='bilinear')
                    axes[i].set_title(f'ä¸»é¢˜ {i + 1} è¯äº‘')
                    axes[i].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for j in range(len(topics), len(axes)):
            axes[j].set_visible(False)
        
        plt.tight_layout()
        plt.show()
    
    def find_optimal_topics(self, max_topics: int = 10, method: str = 'lda'):
        """å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡"""
        if not self.documents:
            self.load_documents()
        
        print(f"\nå¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°é‡ (æ–¹æ³•: {method})...")
        
        coherence_scores = []
        perplexity_scores = []
        topic_numbers = range(2, max_topics + 1)
        
        for n_topics in topic_numbers:
            try:
                if method == 'lda':
                    modeler = LDATopicModeler(n_topics=n_topics)
                elif method == 'nmf':
                    modeler = NMFTopicModeler(n_topics=n_topics)
                elif method == 'lsa':
                    modeler = LSATopicModeler(n_topics=n_topics)
                else:
                    print(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
                    return
                
                topics = modeler.fit(self.documents)
                
                # è®¡ç®—å›°æƒ‘åº¦ï¼ˆä»…å¯¹LDAï¼‰
                if method == 'lda' and hasattr(modeler.lda_model, 'perplexity'):
                    doc_term_matrix = modeler.vectorizer.transform(self.documents)
                    perplexity = modeler.lda_model.perplexity(doc_term_matrix)
                    perplexity_scores.append(perplexity)
                else:
                    perplexity_scores.append(0)
                
                # ç®€å•çš„ä¸€è‡´æ€§è¯„åˆ†ï¼ˆåŸºäºä¸»é¢˜å†…è¯è¯­ç›¸å…³æ€§ï¼‰
                coherence = self._calculate_simple_coherence(topics)
                coherence_scores.append(coherence)
                
                print(f"ä¸»é¢˜æ•° {n_topics}: ä¸€è‡´æ€§ {coherence:.3f}")
                
            except Exception as e:
                print(f"è¯„ä¼°ä¸»é¢˜æ•° {n_topics} å¤±è´¥: {e}")
                coherence_scores.append(0)
                perplexity_scores.append(0)
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.plot(topic_numbers, coherence_scores, 'bo-')
        plt.xlabel('ä¸»é¢˜æ•°é‡')
        plt.ylabel('ä¸€è‡´æ€§åˆ†æ•°')
        plt.title('ä¸»é¢˜ä¸€è‡´æ€§ vs ä¸»é¢˜æ•°é‡')
        plt.grid(True)
        
        if method == 'lda' and any(score > 0 for score in perplexity_scores):
            plt.subplot(1, 2, 2)
            valid_perplexity = [(n, p) for n, p in zip(topic_numbers, perplexity_scores) if p > 0]
            if valid_perplexity:
                numbers, scores = zip(*valid_perplexity)
                plt.plot(numbers, scores, 'ro-')
                plt.xlabel('ä¸»é¢˜æ•°é‡')
                plt.ylabel('å›°æƒ‘åº¦')
                plt.title('å›°æƒ‘åº¦ vs ä¸»é¢˜æ•°é‡')
                plt.grid(True)
        
        plt.tight_layout()
        plt.show()
        
        # æ¨èæœ€ä¼˜ä¸»é¢˜æ•°
        if coherence_scores:
            optimal_topics = topic_numbers[np.argmax(coherence_scores)]
            print(f"\næ¨èçš„ä¸»é¢˜æ•°é‡: {optimal_topics} (ä¸€è‡´æ€§åˆ†æ•°: {max(coherence_scores):.3f})")
    
    def _calculate_simple_coherence(self, topics: List[Topic]) -> float:
        """è®¡ç®—ç®€å•çš„ä¸»é¢˜ä¸€è‡´æ€§"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ä¸€è‡´æ€§è®¡ç®—
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•
        total_coherence = 0
        
        for topic in topics:
            # è®¡ç®—ä¸»é¢˜å†…è¯è¯­æƒé‡çš„åˆ†å¸ƒå‡åŒ€æ€§
            weights = [weight for _, weight in topic.words[:10]]
            if len(weights) > 1:
                # ä½¿ç”¨æƒé‡çš„æ ‡å‡†å·®çš„å€’æ•°ä½œä¸ºä¸€è‡´æ€§åº¦é‡
                coherence = 1.0 / (np.std(weights) + 1e-6)
                total_coherence += coherence
        
        return total_coherence / len(topics) if topics else 0
    
    def run_interactive_demo(self):
        """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
        print("\nğŸ“Š ä¸»é¢˜å»ºæ¨¡æ•™è‚²åº”ç”¨")
        print("=" * 50)
        print("å¯ç”¨çš„ä¸»é¢˜å»ºæ¨¡æ–¹æ³•:")
        for i, method in enumerate(self.modelers.keys(), 1):
            print(f"  {i}. {method.upper()}")
        
        while True:
            print("\né€‰æ‹©æ“ä½œ:")
            print("1. åŠ è½½æ–‡æ¡£")
            print("2. è®­ç»ƒä¸»é¢˜æ¨¡å‹")
            print("3. æ˜¾ç¤ºä¸»é¢˜")
            print("4. æ¯”è¾ƒæ–¹æ³•")
            print("5. åˆ†ææ–‡æ¡£ä¸»é¢˜")
            print("6. å¯è§†åŒ–ä¸»é¢˜")
            print("7. åˆ›å»ºè¯äº‘")
            print("8. å¯»æ‰¾æœ€ä¼˜ä¸»é¢˜æ•°")
            print("9. åŠ è½½è‡ªå®šä¹‰æ–‡æ¡£")
            print("0. é€€å‡º")
            
            choice = input("\nè¯·é€‰æ‹© (0-9): ").strip()
            
            if choice == '0':
                break
            
            elif choice == '1':
                self.load_documents()
            
            elif choice == '2':
                try:
                    n_topics = int(input("è¯·è¾“å…¥ä¸»é¢˜æ•°é‡ (é»˜è®¤5): ").strip() or "5")
                    self.train_models(n_topics)
                except ValueError:
                    print("æ— æ•ˆçš„ä¸»é¢˜æ•°é‡")
            
            elif choice == '3':
                method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.modelers.keys())}): ").strip()
                if method in self.topics:
                    n_words = int(input("æ˜¾ç¤ºå¤šå°‘ä¸ªå…³é”®è¯ (é»˜è®¤10): ").strip() or "10")
                    self.display_topics(method, n_words)
                else:
                    print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–é€‰æ‹©æœ‰æ•ˆæ–¹æ³•")
            
            elif choice == '4':
                self.compare_methods()
            
            elif choice == '5':
                method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.modelers.keys())}): ").strip()
                if method in self.document_topics:
                    doc_idx_str = input("è¾“å…¥æ–‡æ¡£ç´¢å¼• (ç•™ç©ºåˆ†ææ‰€æœ‰æ–‡æ¡£): ").strip()
                    doc_idx = int(doc_idx_str) - 1 if doc_idx_str else None
                    self.analyze_document_topics(method, doc_idx)
                else:
                    print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–é€‰æ‹©æœ‰æ•ˆæ–¹æ³•")
            
            elif choice == '6':
                method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.modelers.keys())}): ").strip()
                if method in self.topics:
                    self.visualize_topics(method)
                else:
                    print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–é€‰æ‹©æœ‰æ•ˆæ–¹æ³•")
            
            elif choice == '7':
                method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.modelers.keys())}): ").strip()
                if method in self.topics:
                    try:
                        self.create_topic_wordclouds(method)
                    except Exception as e:
                        print(f"åˆ›å»ºè¯äº‘å¤±è´¥: {e}")
                        print("æç¤º: è¯·ç¡®ä¿å®‰è£…äº†wordcloudåº“: pip install wordcloud")
                else:
                    print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–é€‰æ‹©æœ‰æ•ˆæ–¹æ³•")
            
            elif choice == '8':
                method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.modelers.keys())}): ").strip()
                if method in self.modelers:
                    max_topics = int(input("æœ€å¤§ä¸»é¢˜æ•° (é»˜è®¤10): ").strip() or "10")
                    self.find_optimal_topics(max_topics, method)
                else:
                    print("æ— æ•ˆçš„æ–¹æ³•")
            
            elif choice == '9':
                try:
                    file_path = input("è¯·è¾“å…¥æ–‡æ¡£æ–‡ä»¶è·¯å¾„ (TXTæˆ–JSON): ").strip()
                    if os.path.exists(file_path):
                        if file_path.endswith('.json'):
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                            
                            if isinstance(data, list) and len(data) > 0:
                                if isinstance(data[0], dict) and 'content' in data[0]:
                                    documents = [item['content'] for item in data]
                                elif isinstance(data[0], str):
                                    documents = data
                                else:
                                    print("JSONæ ¼å¼é”™è¯¯")
                                    continue
                            else:
                                print("JSONæ•°æ®ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                                continue
                        
                        else:  # TXTæ–‡ä»¶
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            # æŒ‰æ®µè½åˆ†å‰²
                            documents = [para.strip() for para in content.split('\n\n') if para.strip()]
                        
                        self.load_documents(documents)
                    else:
                        print("æ–‡ä»¶ä¸å­˜åœ¨")
                except Exception as e:
                    print(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("åˆå§‹åŒ–ä¸»é¢˜å»ºæ¨¡åº”ç”¨...")
    
    app = TopicModelingApp()
    
    # åŠ è½½ç¤ºä¾‹æ–‡æ¡£
    app.load_documents()
    
    # è®­ç»ƒæ¨¡å‹
    print("\nè®­ç»ƒä¸»é¢˜æ¨¡å‹...")
    app.train_models(n_topics=3)
    
    print("\nğŸ¯ æ¼”ç¤º: ä¸»é¢˜å»ºæ¨¡åˆ†æ")
    print("=" * 40)
    
    # æ¯”è¾ƒä¸åŒæ–¹æ³•
    app.compare_methods()
    
    # æ˜¾ç¤ºä¸»é¢˜è¯¦æƒ…
    for method in app.modelers.keys():
        if method in app.topics:
            app.display_topics(method, n_words=5)
    
    # å¯è§†åŒ–ä¸»é¢˜
    print("\nğŸ“Š ä¸»é¢˜å¯è§†åŒ–")
    for method in app.modelers.keys():
        if method in app.topics:
            print(f"\n{method.upper()} æ–¹æ³•å¯è§†åŒ–:")
            app.visualize_topics(method)
    
    # è¿è¡Œäº¤äº’å¼æ¼”ç¤º
    app.run_interactive_demo()

if __name__ == "__main__":
    main()
