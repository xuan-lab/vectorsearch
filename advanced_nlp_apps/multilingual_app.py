#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šè¯­è¨€æ–‡æœ¬å¤„ç†æ•™è‚²åº”ç”¨
Multilingual Text Processing Educational Application

è¿™ä¸ªåº”ç”¨å±•ç¤ºäº†å¤šè¯­è¨€æ–‡æœ¬å¤„ç†çš„å„ç§æŠ€æœ¯ï¼š
- è¯­è¨€æ£€æµ‹
- å¤šè¯­è¨€æ–‡æœ¬åˆ†æ
- è·¨è¯­è¨€ç›¸ä¼¼æ€§è®¡ç®—
- æœºå™¨ç¿»è¯‘
- å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ

This application demonstrates various multilingual text processing techniques:
- Language detection
- Multilingual text analysis
- Cross-lingual similarity computation
- Machine translation
- Multilingual sentiment analysis
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
import re
import time
from collections import defaultdict, Counter
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from src.text_vectorizer import TextVectorizer
from src.utils import load_documents

# å°è¯•å¯¼å…¥è¯­è¨€æ£€æµ‹åº“
try:
    from langdetect import detect, detect_langs, DetectorFactory
    DetectorFactory.seed = 0  # ç¡®ä¿ç»“æœå¯é‡ç°
    HAS_LANGDETECT = True
except ImportError:
    HAS_LANGDETECT = False
    print("æç¤º: å®‰è£…langdetectåº“ä»¥è¿›è¡Œè¯­è¨€æ£€æµ‹: pip install langdetect")

# å°è¯•å¯¼å…¥ç¿»è¯‘åº“
try:
    from googletrans import Translator
    HAS_GOOGLETRANS = True
except ImportError:
    HAS_GOOGLETRANS = False
    print("æç¤º: å®‰è£…googletransåº“ä»¥è¿›è¡Œæœºå™¨ç¿»è¯‘: pip install googletrans==4.0.0-rc1")

# å°è¯•å¯¼å…¥å¤šè¯­è¨€BERT
try:
    from transformers import AutoTokenizer, AutoModel, pipeline
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    print("æç¤º: å®‰è£…transformersåº“ä»¥ä½¿ç”¨å¤šè¯­è¨€BERT: pip install transformers")

# å°è¯•å¯¼å…¥å¥å­å˜æ¢å™¨
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("æç¤º: å®‰è£…sentence-transformersåº“ä»¥è¿›è¡Œè·¨è¯­è¨€ç›¸ä¼¼æ€§è®¡ç®—: pip install sentence-transformers")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class LanguageDetectionResult:
    """è¯­è¨€æ£€æµ‹ç»“æœ"""
    text: str
    detected_language: str
    confidence: float
    all_probabilities: Dict[str, float]

@dataclass
class TranslationResult:
    """ç¿»è¯‘ç»“æœ"""
    original_text: str
    translated_text: str
    source_language: str
    target_language: str
    confidence: float = 1.0

@dataclass
class MultilingualSimilarity:
    """å¤šè¯­è¨€ç›¸ä¼¼æ€§ç»“æœ"""
    text1: str
    text2: str
    language1: str
    language2: str
    similarity: float
    method: str

class LanguageDetector:
    """è¯­è¨€æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.language_names = {
            'zh-cn': 'ä¸­æ–‡',
            'zh': 'ä¸­æ–‡',
            'en': 'è‹±è¯­',
            'ja': 'æ—¥è¯­',
            'ko': 'éŸ©è¯­',
            'fr': 'æ³•è¯­',
            'de': 'å¾·è¯­',
            'es': 'è¥¿ç­ç‰™è¯­',
            'it': 'æ„å¤§åˆ©è¯­',
            'pt': 'è‘¡è„ç‰™è¯­',
            'ru': 'ä¿„è¯­',
            'ar': 'é˜¿æ‹‰ä¼¯è¯­',
            'hi': 'å°åœ°è¯­',
            'th': 'æ³°è¯­',
            'vi': 'è¶Šå—è¯­'
        }
    
    def detect_language(self, text: str) -> LanguageDetectionResult:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        # ç®€å•çš„åŸºäºå­—ç¬¦çš„æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(text.replace(' ', ''))
        
        if total_chars == 0:
            return LanguageDetectionResult(
                text=text,
                detected_language='unknown',
                confidence=0.0,
                all_probabilities={'unknown': 1.0}
            )
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        probabilities = {}
        
        if HAS_LANGDETECT and len(text.strip()) > 3:
            try:
                # ä½¿ç”¨langdetectåº“
                detected_lang = detect(text)
                lang_probs = detect_langs(text)
                
                probabilities = {}
                for lang_prob in lang_probs:
                    lang_code = lang_prob.lang
                    probabilities[lang_code] = lang_prob.prob
                
                confidence = max(probabilities.values())
                
                return LanguageDetectionResult(
                    text=text,
                    detected_language=detected_lang,
                    confidence=confidence,
                    all_probabilities=probabilities
                )
            
            except Exception as e:
                print(f"è¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•è§„åˆ™: {e}")
        
        # åŸºäºè§„åˆ™çš„ç®€å•æ£€æµ‹
        if chinese_ratio > 0.3:
            detected_lang = 'zh'
            confidence = chinese_ratio
            probabilities = {'zh': chinese_ratio, 'en': english_ratio}
        elif english_ratio > 0.5:
            detected_lang = 'en'
            confidence = english_ratio
            probabilities = {'en': english_ratio, 'zh': chinese_ratio}
        else:
            detected_lang = 'unknown'
            confidence = 0.5
            probabilities = {'unknown': 0.5, 'en': english_ratio, 'zh': chinese_ratio}
        
        return LanguageDetectionResult(
            text=text,
            detected_language=detected_lang,
            confidence=confidence,
            all_probabilities=probabilities
        )
    
    def get_language_name(self, lang_code: str) -> str:
        """è·å–è¯­è¨€åç§°"""
        return self.language_names.get(lang_code, lang_code)

class MachineTranslator:
    """æœºå™¨ç¿»è¯‘å™¨"""
    
    def __init__(self):
        self.translator = None
        if HAS_GOOGLETRANS:
            try:
                self.translator = Translator()
            except Exception as e:
                print(f"åˆå§‹åŒ–ç¿»è¯‘å™¨å¤±è´¥: {e}")
        
        # è¯­è¨€ä»£ç æ˜ å°„
        self.language_codes = {
            'ä¸­æ–‡': 'zh',
            'è‹±è¯­': 'en',
            'æ—¥è¯­': 'ja',
            'éŸ©è¯­': 'ko',
            'æ³•è¯­': 'fr',
            'å¾·è¯­': 'de',
            'è¥¿ç­ç‰™è¯­': 'es',
            'æ„å¤§åˆ©è¯­': 'it',
            'è‘¡è„ç‰™è¯­': 'pt',
            'ä¿„è¯­': 'ru'
        }
    
    def translate_text(self, text: str, target_lang: str, source_lang: str = None) -> TranslationResult:
        """ç¿»è¯‘æ–‡æœ¬"""
        if not self.translator:
            # ç®€å•çš„æ¨¡æ‹Ÿç¿»è¯‘
            return self._mock_translation(text, target_lang, source_lang)
        
        try:
            # æ£€æµ‹æºè¯­è¨€
            if source_lang is None:
                detection = self.translator.detect(text)
                source_lang = detection.lang
                confidence = detection.confidence
            else:
                confidence = 1.0
            
            # æ‰§è¡Œç¿»è¯‘
            translation = self.translator.translate(text, dest=target_lang, src=source_lang)
            
            return TranslationResult(
                original_text=text,
                translated_text=translation.text,
                source_language=source_lang,
                target_language=target_lang,
                confidence=confidence
            )
        
        except Exception as e:
            print(f"ç¿»è¯‘å¤±è´¥: {e}")
            return self._mock_translation(text, target_lang, source_lang)
    
    def _mock_translation(self, text: str, target_lang: str, source_lang: str) -> TranslationResult:
        """æ¨¡æ‹Ÿç¿»è¯‘ï¼ˆå½“çœŸå®ç¿»è¯‘ä¸å¯ç”¨æ—¶ï¼‰"""
        # ç®€å•çš„æ›¿æ¢ç¤ºä¾‹
        mock_translations = {
            ('zh', 'en'): {
                'ä½ å¥½': 'Hello',
                'è°¢è°¢': 'Thank you',
                'å†è§': 'Goodbye',
                'äººå·¥æ™ºèƒ½': 'Artificial Intelligence',
                'æœºå™¨å­¦ä¹ ': 'Machine Learning',
                'æ·±åº¦å­¦ä¹ ': 'Deep Learning'
            },
            ('en', 'zh'): {
                'Hello': 'ä½ å¥½',
                'Thank you': 'è°¢è°¢',
                'Goodbye': 'å†è§',
                'Artificial Intelligence': 'äººå·¥æ™ºèƒ½',
                'Machine Learning': 'æœºå™¨å­¦ä¹ ',
                'Deep Learning': 'æ·±åº¦å­¦ä¹ '
            }
        }
        
        if source_lang is None:
            source_lang = 'auto'
        
        translation_dict = mock_translations.get((source_lang, target_lang), {})
        translated_text = text
        
        for original, translation in translation_dict.items():
            translated_text = translated_text.replace(original, translation)
        
        return TranslationResult(
            original_text=text,
            translated_text=translated_text,
            source_language=source_lang,
            target_language=target_lang,
            confidence=0.8
        )

class CrosslingualSimilarity:
    """è·¨è¯­è¨€ç›¸ä¼¼æ€§è®¡ç®—"""
    
    def __init__(self):
        self.model = None
        self.vectorizer = TextVectorizer()
        
        if HAS_SENTENCE_TRANSFORMERS:
            try:
                # ä½¿ç”¨å¤šè¯­è¨€sentence transformeræ¨¡å‹
                self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                print("åŠ è½½å¤šè¯­è¨€ç›¸ä¼¼æ€§æ¨¡å‹æˆåŠŸ")
            except Exception as e:
                print(f"åŠ è½½å¤šè¯­è¨€æ¨¡å‹å¤±è´¥: {e}")
    
    def calculate_similarity(self, text1: str, text2: str, lang1: str = None, lang2: str = None) -> MultilingualSimilarity:
        """è®¡ç®—è·¨è¯­è¨€ç›¸ä¼¼æ€§"""
        if self.model:
            try:
                # ä½¿ç”¨sentence transformerè®¡ç®—ç›¸ä¼¼æ€§
                embeddings = self.model.encode([text1, text2])
                similarity = np.dot(embeddings[0], embeddings[1]) / (
                    np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
                )
                method = 'sentence_transformer'
            except Exception as e:
                print(f"ä½¿ç”¨æ¨¡å‹è®¡ç®—ç›¸ä¼¼æ€§å¤±è´¥: {e}")
                similarity = self._calculate_simple_similarity(text1, text2)
                method = 'simple'
        else:
            similarity = self._calculate_simple_similarity(text1, text2)
            method = 'simple'
        
        return MultilingualSimilarity(
            text1=text1,
            text2=text2,
            language1=lang1 or 'unknown',
            language2=lang2 or 'unknown',
            similarity=similarity,
            method=method
        )
    
    def _calculate_simple_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„ç›¸ä¼¼æ€§è®¡ç®—ï¼ˆåŸºäºå­—ç¬¦é‡å ï¼‰"""
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—ç¬¦é›†åˆ
        chars1 = set(text1.lower())
        chars2 = set(text2.lower())
        
        # è®¡ç®—Jaccardç›¸ä¼¼æ€§
        intersection = len(chars1 & chars2)
        union = len(chars1 | chars2)
        
        return intersection / union if union > 0 else 0.0

class MultilingualSentiment:
    """å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ"""
    
    def __init__(self):
        self.sentiment_model = None
        
        if HAS_TRANSFORMERS:
            try:
                # ä½¿ç”¨å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¨¡å‹
                self.sentiment_model = pipeline(
                    "sentiment-analysis",
                    model="cardiffnlp/twitter-xlm-roberta-base-sentiment",
                    return_all_scores=True
                )
                print("åŠ è½½å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¨¡å‹æˆåŠŸ")
            except Exception as e:
                print(f"åŠ è½½å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¨¡å‹å¤±è´¥: {e}")
    
    def analyze_sentiment(self, text: str, language: str = None) -> Dict[str, Any]:
        """åˆ†æå¤šè¯­è¨€æƒ…æ„Ÿ"""
        if self.sentiment_model:
            try:
                results = self.sentiment_model(text)
                if isinstance(results[0], list):
                    results = results[0]
                
                sentiment_scores = {result['label']: result['score'] for result in results}
                predicted_sentiment = max(sentiment_scores, key=sentiment_scores.get)
                confidence = sentiment_scores[predicted_sentiment]
                
                return {
                    'text': text,
                    'language': language,
                    'sentiment': predicted_sentiment,
                    'confidence': confidence,
                    'scores': sentiment_scores,
                    'method': 'multilingual_transformer'
                }
            except Exception as e:
                print(f"å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
        
        # ç®€å•çš„åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æ
        positive_words = {
            'zh': ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'é«˜å…´', 'å¿«ä¹', 'æ»¡æ„', 'èµ'],
            'en': ['good', 'great', 'excellent', 'like', 'happy', 'love', 'satisfied', 'amazing']
        }
        
        negative_words = {
            'zh': ['å', 'å·®', 'è®¨åŒ', 'ç”Ÿæ°”', 'å¤±æœ›', 'ç³Ÿç³•', 'ä¸æ»¡', 'æ„¤æ€’'],
            'en': ['bad', 'terrible', 'hate', 'angry', 'disappointed', 'awful', 'dissatisfied', 'furious']
        }
        
        lang = language or 'en'
        pos_words = positive_words.get(lang, positive_words['en'])
        neg_words = negative_words.get(lang, negative_words['en'])
        
        text_lower = text.lower()
        pos_count = sum(1 for word in pos_words if word in text_lower)
        neg_count = sum(1 for word in neg_words if word in text_lower)
        
        if pos_count > neg_count:
            sentiment = 'POSITIVE'
            confidence = pos_count / (pos_count + neg_count + 1)
        elif neg_count > pos_count:
            sentiment = 'NEGATIVE'
            confidence = neg_count / (pos_count + neg_count + 1)
        else:
            sentiment = 'NEUTRAL'
            confidence = 0.5
        
        return {
            'text': text,
            'language': language,
            'sentiment': sentiment,
            'confidence': confidence,
            'scores': {sentiment: confidence},
            'method': 'lexicon_based'
        }

class MultilingualApp:
    """å¤šè¯­è¨€æ–‡æœ¬å¤„ç†æ•™è‚²åº”ç”¨"""
    
    def __init__(self):
        self.language_detector = LanguageDetector()
        self.translator = MachineTranslator()
        self.similarity_calculator = CrosslingualSimilarity()
        self.sentiment_analyzer = MultilingualSentiment()
        self.processed_texts = []
        
    def analyze_text_languages(self, texts: List[str]) -> List[LanguageDetectionResult]:
        """åˆ†ææ–‡æœ¬è¯­è¨€"""
        results = []
        
        print("åˆ†ææ–‡æœ¬è¯­è¨€...")
        for i, text in enumerate(texts):
            result = self.language_detector.detect_language(text)
            results.append(result)
            
            lang_name = self.language_detector.get_language_name(result.detected_language)
            print(f"æ–‡æœ¬ {i+1}: {lang_name} (ç½®ä¿¡åº¦: {result.confidence:.3f})")
            print(f"  å†…å®¹: {text[:50]}...")
        
        return results
    
    def translate_texts(self, texts: List[str], target_language: str) -> List[TranslationResult]:
        """ç¿»è¯‘æ–‡æœ¬"""
        results = []
        
        print(f"ç¿»è¯‘æ–‡æœ¬åˆ° {target_language}...")
        for i, text in enumerate(texts):
            result = self.translator.translate_text(text, target_language)
            results.append(result)
            
            print(f"æ–‡æœ¬ {i+1}:")
            print(f"  åŸæ–‡: {result.original_text}")
            print(f"  è¯‘æ–‡: {result.translated_text}")
            print(f"  ç½®ä¿¡åº¦: {result.confidence:.3f}")
        
        return results
    
    def compare_crosslingual_similarity(self, text_pairs: List[Tuple[str, str]]) -> List[MultilingualSimilarity]:
        """æ¯”è¾ƒè·¨è¯­è¨€ç›¸ä¼¼æ€§"""
        results = []
        
        print("è®¡ç®—è·¨è¯­è¨€ç›¸ä¼¼æ€§...")
        for i, (text1, text2) in enumerate(text_pairs):
            # æ£€æµ‹è¯­è¨€
            lang1_result = self.language_detector.detect_language(text1)
            lang2_result = self.language_detector.detect_language(text2)
            
            # è®¡ç®—ç›¸ä¼¼æ€§
            similarity = self.similarity_calculator.calculate_similarity(
                text1, text2, 
                lang1_result.detected_language, 
                lang2_result.detected_language
            )
            results.append(similarity)
            
            lang1_name = self.language_detector.get_language_name(lang1_result.detected_language)
            lang2_name = self.language_detector.get_language_name(lang2_result.detected_language)
            
            print(f"æ–‡æœ¬å¯¹ {i+1}:")
            print(f"  æ–‡æœ¬1 ({lang1_name}): {text1}")
            print(f"  æ–‡æœ¬2 ({lang2_name}): {text2}")
            print(f"  ç›¸ä¼¼æ€§: {similarity.similarity:.3f} ({similarity.method})")
        
        return results
    
    def analyze_multilingual_sentiment(self, texts: List[str]) -> List[Dict[str, Any]]:
        """åˆ†æå¤šè¯­è¨€æƒ…æ„Ÿ"""
        results = []
        
        print("åˆ†æå¤šè¯­è¨€æƒ…æ„Ÿ...")
        for i, text in enumerate(texts):
            # æ£€æµ‹è¯­è¨€
            lang_result = self.language_detector.detect_language(text)
            
            # åˆ†ææƒ…æ„Ÿ
            sentiment_result = self.sentiment_analyzer.analyze_sentiment(
                text, lang_result.detected_language
            )
            results.append(sentiment_result)
            
            lang_name = self.language_detector.get_language_name(lang_result.detected_language)
            
            print(f"æ–‡æœ¬ {i+1} ({lang_name}):")
            print(f"  å†…å®¹: {text}")
            print(f"  æƒ…æ„Ÿ: {sentiment_result['sentiment']}")
            print(f"  ç½®ä¿¡åº¦: {sentiment_result['confidence']:.3f}")
        
        return results
    
    def visualize_language_distribution(self, texts: List[str]):
        """å¯è§†åŒ–è¯­è¨€åˆ†å¸ƒ"""
        # æ£€æµ‹æ‰€æœ‰æ–‡æœ¬çš„è¯­è¨€
        language_counts = Counter()
        confidence_scores = []
        
        for text in texts:
            result = self.language_detector.detect_language(text)
            lang_name = self.language_detector.get_language_name(result.detected_language)
            language_counts[lang_name] += 1
            confidence_scores.append(result.confidence)
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # è¯­è¨€åˆ†å¸ƒé¥¼å›¾
        if language_counts:
            languages = list(language_counts.keys())
            counts = list(language_counts.values())
            
            axes[0].pie(counts, labels=languages, autopct='%1.1f%%')
            axes[0].set_title('è¯­è¨€åˆ†å¸ƒ')
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
        if confidence_scores:
            axes[1].hist(confidence_scores, bins=20, alpha=0.7, edgecolor='black')
            axes[1].set_xlabel('ç½®ä¿¡åº¦')
            axes[1].set_ylabel('é¢‘æ¬¡')
            axes[1].set_title('è¯­è¨€æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nè¯­è¨€ç»Ÿè®¡:")
        for lang, count in language_counts.most_common():
            percentage = count / len(texts) * 100
            print(f"  {lang}: {count} ä¸ªæ–‡æœ¬ ({percentage:.1f}%)")
        
        if confidence_scores:
            print(f"\nç½®ä¿¡åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")
            print(f"  æœ€ä½ç½®ä¿¡åº¦: {np.min(confidence_scores):.3f}")
            print(f"  æœ€é«˜ç½®ä¿¡åº¦: {np.max(confidence_scores):.3f}")
    
    def create_translation_comparison(self, texts: List[str], target_languages: List[str]):
        """åˆ›å»ºç¿»è¯‘æ¯”è¾ƒè¡¨"""
        print("åˆ›å»ºç¿»è¯‘æ¯”è¾ƒè¡¨...")
        
        # æ£€æµ‹åŸå§‹è¯­è¨€
        original_results = []
        for text in texts:
            lang_result = self.language_detector.detect_language(text)
            original_results.append(lang_result)
        
        # åˆ›å»ºDataFrame
        data = []
        
        for i, text in enumerate(texts):
            row = {
                'åŸæ–‡': text[:50] + '...' if len(text) > 50 else text,
                'åŸå§‹è¯­è¨€': self.language_detector.get_language_name(original_results[i].detected_language)
            }
            
            # ç¿»è¯‘åˆ°å„ç›®æ ‡è¯­è¨€
            for target_lang in target_languages:
                translation_result = self.translator.translate_text(text, target_lang)
                lang_name = self.language_detector.get_language_name(target_lang)
                row[f'ç¿»è¯‘åˆ°{lang_name}'] = translation_result.translated_text[:50] + '...' if len(translation_result.translated_text) > 50 else translation_result.translated_text
            
            data.append(row)
        
        df = pd.DataFrame(data)
        print("\nç¿»è¯‘æ¯”è¾ƒè¡¨:")
        print(df.to_string(index=False))
        
        return df
    
    def run_interactive_demo(self):
        """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
        print("\nğŸŒ å¤šè¯­è¨€æ–‡æœ¬å¤„ç†æ•™è‚²åº”ç”¨")
        print("=" * 50)
        
        sample_texts = [
            "Hello, how are you today?",
            "ä½ å¥½ï¼Œä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ",
            "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
            "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ì–´ë– ì„¸ìš”?",
            "Bonjour, comment allez-vous aujourd'hui?"
        ]
        
        while True:
            print("\né€‰æ‹©æ“ä½œ:")
            print("1. è¯­è¨€æ£€æµ‹")
            print("2. æœºå™¨ç¿»è¯‘")
            print("3. è·¨è¯­è¨€ç›¸ä¼¼æ€§")
            print("4. å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ")
            print("5. è¯­è¨€åˆ†å¸ƒå¯è§†åŒ–")
            print("6. ç¿»è¯‘æ¯”è¾ƒè¡¨")
            print("7. ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬æ¼”ç¤º")
            print("8. åŠ è½½è‡ªå®šä¹‰æ–‡æœ¬")
            print("0. é€€å‡º")
            
            choice = input("\nè¯·é€‰æ‹© (0-8): ").strip()
            
            if choice == '0':
                break
            
            elif choice == '1':
                text = input("è¯·è¾“å…¥è¦æ£€æµ‹è¯­è¨€çš„æ–‡æœ¬: ").strip()
                if text:
                    result = self.language_detector.detect_language(text)
                    lang_name = self.language_detector.get_language_name(result.detected_language)
                    print(f"\næ£€æµ‹ç»“æœ:")
                    print(f"  è¯­è¨€: {lang_name}")
                    print(f"  ç½®ä¿¡åº¦: {result.confidence:.3f}")
                    print(f"  æ‰€æœ‰æ¦‚ç‡:")
                    for lang, prob in result.all_probabilities.items():
                        lang_name = self.language_detector.get_language_name(lang)
                        print(f"    {lang_name}: {prob:.3f}")
            
            elif choice == '2':
                text = input("è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬: ").strip()
                if text:
                    target_lang = input("ç›®æ ‡è¯­è¨€ä»£ç  (en/zh/ja/ko/fr/de/es): ").strip()
                    if target_lang:
                        result = self.translator.translate_text(text, target_lang)
                        print(f"\nç¿»è¯‘ç»“æœ:")
                        print(f"  åŸæ–‡: {result.original_text}")
                        print(f"  è¯‘æ–‡: {result.translated_text}")
                        print(f"  æºè¯­è¨€: {result.source_language}")
                        print(f"  ç›®æ ‡è¯­è¨€: {result.target_language}")
                        print(f"  ç½®ä¿¡åº¦: {result.confidence:.3f}")
            
            elif choice == '3':
                text1 = input("è¯·è¾“å…¥ç¬¬ä¸€ä¸ªæ–‡æœ¬: ").strip()
                text2 = input("è¯·è¾“å…¥ç¬¬äºŒä¸ªæ–‡æœ¬: ").strip()
                if text1 and text2:
                    results = self.compare_crosslingual_similarity([(text1, text2)])
                    result = results[0]
                    print(f"\nç›¸ä¼¼æ€§åˆ†æ:")
                    print(f"  æ–‡æœ¬1è¯­è¨€: {result.language1}")
                    print(f"  æ–‡æœ¬2è¯­è¨€: {result.language2}")
                    print(f"  ç›¸ä¼¼æ€§åˆ†æ•°: {result.similarity:.3f}")
                    print(f"  è®¡ç®—æ–¹æ³•: {result.method}")
            
            elif choice == '4':
                text = input("è¯·è¾“å…¥è¦åˆ†ææƒ…æ„Ÿçš„æ–‡æœ¬: ").strip()
                if text:
                    results = self.analyze_multilingual_sentiment([text])
                    result = results[0]
                    print(f"\næƒ…æ„Ÿåˆ†æç»“æœ:")
                    print(f"  è¯­è¨€: {result['language']}")
                    print(f"  æƒ…æ„Ÿ: {result['sentiment']}")
                    print(f"  ç½®ä¿¡åº¦: {result['confidence']:.3f}")
                    print(f"  æ–¹æ³•: {result['method']}")
            
            elif choice == '5':
                print("è¯·è¾“å…¥å¤šä¸ªæ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ª (è¾“å…¥ç©ºè¡Œç»“æŸ):")
                texts = []
                while True:
                    line = input().strip()
                    if not line:
                        break
                    texts.append(line)
                
                if texts:
                    self.visualize_language_distribution(texts)
                else:
                    print("æ²¡æœ‰è¾“å…¥æ–‡æœ¬")
            
            elif choice == '6':
                print("è¯·è¾“å…¥å¤šä¸ªæ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ª (è¾“å…¥ç©ºè¡Œç»“æŸ):")
                texts = []
                while True:
                    line = input().strip()
                    if not line:
                        break
                    texts.append(line)
                
                if texts:
                    target_langs = input("ç›®æ ‡è¯­è¨€ä»£ç  (ç”¨é€—å·åˆ†éš”, å¦‚: en,zh,ja): ").strip().split(',')
                    target_langs = [lang.strip() for lang in target_langs if lang.strip()]
                    if target_langs:
                        self.create_translation_comparison(texts, target_langs)
                    else:
                        print("æ²¡æœ‰æŒ‡å®šç›®æ ‡è¯­è¨€")
                else:
                    print("æ²¡æœ‰è¾“å…¥æ–‡æœ¬")
            
            elif choice == '7':
                print("ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬è¿›è¡Œæ¼”ç¤º...")
                
                # è¯­è¨€æ£€æµ‹
                print("\n1. è¯­è¨€æ£€æµ‹æ¼”ç¤º:")
                self.analyze_text_languages(sample_texts)
                
                # ç¿»è¯‘æ¼”ç¤º
                print("\n2. ç¿»è¯‘æ¼”ç¤º:")
                self.translate_texts(sample_texts[:3], 'zh')
                
                # ç›¸ä¼¼æ€§æ¼”ç¤º
                print("\n3. è·¨è¯­è¨€ç›¸ä¼¼æ€§æ¼”ç¤º:")
                text_pairs = [
                    ("Hello world", "ä½ å¥½ä¸–ç•Œ"),
                    ("Good morning", "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™"),
                    ("Thank you", "ê°ì‚¬í•©ë‹ˆë‹¤")
                ]
                self.compare_crosslingual_similarity(text_pairs)
                
                # æƒ…æ„Ÿåˆ†ææ¼”ç¤º
                print("\n4. å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¼”ç¤º:")
                sentiment_texts = [
                    "I love this product!",
                    "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼",
                    "Cette application est terrible.",
                    "ì´ ì„œë¹„ìŠ¤ëŠ” ì •ë§ ì¢‹ìŠµë‹ˆë‹¤."
                ]
                self.analyze_multilingual_sentiment(sentiment_texts)
                
                # å¯è§†åŒ–
                print("\n5. è¯­è¨€åˆ†å¸ƒå¯è§†åŒ–:")
                self.visualize_language_distribution(sample_texts + sentiment_texts)
            
            elif choice == '8':
                try:
                    file_path = input("è¯·è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„ (TXTæˆ–JSON): ").strip()
                    if os.path.exists(file_path):
                        if file_path.endswith('.json'):
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                            
                            if isinstance(data, list):
                                if isinstance(data[0], dict) and 'text' in data[0]:
                                    texts = [item['text'] for item in data]
                                elif isinstance(data[0], str):
                                    texts = data
                                else:
                                    print("JSONæ ¼å¼é”™è¯¯")
                                    continue
                            else:
                                print("JSONæ•°æ®æ ¼å¼é”™è¯¯")
                                continue
                        
                        else:  # TXTæ–‡ä»¶
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            texts = [line.strip() for line in content.split('\n') if line.strip()]
                        
                        print(f"åŠ è½½äº† {len(texts)} ä¸ªæ–‡æœ¬")
                        
                        # åˆ†æè¯­è¨€åˆ†å¸ƒ
                        self.visualize_language_distribution(texts)
                        
                        # è¯¢é—®æ˜¯å¦è¿›è¡Œå…¶ä»–åˆ†æ
                        if input("æ˜¯å¦è¿›è¡Œæƒ…æ„Ÿåˆ†æ? (y/n): ").strip().lower() == 'y':
                            self.analyze_multilingual_sentiment(texts[:5])  # é™åˆ¶æ•°é‡
                    
                    else:
                        print("æ–‡ä»¶ä¸å­˜åœ¨")
                
                except Exception as e:
                    print(f"åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("åˆå§‹åŒ–å¤šè¯­è¨€æ–‡æœ¬å¤„ç†åº”ç”¨...")
    
    app = MultilingualApp()
    
    # ç¤ºä¾‹æ¼”ç¤º
    print("\nğŸ¯ æ¼”ç¤º: å¤šè¯­è¨€æ–‡æœ¬å¤„ç†")
    print("=" * 40)
    
    sample_texts = [
        "Artificial intelligence is changing the world.",
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚",
        "äººå·¥çŸ¥èƒ½ã¯ä¸–ç•Œã‚’å¤‰ãˆã¦ã„ã¾ã™ã€‚",
        "ì¸ê³µì§€ëŠ¥ì´ ì„¸ìƒì„ ë°”ê¾¸ê³  ìˆìŠµë‹ˆë‹¤.",
        "L'intelligence artificielle change le monde."
    ]
    
    # è¯­è¨€æ£€æµ‹æ¼”ç¤º
    print("\nğŸ“ è¯­è¨€æ£€æµ‹æ¼”ç¤º:")
    app.analyze_text_languages(sample_texts)
    
    # å¯è§†åŒ–è¯­è¨€åˆ†å¸ƒ
    print("\nğŸ“Š è¯­è¨€åˆ†å¸ƒå¯è§†åŒ–:")
    app.visualize_language_distribution(sample_texts)
    
    # è·¨è¯­è¨€ç›¸ä¼¼æ€§æ¼”ç¤º
    print("\nğŸ”— è·¨è¯­è¨€ç›¸ä¼¼æ€§æ¼”ç¤º:")
    similar_pairs = [
        ("Hello world", "ä½ å¥½ä¸–ç•Œ"),
        ("Machine learning", "æ©Ÿæ¢°å­¦ç¿’"),
        ("Good morning", "Buenos dÃ­as")
    ]
    app.compare_crosslingual_similarity(similar_pairs)
    
    # è¿è¡Œäº¤äº’å¼æ¼”ç¤º
    app.run_interactive_demo()

if __name__ == "__main__":
    main()
