#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘½åå®ä½“è¯†åˆ«æ•™è‚²åº”ç”¨
Named Entity Recognition (NER) Educational Application

è¿™ä¸ªåº”ç”¨å±•ç¤ºäº†å‘½åå®ä½“è¯†åˆ«çš„å„ç§æŠ€æœ¯ï¼š
- åŸºäºè§„åˆ™çš„å®ä½“è¯†åˆ«
- åŸºäºæœºå™¨å­¦ä¹ çš„å®ä½“è¯†åˆ«
- åŸºäºæ·±åº¦å­¦ä¹ çš„å®ä½“è¯†åˆ«
- è‡ªå®šä¹‰å®ä½“ç±»å‹
- å®ä½“å…³ç³»æŠ½å–
- çŸ¥è¯†å›¾è°±æ„å»º

This application demonstrates various NER techniques:
- Rule-based entity recognition
- Machine learning-based entity recognition
- Deep learning-based entity recognition
- Custom entity types
- Entity relationship extraction
- Knowledge graph construction
"""

import os
import sys
import re
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from typing import List, Dict, Tuple, Optional, Any, Set
from dataclasses import dataclass
from collections import defaultdict, Counter
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from src.text_vectorizer import TextVectorizer
from src.utils import load_documents

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    print("æç¤º: å®‰è£…transformersåº“ä»¥ä½¿ç”¨æ·±åº¦å­¦ä¹ NER: pip install transformers")

# å°è¯•å¯¼å…¥spaCy
try:
    import spacy
    HAS_SPACY = True
except ImportError:
    HAS_SPACY = False
    print("æç¤º: å®‰è£…spaCyåº“ä»¥è·å¾—æ›´å¥½çš„NERæ•ˆæœ: pip install spacy")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class Entity:
    """å‘½åå®ä½“"""
    text: str
    label: str
    start: int
    end: int
    confidence: float = 1.0
    context: str = ""

@dataclass
class Relation:
    """å®ä½“å…³ç³»"""
    subject: Entity
    predicate: str
    object: Entity
    confidence: float = 1.0

class RuleBasedNER:
    """åŸºäºè§„åˆ™çš„å‘½åå®ä½“è¯†åˆ«"""
    
    def __init__(self):
        # å®šä¹‰å®ä½“è§„åˆ™æ¨¡å¼
        self.patterns = {
            'PERSON': [
                r'[A-Z][a-z]+\s+[A-Z][a-z]+',  # è‹±æ–‡äººå
                r'[ç‹æå¼ åˆ˜é™ˆæ¨é»„èµµå‘¨å´å¾å­™æœ±é©¬èƒ¡éƒ­æ—ä½•é«˜æ¢éƒ‘ç½—å®‹è°¢å”éŸ©æ›¹è®¸é‚“è§å†¯æ›¾ç¨‹è”¡å½­æ½˜è¢äºè‘£ä½™è‹å¶å•é­è’‹ç”°æœä¸æ²ˆå§œèŒƒæ±Ÿå‚…é’Ÿå¢æ±ªæˆ´å´”ä»»é™†å»–å§šæ–¹é‡‘é‚±å¤è°­éŸ¦è´¾é‚¹çŸ³ç†Šå­Ÿç§¦é˜è–›ä¾¯é›·ç™½é¾™æ®µéƒå­”é‚µå²æ¯›å¸¸ä¸‡é¡¾èµ–æ­¦åº·è´ºä¸¥å°¹é’±æ–½ç‰›æ´ªé¾š][A-Za-z\u4e00-\u9fff]{1,3}',  # ä¸­æ–‡äººå
            ],
            'LOCATION': [
                r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s+(?:City|State|Province|County|District))?',  # è‹±æ–‡åœ°å
                r'[åŒ—äº¬ä¸Šæµ·å¤©æ´¥é‡åº†æ²³åŒ—å±±è¥¿è¾½å®å‰æ—é»‘é¾™æ±Ÿæ±Ÿè‹æµ™æ±Ÿå®‰å¾½ç¦å»ºæ±Ÿè¥¿å±±ä¸œæ²³å—æ¹–åŒ—æ¹–å—å¹¿ä¸œå¹¿è¥¿æµ·å—å››å·è´µå·äº‘å—è¥¿è—é™•è¥¿ç”˜è‚ƒé’æµ·å®å¤æ–°ç–†å†…è’™å¤é¦™æ¸¯æ¾³é—¨å°æ¹¾][å¸‚çœåŒºå¿é•‡æ‘]?',  # ä¸­å›½åœ°å
                r'[ä¸œè¥¿å—åŒ—ä¸­][A-Za-z\u4e00-\u9fff]{1,8}[å¸‚çœåŒºå¿é•‡æ‘è·¯è¡—é“]',  # æ–¹ä½åœ°å
            ],
            'ORGANIZATION': [
                r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s+(?:Inc|Corp|Ltd|Co|Company|University|College|School|Hospital|Bank))',  # è‹±æ–‡æœºæ„
                r'[A-Za-z\u4e00-\u9fff]{2,10}[å…¬å¸é›†å›¢é“¶è¡ŒåŒ»é™¢å­¦æ ¡å¤§å­¦å­¦é™¢]',  # ä¸­æ–‡æœºæ„
                r'[A-Za-z\u4e00-\u9fff]{2,10}[æœ‰é™è´£ä»»è‚¡ä»½]å…¬å¸',  # å…¬å¸ç±»å‹
            ],
            'DATE': [
                r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',  # ä¸­æ–‡æ—¥æœŸ
                r'\d{4}-\d{1,2}-\d{1,2}',  # æ•°å­—æ—¥æœŸ
                r'\d{1,2}/\d{1,2}/\d{4}',  # ç¾å¼æ—¥æœŸ
                r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4}',  # è‹±æ–‡æ—¥æœŸ
            ],
            'MONEY': [
                r'\$\d+(?:,\d{3})*(?:\.\d{2})?',  # ç¾å…ƒ
                r'Â¥\d+(?:,\d{3})*(?:\.\d{2})?',  # äººæ°‘å¸
                r'\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:ç¾å…ƒ|äººæ°‘å¸|å…ƒ|ä¸‡å…ƒ|äº¿å…ƒ)',  # ä¸­æ–‡è´§å¸
            ],
            'PHONE': [
                r'\d{3}-\d{3}-\d{4}',  # ç¾å¼ç”µè¯
                r'\d{11}',  # ä¸­å›½æ‰‹æœºå·
                r'\(\d{3}\)\s*\d{3}-\d{4}',  # å¸¦æ‹¬å·ç”µè¯
            ],
            'EMAIL': [
                r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',  # é‚®ç®±
            ],
            'URL': [
                r'https?://[^\s]+',  # ç½‘å€
                r'www\.[^\s]+',  # wwwç½‘å€
            ]
        }
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.compiled_patterns = {}
        for label, patterns in self.patterns.items():
            self.compiled_patterns[label] = [re.compile(pattern) for pattern in patterns]
    
    def extract_entities(self, text: str) -> List[Entity]:
        """æå–å‘½åå®ä½“"""
        entities = []
        
        for label, patterns in self.compiled_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    entity = Entity(
                        text=match.group(),
                        label=label,
                        start=match.start(),
                        end=match.end(),
                        confidence=1.0,
                        context=text[max(0, match.start()-20):match.end()+20]
                    )
                    entities.append(entity)
        
        # å»é‡å’Œåˆå¹¶é‡å å®ä½“
        entities = self._merge_overlapping_entities(entities)
        
        return entities
    
    def _merge_overlapping_entities(self, entities: List[Entity]) -> List[Entity]:
        """åˆå¹¶é‡å çš„å®ä½“"""
        if not entities:
            return entities
        
        # æŒ‰ä½ç½®æ’åº
        entities.sort(key=lambda x: (x.start, x.end))
        
        merged = [entities[0]]
        
        for current in entities[1:]:
            last = merged[-1]
            
            # æ£€æŸ¥æ˜¯å¦é‡å 
            if current.start < last.end:
                # é€‰æ‹©æ›´é•¿çš„å®ä½“æˆ–ç½®ä¿¡åº¦æ›´é«˜çš„å®ä½“
                if (current.end - current.start) > (last.end - last.start):
                    merged[-1] = current
                elif (current.end - current.start) == (last.end - last.start) and current.confidence > last.confidence:
                    merged[-1] = current
            else:
                merged.append(current)
        
        return merged

class TransformerNER:
    """åŸºäºTransformerçš„å‘½åå®ä½“è¯†åˆ«"""
    
    def __init__(self, model_name: str = "dbmdz/bert-large-cased-finetuned-conll03-english"):
        if not HAS_TRANSFORMERS:
            raise ImportError("éœ€è¦å®‰è£…transformersåº“")
        
        try:
            self.ner_pipeline = pipeline(
                "ner",
                model=model_name,
                tokenizer=model_name,
                aggregation_strategy="simple"
            )
            self.model_name = model_name
        except Exception as e:
            print(f"æ— æ³•åŠ è½½æ¨¡å‹ {model_name}ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹")
            try:
                self.ner_pipeline = pipeline("ner", aggregation_strategy="simple")
                self.model_name = "default"
            except Exception as e2:
                raise ImportError(f"æ— æ³•åˆå§‹åŒ–NERæ¨¡å‹: {e2}")
    
    def extract_entities(self, text: str) -> List[Entity]:
        """æå–å‘½åå®ä½“"""
        try:
            results = self.ner_pipeline(text)
            entities = []
            
            for result in results:
                # æ ‡å‡†åŒ–æ ‡ç­¾
                label = result['entity_group'].replace('B-', '').replace('I-', '')
                
                entity = Entity(
                    text=result['word'],
                    label=label,
                    start=result.get('start', 0),
                    end=result.get('end', len(result['word'])),
                    confidence=result['score'],
                    context=text
                )
                entities.append(entity)
            
            return entities
            
        except Exception as e:
            print(f"Transformer NERå¤±è´¥: {e}")
            return []

class SpacyNER:
    """åŸºäºspaCyçš„å‘½åå®ä½“è¯†åˆ«"""
    
    def __init__(self, model_name: str = "en_core_web_sm"):
        if not HAS_SPACY:
            raise ImportError("éœ€è¦å®‰è£…spaCyåº“")
        
        try:
            self.nlp = spacy.load(model_name)
            self.model_name = model_name
        except OSError:
            print(f"æ— æ³•åŠ è½½spaCyæ¨¡å‹ {model_name}")
            print("è¯·å…ˆä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm")
            raise
    
    def extract_entities(self, text: str) -> List[Entity]:
        """æå–å‘½åå®ä½“"""
        try:
            doc = self.nlp(text)
            entities = []
            
            for ent in doc.ents:
                entity = Entity(
                    text=ent.text,
                    label=ent.label_,
                    start=ent.start_char,
                    end=ent.end_char,
                    confidence=1.0,  # spaCyä¸æä¾›ç½®ä¿¡åº¦
                    context=text[max(0, ent.start_char-20):ent.end_char+20]
                )
                entities.append(entity)
            
            return entities
            
        except Exception as e:
            print(f"spaCy NERå¤±è´¥: {e}")
            return []

class CustomNER:
    """è‡ªå®šä¹‰å‘½åå®ä½“è¯†åˆ«"""
    
    def __init__(self):
        self.custom_entities = {
            'PRODUCT': [
                'iPhone', 'iPad', 'MacBook', 'Android', 'Windows',
                'Tesla Model', 'BMW', 'Mercedes', 'Toyota',
                'PlayStation', 'Xbox', 'Nintendo'
            ],
            'TECHNOLOGY': [
                'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'è‡ªç„¶è¯­è¨€å¤„ç†',
                'è®¡ç®—æœºè§†è§‰', 'æ•°æ®ç§‘å­¦', 'åŒºå—é“¾', 'äº‘è®¡ç®—',
                'Python', 'JavaScript', 'Java', 'C++', 'TensorFlow',
                'PyTorch', 'React', 'Vue', 'Angular'
            ],
            'DISEASE': [
                'æ–°å† è‚ºç‚', 'COVID-19', 'ç³–å°¿ç—…', 'é«˜è¡€å‹', 'ç™Œç—‡',
                'å¿ƒè„ç—…', 'è‚ºç‚', 'æµæ„Ÿ', 'æ„Ÿå†’', 'å‘çƒ§'
            ]
        }
    
    def add_entity_type(self, entity_type: str, entities: List[str]):
        """æ·»åŠ è‡ªå®šä¹‰å®ä½“ç±»å‹"""
        if entity_type not in self.custom_entities:
            self.custom_entities[entity_type] = []
        self.custom_entities[entity_type].extend(entities)
    
    def extract_entities(self, text: str) -> List[Entity]:
        """æå–è‡ªå®šä¹‰å®ä½“"""
        entities = []
        text_lower = text.lower()
        
        for entity_type, entity_list in self.custom_entities.items():
            for entity_text in entity_list:
                # æŸ¥æ‰¾å®ä½“åœ¨æ–‡æœ¬ä¸­çš„æ‰€æœ‰å‡ºç°ä½ç½®
                start = 0
                while True:
                    pos = text_lower.find(entity_text.lower(), start)
                    if pos == -1:
                        break
                    
                    entity = Entity(
                        text=text[pos:pos+len(entity_text)],
                        label=entity_type,
                        start=pos,
                        end=pos + len(entity_text),
                        confidence=1.0,
                        context=text[max(0, pos-20):pos+len(entity_text)+20]
                    )
                    entities.append(entity)
                    start = pos + 1
        
        return entities

class RelationExtractor:
    """å…³ç³»æŠ½å–å™¨"""
    
    def __init__(self):
        # å®šä¹‰å…³ç³»æ¨¡å¼
        self.relation_patterns = [
            (r'(.+)\s*æ˜¯\s*(.+)çš„.*', 'is_part_of'),
            (r'(.+)\s*åœ¨\s*(.+)', 'located_in'),
            (r'(.+)\s*å·¥ä½œåœ¨\s*(.+)', 'works_at'),
            (r'(.+)\s*å±äº\s*(.+)', 'belongs_to'),
            (r'(.+)\s*åˆ›ç«‹äº†\s*(.+)', 'founded'),
            (r'(.+)\s*ç”Ÿäº§\s*(.+)', 'produces'),
            (r'(.+)\s*ä¸\s*(.+)\s*åˆä½œ', 'cooperates_with'),
        ]
        
        self.compiled_patterns = [(re.compile(pattern), relation) 
                                for pattern, relation in self.relation_patterns]
    
    def extract_relations(self, text: str, entities: List[Entity]) -> List[Relation]:
        """æŠ½å–å®ä½“å…³ç³»"""
        relations = []
        
        # åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–
        for pattern, relation_type in self.compiled_patterns:
            for match in pattern.finditer(text):
                subject_text = match.group(1).strip()
                object_text = match.group(2).strip()
                
                # æŸ¥æ‰¾åŒ¹é…çš„å®ä½“
                subject_entity = self._find_entity_by_text(entities, subject_text)
                object_entity = self._find_entity_by_text(entities, object_text)
                
                if subject_entity and object_entity:
                    relation = Relation(
                        subject=subject_entity,
                        predicate=relation_type,
                        object=object_entity,
                        confidence=0.8
                    )
                    relations.append(relation)
        
        return relations
    
    def _find_entity_by_text(self, entities: List[Entity], text: str) -> Optional[Entity]:
        """æ ¹æ®æ–‡æœ¬æŸ¥æ‰¾å®ä½“"""
        for entity in entities:
            if text in entity.text or entity.text in text:
                return entity
        return None

class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±"""
    
    def __init__(self):
        self.entities = {}  # entity_id -> Entity
        self.relations = []  # List[Relation]
        self.entity_counter = 0
    
    def add_entity(self, entity: Entity) -> str:
        """æ·»åŠ å®ä½“"""
        entity_id = f"{entity.label}_{self.entity_counter}"
        self.entities[entity_id] = entity
        self.entity_counter += 1
        return entity_id
    
    def add_relation(self, relation: Relation):
        """æ·»åŠ å…³ç³»"""
        self.relations.append(relation)
    
    def build_from_entities_and_relations(self, entities: List[Entity], relations: List[Relation]):
        """ä»å®ä½“å’Œå…³ç³»æ„å»ºçŸ¥è¯†å›¾è°±"""
        # æ·»åŠ å®ä½“
        entity_map = {}
        for entity in entities:
            entity_id = self.add_entity(entity)
            entity_map[entity.text] = entity_id
        
        # æ·»åŠ å…³ç³»
        for relation in relations:
            self.add_relation(relation)
    
    def visualize(self, max_nodes: int = 50):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        if not self.entities:
            print("çŸ¥è¯†å›¾è°±ä¸ºç©º")
            return
        
        G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for entity_id, entity in list(self.entities.items())[:max_nodes]:
            G.add_node(entity_id, 
                      label=entity.text, 
                      entity_type=entity.label)
        
        # æ·»åŠ è¾¹
        for relation in self.relations:
            subject_id = None
            object_id = None
            
            # æŸ¥æ‰¾å®ä½“ID
            for entity_id, entity in self.entities.items():
                if entity.text == relation.subject.text:
                    subject_id = entity_id
                if entity.text == relation.object.text:
                    object_id = entity_id
            
            if subject_id and object_id and subject_id in G.nodes and object_id in G.nodes:
                G.add_edge(subject_id, object_id, 
                          relation=relation.predicate,
                          weight=relation.confidence)
        
        # ç»˜åˆ¶å›¾è°±
        plt.figure(figsize=(15, 10))
        pos = nx.spring_layout(G, k=1, iterations=50)
        
        # æ ¹æ®å®ä½“ç±»å‹è®¾ç½®é¢œè‰²
        entity_types = set(self.entities[node]['entity_type'] for node in G.nodes())
        colors = plt.cm.Set3(np.linspace(0, 1, len(entity_types)))
        color_map = dict(zip(entity_types, colors))
        
        node_colors = [color_map[self.entities[node]['entity_type']] for node in G.nodes()]
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, 
                             node_size=500, alpha=0.8)
        
        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(G, pos, alpha=0.5, width=1)
        
        # ç»˜åˆ¶æ ‡ç­¾
        labels = {node: self.entities[node]['label'][:10] + '...' 
                 if len(self.entities[node]['label']) > 10 
                 else self.entities[node]['label'] 
                 for node in G.nodes()}
        nx.draw_networkx_labels(G, pos, labels, font_size=8)
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                    markerfacecolor=color_map[entity_type], 
                                    markersize=10, label=entity_type)
                         for entity_type in entity_types]
        plt.legend(handles=legend_elements, loc='upper right')
        
        plt.title('çŸ¥è¯†å›¾è°±å¯è§†åŒ–')
        plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        entity_type_counts = Counter(entity.label for entity in self.entities.values())
        relation_type_counts = Counter(relation.predicate for relation in self.relations)
        
        return {
            'total_entities': len(self.entities),
            'total_relations': len(self.relations),
            'entity_types': dict(entity_type_counts),
            'relation_types': dict(relation_type_counts)
        }

class NERApp:
    """å‘½åå®ä½“è¯†åˆ«æ•™è‚²åº”ç”¨"""
    
    def __init__(self):
        self.extractors = {}
        self.relation_extractor = RelationExtractor()
        self.knowledge_graph = KnowledgeGraph()
        
        # åˆå§‹åŒ–å®ä½“è¯†åˆ«å™¨
        self.extractors['rule_based'] = RuleBasedNER()
        self.extractors['custom'] = CustomNER()
        
        if HAS_TRANSFORMERS:
            try:
                self.extractors['transformer'] = TransformerNER()
            except Exception as e:
                print(f"æ— æ³•åˆå§‹åŒ–Transformer NER: {e}")
        
        if HAS_SPACY:
            try:
                self.extractors['spacy'] = SpacyNER()
            except Exception as e:
                print(f"æ— æ³•åˆå§‹åŒ–spaCy NER: {e}")
    
    def extract_entities(self, text: str, methods: List[str] = None) -> Dict[str, List[Entity]]:
        """ä½¿ç”¨å¤šç§æ–¹æ³•æå–å®ä½“"""
        if methods is None:
            methods = list(self.extractors.keys())
        
        results = {}
        for method in methods:
            if method in self.extractors:
                try:
                    entities = self.extractors[method].extract_entities(text)
                    results[method] = entities
                except Exception as e:
                    print(f"æ–¹æ³• {method} æå–å®ä½“å¤±è´¥: {e}")
                    results[method] = []
        
        return results
    
    def compare_ner_methods(self, text: str):
        """æ¯”è¾ƒä¸åŒNERæ–¹æ³•"""
        results = self.extract_entities(text)
        
        print(f"\næ–‡æœ¬: {text}")
        print("=" * 80)
        
        for method, entities in results.items():
            print(f"\n{method.upper()} æ–¹æ³•è¯†åˆ«å‡º {len(entities)} ä¸ªå®ä½“:")
            print("-" * 50)
            
            if entities:
                for entity in entities:
                    print(f"  {entity.text:<15} | {entity.label:<12} | ç½®ä¿¡åº¦: {entity.confidence:.3f}")
            else:
                print("  æœªè¯†åˆ«å‡ºä»»ä½•å®ä½“")
    
    def analyze_entity_distribution(self, texts: List[str], method: str = 'rule_based'):
        """åˆ†æå®ä½“åˆ†å¸ƒ"""
        if method not in self.extractors:
            print(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
            return
        
        all_entities = []
        entity_type_counts = Counter()
        entity_text_counts = Counter()
        
        for text in texts:
            entities = self.extractors[method].extract_entities(text)
            all_entities.extend(entities)
            
            for entity in entities:
                entity_type_counts[entity.label] += 1
                entity_text_counts[entity.text.lower()] += 1
        
        # å¯è§†åŒ–å®ä½“ç±»å‹åˆ†å¸ƒ
        plt.figure(figsize=(15, 5))
        
        # å®ä½“ç±»å‹åˆ†å¸ƒ
        plt.subplot(1, 3, 1)
        if entity_type_counts:
            labels, counts = zip(*entity_type_counts.most_common(10))
            plt.pie(counts, labels=labels, autopct='%1.1f%%')
            plt.title('å®ä½“ç±»å‹åˆ†å¸ƒ')
        
        # å®ä½“é¢‘æ¬¡åˆ†å¸ƒ
        plt.subplot(1, 3, 2)
        if entity_text_counts:
            top_entities = entity_text_counts.most_common(10)
            entities, counts = zip(*top_entities)
            plt.barh(range(len(entities)), counts)
            plt.yticks(range(len(entities)), entities)
            plt.xlabel('é¢‘æ¬¡')
            plt.title('é«˜é¢‘å®ä½“')
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        plt.subplot(1, 3, 3)
        confidences = [entity.confidence for entity in all_entities]
        if confidences:
            plt.hist(confidences, bins=20, alpha=0.7, edgecolor='black')
            plt.xlabel('ç½®ä¿¡åº¦')
            plt.ylabel('é¢‘æ¬¡')
            plt.title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nå®ä½“åˆ†æç»Ÿè®¡ ({method}):")
        print(f"æ€»å®ä½“æ•°: {len(all_entities)}")
        print(f"å”¯ä¸€å®ä½“æ•°: {len(entity_text_counts)}")
        print(f"å®ä½“ç±»å‹æ•°: {len(entity_type_counts)}")
        
        print(f"\næœ€å¸¸è§å®ä½“ç±»å‹:")
        for entity_type, count in entity_type_counts.most_common(5):
            print(f"  {entity_type}: {count}")
    
    def build_knowledge_graph_from_text(self, text: str, method: str = 'rule_based'):
        """ä»æ–‡æœ¬æ„å»ºçŸ¥è¯†å›¾è°±"""
        # æå–å®ä½“
        entities = self.extractors[method].extract_entities(text)
        
        # æå–å…³ç³»
        relations = self.relation_extractor.extract_relations(text, entities)
        
        # æ„å»ºçŸ¥è¯†å›¾è°±
        self.knowledge_graph = KnowledgeGraph()
        self.knowledge_graph.build_from_entities_and_relations(entities, relations)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = self.knowledge_graph.get_statistics()
        print(f"\nçŸ¥è¯†å›¾è°±ç»Ÿè®¡:")
        print(f"å®ä½“æ•°: {stats['total_entities']}")
        print(f"å…³ç³»æ•°: {stats['total_relations']}")
        
        if stats['entity_types']:
            print(f"å®ä½“ç±»å‹åˆ†å¸ƒ:")
            for entity_type, count in stats['entity_types'].items():
                print(f"  {entity_type}: {count}")
        
        if stats['relation_types']:
            print(f"å…³ç³»ç±»å‹åˆ†å¸ƒ:")
            for relation_type, count in stats['relation_types'].items():
                print(f"  {relation_type}: {count}")
        
        # å¯è§†åŒ–çŸ¥è¯†å›¾è°±
        if stats['total_entities'] > 0:
            self.knowledge_graph.visualize()
    
    def evaluate_ner_performance(self, text: str, ground_truth: List[Entity], method: str = 'rule_based'):
        """è¯„ä¼°NERæ€§èƒ½"""
        predicted_entities = self.extractors[method].extract_entities(text)
        
        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        true_positives = 0
        false_positives = 0
        false_negatives = 0
        
        # ç®€å•çš„åŒ¹é…ç­–ç•¥ï¼šæ–‡æœ¬å’Œæ ‡ç­¾éƒ½åŒ¹é…
        predicted_set = {(e.text.lower(), e.label) for e in predicted_entities}
        ground_truth_set = {(e.text.lower(), e.label) for e in ground_truth}
        
        true_positives = len(predicted_set & ground_truth_set)
        false_positives = len(predicted_set - ground_truth_set)
        false_negatives = len(ground_truth_set - predicted_set)
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"\nNERæ€§èƒ½è¯„ä¼° ({method}):")
        print(f"ç²¾ç¡®ç‡: {precision:.3f}")
        print(f"å¬å›ç‡: {recall:.3f}")
        print(f"F1åˆ†æ•°: {f1:.3f}")
        print(f"æ­£ç¡®è¯†åˆ«: {true_positives}")
        print(f"é”™è¯¯è¯†åˆ«: {false_positives}")
        print(f"é—æ¼è¯†åˆ«: {false_negatives}")
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives
        }
    
    def run_interactive_demo(self):
        """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
        print("\nğŸ·ï¸ å‘½åå®ä½“è¯†åˆ«æ•™è‚²åº”ç”¨")
        print("=" * 50)
        print("å¯ç”¨çš„NERæ–¹æ³•:")
        for i, method in enumerate(self.extractors.keys(), 1):
            print(f"  {i}. {method}")
        
        while True:
            print("\né€‰æ‹©æ“ä½œ:")
            print("1. å•æ–‡æœ¬å®ä½“è¯†åˆ«")
            print("2. æ–¹æ³•æ¯”è¾ƒ")
            print("3. å®ä½“åˆ†å¸ƒåˆ†æ")
            print("4. æ„å»ºçŸ¥è¯†å›¾è°±")
            print("5. æ·»åŠ è‡ªå®šä¹‰å®ä½“")
            print("6. åŠ è½½ç¤ºä¾‹æ–‡æ¡£")
            print("0. é€€å‡º")
            
            choice = input("\nè¯·é€‰æ‹© (0-6): ").strip()
            
            if choice == '0':
                break
            elif choice == '1':
                text = input("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬: ").strip()
                if text:
                    method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.extractors.keys())}): ").strip()
                    if method in self.extractors:
                        entities = self.extractors[method].extract_entities(text)
                        print(f"\nè¯†åˆ«å‡º {len(entities)} ä¸ªå®ä½“:")
                        for entity in entities:
                            print(f"  {entity.text} [{entity.label}] (ç½®ä¿¡åº¦: {entity.confidence:.3f})")
                    else:
                        print("æ— æ•ˆçš„æ–¹æ³•")
            
            elif choice == '2':
                text = input("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬: ").strip()
                if text:
                    self.compare_ner_methods(text)
            
            elif choice == '3':
                print("è¯·è¾“å…¥å¤šä¸ªæ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ª (è¾“å…¥ç©ºè¡Œç»“æŸ):")
                texts = []
                while True:
                    line = input().strip()
                    if not line:
                        break
                    texts.append(line)
                
                if texts:
                    method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.extractors.keys())}): ").strip()
                    if method in self.extractors:
                        self.analyze_entity_distribution(texts, method)
                    else:
                        print("æ— æ•ˆçš„æ–¹æ³•")
            
            elif choice == '4':
                text = input("è¯·è¾“å…¥è¦æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–‡æœ¬: ").strip()
                if text:
                    method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.extractors.keys())}): ").strip()
                    if method in self.extractors:
                        self.build_knowledge_graph_from_text(text, method)
                    else:
                        print("æ— æ•ˆçš„æ–¹æ³•")
            
            elif choice == '5':
                entity_type = input("è¯·è¾“å…¥å®ä½“ç±»å‹: ").strip().upper()
                print("è¯·è¾“å…¥å®ä½“åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ª (è¾“å…¥ç©ºè¡Œç»“æŸ):")
                entities = []
                while True:
                    line = input().strip()
                    if not line:
                        break
                    entities.append(line)
                
                if entity_type and entities:
                    self.extractors['custom'].add_entity_type(entity_type, entities)
                    print(f"å·²æ·»åŠ  {len(entities)} ä¸ª {entity_type} ç±»å‹çš„å®ä½“")
            
            elif choice == '6':
                try:
                    documents = load_documents('data/sample_documents.json')
                    texts = [doc['content'] for doc in documents[:5]]
                    print(f"åŠ è½½äº† {len(texts)} ä¸ªæ–‡æ¡£")
                    
                    method = input(f"é€‰æ‹©åˆ†ææ–¹æ³• ({'/'.join(self.extractors.keys())}): ").strip()
                    if method in self.extractors:
                        print("æ­£åœ¨åˆ†æ...")
                        self.analyze_entity_distribution(texts, method)
                        
                        # æ„å»ºçŸ¥è¯†å›¾è°±
                        print("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
                        combined_text = ' '.join(texts)
                        self.build_knowledge_graph_from_text(combined_text, method)
                    else:
                        print("æ— æ•ˆçš„æ–¹æ³•")
                except Exception as e:
                    print(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("åˆå§‹åŒ–å‘½åå®ä½“è¯†åˆ«åº”ç”¨...")
    
    app = NERApp()
    
    # ç¤ºä¾‹æ–‡æœ¬
    sample_text = """
    è‹¹æœå…¬å¸ï¼ˆApple Inc.ï¼‰æ˜¯ä¸€å®¶æ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·åº“æ¯”è’‚è¯ºçš„ç¾å›½è·¨å›½ç§‘æŠ€å…¬å¸ã€‚
    è¯¥å…¬å¸ç”±å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ã€å²è’‚å¤«Â·æ²ƒå…¹å°¼äºšå…‹å’Œç½—çº³å¾·Â·éŸ¦æ©äº1976å¹´4æœˆ1æ—¥åˆ›ç«‹ã€‚
    è‹¹æœå…¬å¸è®¾è®¡ã€å¼€å‘å’Œé”€å”®æ¶ˆè´¹ç”µå­äº§å“ã€è®¡ç®—æœºè½¯ä»¶å’Œåœ¨çº¿æœåŠ¡ã€‚
    å…¬å¸çš„ç¡¬ä»¶äº§å“åŒ…æ‹¬iPhoneæ™ºèƒ½æ‰‹æœºã€iPadå¹³æ¿ç”µè„‘ã€Macä¸ªäººç”µè„‘ã€iPodä¾¿æºå¼åª’ä½“æ’­æ”¾å™¨ã€
    Apple Watchæ™ºèƒ½æ‰‹è¡¨ã€Apple TVæ•°å­—åª’ä½“æ’­æ”¾å™¨å’ŒHomePodæ™ºèƒ½éŸ³ç®±ã€‚
    
    è‹¹æœå…¬å¸çš„CEOè’‚å§†Â·åº“å…‹åœ¨2023å¹´å®£å¸ƒå…¬å¸å°†æŠ•èµ„10äº¿ç¾å…ƒç”¨äºäººå·¥æ™ºèƒ½ç ”å‘ã€‚
    å…¬å¸æ€»éƒ¨ä½äºè‹¹æœå›­åŒº(Apple Park)ï¼Œè¯¥å›­åŒºäº2017å¹´å¼€æ”¾ï¼Œè€—èµ„çº¦50äº¿ç¾å…ƒå»ºè®¾ã€‚
    ä½ å¯ä»¥é€šè¿‡info@apple.comè”ç³»è‹¹æœå…¬å¸ï¼Œæˆ–è®¿é—®www.apple.comäº†è§£æ›´å¤šä¿¡æ¯ã€‚
    """
    
    print("\nğŸ¯ æ¼”ç¤º: å¤šæ–¹æ³•å®ä½“è¯†åˆ«")
    print("=" * 40)
    
    # æ¯”è¾ƒä¸åŒNERæ–¹æ³•
    app.compare_ner_methods(sample_text)
    
    # æ„å»ºçŸ¥è¯†å›¾è°±
    print("\næ„å»ºçŸ¥è¯†å›¾è°±...")
    app.build_knowledge_graph_from_text(sample_text)
    
    # è¿è¡Œäº¤äº’å¼æ¼”ç¤º
    app.run_interactive_demo()

if __name__ == "__main__":
    main()
