#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æ‘˜è¦æ•™è‚²åº”ç”¨
Text Summarization Educational Application

è¿™ä¸ªåº”ç”¨å±•ç¤ºäº†æ–‡æœ¬æ‘˜è¦çš„å„ç§æŠ€æœ¯ï¼š
- æå–å¼æ‘˜è¦ (Extractive Summarization)
- æŠ½è±¡å¼æ‘˜è¦ (Abstractive Summarization)
- å…³é”®è¯æå–
- ä¸»é¢˜å»ºæ¨¡
- æ‘˜è¦è´¨é‡è¯„ä¼°

This application demonstrates various text summarization techniques:
- Extractive summarization
- Abstractive summarization
- Keyword extraction
- Topic modeling
- Summary quality evaluation
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
import re
import time
from collections import defaultdict, Counter
import heapq

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from src.text_vectorizer import TextVectorizer
from src.utils import load_documents

# å°è¯•å¯¼å…¥NLTK
try:
    import nltk
    from nltk.tokenize import sent_tokenize, word_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import SnowballStemmer
    HAS_NLTK = True
except ImportError:
    HAS_NLTK = False
    print("æç¤º: å®‰è£…NLTKåº“ä»¥è·å¾—æ›´å¥½çš„æ–‡æœ¬å¤„ç†: pip install nltk")

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    print("æç¤º: å®‰è£…transformersåº“ä»¥ä½¿ç”¨æ·±åº¦å­¦ä¹ æ‘˜è¦: pip install transformers")

# å°è¯•å¯¼å…¥scikit-learn
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.cluster import KMeans
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("æç¤º: å®‰è£…scikit-learnåº“ä»¥ä½¿ç”¨æœºå™¨å­¦ä¹ åŠŸèƒ½: pip install scikit-learn")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class SummaryResult:
    """æ‘˜è¦ç»“æœ"""
    original_text: str
    summary: str
    method: str
    compression_ratio: float
    keywords: Optional[List[str]] = None
    key_sentences: Optional[List[str]] = None
    topics: Optional[List[Tuple[str, float]]] = None
    quality_score: Optional[float] = None

class ExtractiveSummarizer:
    """æå–å¼æ‘˜è¦å™¨"""
    
    def __init__(self):
        self.vectorizer = TextVectorizer() if 'TextVectorizer' in globals() else None
        self.stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'äº†', 'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æˆ‘ä»¬', 'ä»–ä»¬', 'å®ƒä»¬'}
    
    def _sentence_tokenize(self, text: str) -> List[str]:
        """å¥å­åˆ†è¯"""
        if HAS_NLTK:
            try:
                return sent_tokenize(text)
            except:
                pass
        
        # ç®€å•çš„ä¸­æ–‡å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _calculate_word_frequency(self, text: str) -> Dict[str, float]:
        """è®¡ç®—è¯é¢‘"""
        words = re.findall(r'\w+', text.lower())
        word_freq = {}
        
        for word in words:
            if word not in self.stop_words and len(word) > 1:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # å½’ä¸€åŒ–
        max_freq = max(word_freq.values()) if word_freq else 1
        for word in word_freq:
            word_freq[word] = word_freq[word] / max_freq
        
        return word_freq
    
    def _score_sentences(self, sentences: List[str], word_freq: Dict[str, float]) -> List[Tuple[int, float]]:
        """ç»™å¥å­æ‰“åˆ†"""
        sentence_scores = []
        
        for i, sentence in enumerate(sentences):
            words = re.findall(r'\w+', sentence.lower())
            score = 0
            word_count = 0
            
            for word in words:
                if word in word_freq:
                    score += word_freq[word]
                    word_count += 1
            
            # å¥å­åˆ†æ•° = è¯é¢‘åˆ†æ•°æ€»å’Œ / å¥å­ä¸­çš„è¯æ•°
            if word_count > 0:
                score = score / word_count
            
            # å¥å­é•¿åº¦æƒ©ç½š - å¤ªçŸ­çš„å¥å­åˆ†æ•°é™ä½
            if len(sentence) < 20:
                score *= 0.5
            
            sentence_scores.append((i, score))
        
        return sentence_scores
    
    def summarize(self, text: str, num_sentences: int = 3) -> SummaryResult:
        """ç”Ÿæˆæå–å¼æ‘˜è¦"""
        sentences = self._sentence_tokenize(text)
        
        if len(sentences) <= num_sentences:
            summary = ' '.join(sentences)
            keywords = list(self._calculate_word_frequency(text).keys())[:10]
            
            return SummaryResult(
                original_text=text,
                summary=summary,
                method='extractive',
                compression_ratio=1.0,
                keywords=keywords,
                key_sentences=sentences
            )
        
        # è®¡ç®—è¯é¢‘
        word_freq = self._calculate_word_frequency(text)
        
        # ç»™å¥å­æ‰“åˆ†
        sentence_scores = self._score_sentences(sentences, word_freq)
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        top_sentences = sentence_scores[:num_sentences]
        
        # æŒ‰åŸæ–‡é¡ºåºæ’åˆ—
        top_sentences.sort(key=lambda x: x[0])
        
        # ç”Ÿæˆæ‘˜è¦
        summary_sentences = [sentences[i] for i, _ in top_sentences]
        summary = ' '.join(summary_sentences)
        
        # æå–å…³é”®è¯
        keywords = list(word_freq.keys())[:10]
        
        # è®¡ç®—å‹ç¼©æ¯”
        compression_ratio = len(summary) / len(text)
        
        return SummaryResult(
            original_text=text,
            summary=summary,
            method='extractive',
            compression_ratio=compression_ratio,
            keywords=keywords,
            key_sentences=summary_sentences
        )

class TfIdfSummarizer:
    """åŸºäºTF-IDFçš„æ‘˜è¦å™¨"""
    
    def __init__(self):
        if not HAS_SKLEARN:
            raise ImportError("éœ€è¦å®‰è£…scikit-learnåº“")
        
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,  # è‡ªå®šä¹‰ä¸­æ–‡åœç”¨è¯
            ngram_range=(1, 2)
        )
    
    def summarize(self, text: str, num_sentences: int = 3) -> SummaryResult:
        """ä½¿ç”¨TF-IDFç”Ÿæˆæ‘˜è¦"""
        # å¥å­åˆ†è¯
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) <= num_sentences:
            summary = ' '.join(sentences)
            return SummaryResult(
                original_text=text,
                summary=summary,
                method='tfidf',
                compression_ratio=1.0,
                key_sentences=sentences
            )
        
        # è®¡ç®—TF-IDFçŸ©é˜µ
        try:
            tfidf_matrix = self.vectorizer.fit_transform(sentences)
        except ValueError:
            # å¦‚æœå¥å­å¤ªå°‘æˆ–è¯æ±‡å¤ªå°‘ï¼Œå›é€€åˆ°ç®€å•æ‘˜è¦
            return ExtractiveSummarizer().summarize(text, num_sentences)
        
        # è®¡ç®—å¥å­é‡è¦æ€§åˆ†æ•°
        sentence_scores = []
        for i in range(tfidf_matrix.shape[0]):
            score = np.sum(tfidf_matrix[i].toarray())
            sentence_scores.append((i, score))
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        top_sentences = sentence_scores[:num_sentences]
        
        # æŒ‰åŸæ–‡é¡ºåºæ’åˆ—
        top_sentences.sort(key=lambda x: x[0])
        
        # ç”Ÿæˆæ‘˜è¦
        summary_sentences = [sentences[i] for i, _ in top_sentences]
        summary = ' '.join(summary_sentences)
        
        # æå–å…³é”®è¯
        feature_names = self.vectorizer.get_feature_names_out()
        tfidf_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()
        keyword_indices = np.argsort(tfidf_scores)[::-1][:10]
        keywords = [feature_names[i] for i in keyword_indices]
        
        compression_ratio = len(summary) / len(text)
        
        return SummaryResult(
            original_text=text,
            summary=summary,
            method='tfidf',
            compression_ratio=compression_ratio,
            keywords=keywords,
            key_sentences=summary_sentences
        )

class TransformerSummarizer:
    """åŸºäºTransformerçš„æ‘˜è¦å™¨"""
    
    def __init__(self, model_name: str = "facebook/bart-large-cnn"):
        if not HAS_TRANSFORMERS:
            raise ImportError("éœ€è¦å®‰è£…transformersåº“")
        
        try:
            self.summarizer = pipeline(
                "summarization", 
                model=model_name,
                tokenizer=model_name
            )
            self.model_name = model_name
        except Exception as e:
            print(f"æ— æ³•åŠ è½½æ¨¡å‹ {model_name}ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹")
            try:
                self.summarizer = pipeline("summarization")
                self.model_name = "default"
            except Exception as e2:
                raise ImportError(f"æ— æ³•åˆå§‹åŒ–æ‘˜è¦æ¨¡å‹: {e2}")
    
    def summarize(self, text: str, max_length: int = 150, min_length: int = 30) -> SummaryResult:
        """ä½¿ç”¨Transformerç”Ÿæˆæ‘˜è¦"""
        # å¤„ç†æ–‡æœ¬é•¿åº¦é™åˆ¶
        max_input_length = 1024  # BARTçš„æœ€å¤§è¾“å…¥é•¿åº¦
        if len(text) > max_input_length:
            text = text[:max_input_length]
        
        try:
            # ç”Ÿæˆæ‘˜è¦
            summary_result = self.summarizer(
                text,
                max_length=max_length,
                min_length=min_length,
                do_sample=False
            )[0]
            
            summary = summary_result['summary_text']
            compression_ratio = len(summary) / len(text)
            
            return SummaryResult(
                original_text=text,
                summary=summary,
                method=f'transformer_{self.model_name}',
                compression_ratio=compression_ratio
            )
            
        except Exception as e:
            print(f"Transformeræ‘˜è¦å¤±è´¥: {e}")
            # å›é€€åˆ°æå–å¼æ‘˜è¦
            fallback_summarizer = ExtractiveSummarizer()
            return fallback_summarizer.summarize(text, 3)

class TopicModeling:
    """ä¸»é¢˜å»ºæ¨¡"""
    
    def __init__(self, n_topics: int = 5):
        if not HAS_SKLEARN:
            raise ImportError("éœ€è¦å®‰è£…scikit-learnåº“")
        
        self.n_topics = n_topics
        self.vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            min_df=2
        )
        self.lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            learning_method='batch'
        )
    
    def extract_topics(self, texts: List[str]) -> List[List[Tuple[str, float]]]:
        """æå–ä¸»é¢˜"""
        if len(texts) < self.n_topics:
            return []
        
        try:
            # å‘é‡åŒ–æ–‡æœ¬
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            
            # è®­ç»ƒLDAæ¨¡å‹
            self.lda.fit(tfidf_matrix)
            
            # æå–ä¸»é¢˜è¯
            feature_names = self.vectorizer.get_feature_names_out()
            topics = []
            
            for topic_idx, topic in enumerate(self.lda.components_):
                top_words_idx = topic.argsort()[::-1][:10]
                top_words = [(feature_names[i], topic[i]) for i in top_words_idx]
                topics.append(top_words)
            
            return topics
            
        except Exception as e:
            print(f"ä¸»é¢˜å»ºæ¨¡å¤±è´¥: {e}")
            return []

class SummarizationApp:
    """æ–‡æœ¬æ‘˜è¦æ•™è‚²åº”ç”¨"""
    
    def __init__(self):
        self.summarizers = {}
        self.topic_modeler = None
        
        # åˆå§‹åŒ–æ‘˜è¦å™¨
        self.summarizers['extractive'] = ExtractiveSummarizer()
        
        if HAS_SKLEARN:
            self.summarizers['tfidf'] = TfIdfSummarizer()
            self.topic_modeler = TopicModeling()
        
        if HAS_TRANSFORMERS:
            try:
                self.summarizers['transformer'] = TransformerSummarizer()
            except Exception as e:
                print(f"æ— æ³•åˆå§‹åŒ–Transformeræ‘˜è¦å™¨: {e}")
    
    def summarize_text(self, text: str, methods: List[str] = None, **kwargs) -> Dict[str, SummaryResult]:
        """ä½¿ç”¨å¤šç§æ–¹æ³•ç”Ÿæˆæ‘˜è¦"""
        if methods is None:
            methods = list(self.summarizers.keys())
        
        results = {}
        for method in methods:
            if method in self.summarizers:
                try:
                    if method == 'transformer':
                        result = self.summarizers[method].summarize(text, **kwargs)
                    else:
                        num_sentences = kwargs.get('num_sentences', 3)
                        result = self.summarizers[method].summarize(text, num_sentences)
                    results[method] = result
                except Exception as e:
                    print(f"æ–¹æ³• {method} æ‘˜è¦å¤±è´¥: {e}")
        
        return results
    
    def compare_summarization_methods(self, text: str):
        """æ¯”è¾ƒä¸åŒæ‘˜è¦æ–¹æ³•"""
        results = self.summarize_text(text)
        
        print(f"\nåŸæ–‡ ({len(text)} å­—ç¬¦):")
        print("=" * 60)
        print(text[:200] + "..." if len(text) > 200 else text)
        
        print(f"\næ‘˜è¦æ¯”è¾ƒ:")
        print("=" * 60)
        
        for method, result in results.items():
            print(f"\n{method.upper()} æ–¹æ³•:")
            print(f"æ‘˜è¦: {result.summary}")
            print(f"å‹ç¼©æ¯”: {result.compression_ratio:.2%}")
            
            if result.keywords:
                print(f"å…³é”®è¯: {', '.join(result.keywords[:5])}")
    
    def analyze_document_collection(self, texts: List[str], titles: List[str] = None):
        """åˆ†ææ–‡æ¡£é›†åˆ"""
        if titles is None:
            titles = [f"æ–‡æ¡£{i+1}" for i in range(len(texts))]
        
        print(f"\nğŸ“š æ–‡æ¡£é›†åˆåˆ†æ ({len(texts)} ä¸ªæ–‡æ¡£)")
        print("=" * 60)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆæ‘˜è¦
        all_summaries = []
        compression_ratios = []
        
        for i, text in enumerate(texts):
            if len(text) > 100:  # åªå¤„ç†è¾ƒé•¿çš„æ–‡æ¡£
                try:
                    result = self.summarizers['extractive'].summarize(text, 2)
                    all_summaries.append(result.summary)
                    compression_ratios.append(result.compression_ratio)
                    
                    print(f"\n{titles[i]}:")
                    print(f"æ‘˜è¦: {result.summary}")
                    if result.keywords:
                        print(f"å…³é”®è¯: {', '.join(result.keywords[:3])}")
                
                except Exception as e:
                    print(f"æ–‡æ¡£ {titles[i]} æ‘˜è¦å¤±è´¥: {e}")
        
        # ä¸»é¢˜å»ºæ¨¡
        if self.topic_modeler and len(texts) >= 3:
            print(f"\nğŸ“– ä¸»é¢˜åˆ†æ:")
            print("-" * 40)
            
            topics = self.topic_modeler.extract_topics(texts)
            for i, topic_words in enumerate(topics):
                if topic_words:
                    top_words = [word for word, _ in topic_words[:5]]
                    print(f"ä¸»é¢˜ {i+1}: {', '.join(top_words)}")
        
        # ç»Ÿè®¡åˆ†æ
        if compression_ratios:
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print("-" * 40)
            print(f"å¹³å‡å‹ç¼©æ¯”: {np.mean(compression_ratios):.2%}")
            print(f"å‹ç¼©æ¯”æ ‡å‡†å·®: {np.std(compression_ratios):.2%}")
            print(f"æœ€å°å‹ç¼©æ¯”: {np.min(compression_ratios):.2%}")
            print(f"æœ€å¤§å‹ç¼©æ¯”: {np.max(compression_ratios):.2%}")
    
    def visualize_summarization_analysis(self, texts: List[str], titles: List[str] = None):
        """å¯è§†åŒ–æ‘˜è¦åˆ†æ"""
        if titles is None:
            titles = [f"æ–‡æ¡£{i+1}" for i in range(len(texts))]
        
        # æ”¶é›†æ•°æ®
        results_data = []
        
        for i, text in enumerate(texts):
            if len(text) > 50:
                results = self.summarize_text(text)
                for method, result in results.items():
                    results_data.append({
                        'document': titles[i],
                        'method': method,
                        'original_length': len(text),
                        'summary_length': len(result.summary),
                        'compression_ratio': result.compression_ratio,
                        'keyword_count': len(result.keywords) if result.keywords else 0
                    })
        
        if not results_data:
            print("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå¯è§†åŒ–")
            return
        
        df = pd.DataFrame(results_data)
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. å‹ç¼©æ¯”åˆ†å¸ƒ
        if 'compression_ratio' in df.columns:
            df.boxplot(column='compression_ratio', by='method', ax=axes[0, 0])
            axes[0, 0].set_title('å„æ–¹æ³•å‹ç¼©æ¯”åˆ†å¸ƒ')
            axes[0, 0].set_xlabel('æ–¹æ³•')
            axes[0, 0].set_ylabel('å‹ç¼©æ¯”')
        
        # 2. åŸæ–‡é•¿åº¦ vs æ‘˜è¦é•¿åº¦
        if 'original_length' in df.columns and 'summary_length' in df.columns:
            for method in df['method'].unique():
                method_data = df[df['method'] == method]
                axes[0, 1].scatter(method_data['original_length'], 
                                 method_data['summary_length'], 
                                 label=method, alpha=0.7)
            axes[0, 1].set_xlabel('åŸæ–‡é•¿åº¦')
            axes[0, 1].set_ylabel('æ‘˜è¦é•¿åº¦')
            axes[0, 1].set_title('åŸæ–‡é•¿åº¦ vs æ‘˜è¦é•¿åº¦')
            axes[0, 1].legend()
        
        # 3. æ–¹æ³•æ¯”è¾ƒ
        if len(df['method'].unique()) > 1:
            method_stats = df.groupby('method')['compression_ratio'].agg(['mean', 'std'])
            method_stats['mean'].plot(kind='bar', ax=axes[1, 0], 
                                    yerr=method_stats['std'], capsize=4)
            axes[1, 0].set_title('å„æ–¹æ³•å¹³å‡å‹ç¼©æ¯”')
            axes[1, 0].set_ylabel('å‹ç¼©æ¯”')
            axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. å…³é”®è¯æ•°é‡åˆ†å¸ƒ
        if 'keyword_count' in df.columns:
            df['keyword_count'].hist(bins=10, ax=axes[1, 1], alpha=0.7)
            axes[1, 1].set_title('å…³é”®è¯æ•°é‡åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('å…³é”®è¯æ•°é‡')
            axes[1, 1].set_ylabel('é¢‘æ¬¡')
        
        plt.tight_layout()
        plt.show()
    
    def evaluate_summary_quality(self, original: str, summary: str) -> Dict[str, float]:
        """è¯„ä¼°æ‘˜è¦è´¨é‡"""
        metrics = {}
        
        # 1. å‹ç¼©æ¯”
        metrics['compression_ratio'] = len(summary) / len(original)
        
        # 2. ä¿¡æ¯å¯†åº¦ (å…³é”®è¯ä¿ç•™ç‡)
        original_words = set(re.findall(r'\w+', original.lower()))
        summary_words = set(re.findall(r'\w+', summary.lower()))
        
        if original_words:
            metrics['word_coverage'] = len(summary_words & original_words) / len(original_words)
        else:
            metrics['word_coverage'] = 0.0
        
        # 3. è¯­ä¹‰ç›¸ä¼¼åº¦ (å¦‚æœå¯ç”¨)
        if hasattr(self, 'vectorizer') and HAS_SKLEARN:
            try:
                vectorizer = TfidfVectorizer()
                tfidf_matrix = vectorizer.fit_transform([original, summary])
                similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
                metrics['semantic_similarity'] = similarity
            except:
                metrics['semantic_similarity'] = 0.0
        
        # 4. å¯è¯»æ€§åˆ†æ•° (ç®€åŒ–ç‰ˆ)
        sentence_count = len(re.split(r'[ã€‚ï¼ï¼Ÿ]', summary))
        word_count = len(re.findall(r'\w+', summary))
        if sentence_count > 0:
            metrics['readability'] = word_count / sentence_count
        else:
            metrics['readability'] = 0.0
        
        return metrics
    
    def run_interactive_demo(self):
        """è¿è¡Œäº¤äº’å¼æ¼”ç¤º"""
        print("\nğŸ“ æ–‡æœ¬æ‘˜è¦æ•™è‚²åº”ç”¨")
        print("=" * 50)
        print("å¯ç”¨çš„æ‘˜è¦æ–¹æ³•:")
        for i, method in enumerate(self.summarizers.keys(), 1):
            print(f"  {i}. {method}")
        
        while True:
            print("\né€‰æ‹©æ“ä½œ:")
            print("1. å•æ–‡æœ¬æ‘˜è¦")
            print("2. æ–¹æ³•æ¯”è¾ƒ")
            print("3. æ–‡æ¡£é›†åˆåˆ†æ")
            print("4. æ‘˜è¦è´¨é‡è¯„ä¼°")
            print("5. åŠ è½½ç¤ºä¾‹æ–‡æ¡£")
            print("0. é€€å‡º")
            
            choice = input("\nè¯·é€‰æ‹© (0-5): ").strip()
            
            if choice == '0':
                break
            elif choice == '1':
                text = input("è¯·è¾“å…¥è¦æ‘˜è¦çš„æ–‡æœ¬: ").strip()
                if text:
                    method = input(f"é€‰æ‹©æ–¹æ³• ({'/'.join(self.summarizers.keys())}): ").strip()
                    if method in self.summarizers:
                        result = self.summarizers[method].summarize(text)
                        print(f"\næ‘˜è¦ç»“æœ:")
                        print(f"æ‘˜è¦: {result.summary}")
                        print(f"å‹ç¼©æ¯”: {result.compression_ratio:.2%}")
                        if result.keywords:
                            print(f"å…³é”®è¯: {', '.join(result.keywords[:5])}")
                    else:
                        print("æ— æ•ˆçš„æ–¹æ³•")
            
            elif choice == '2':
                text = input("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬: ").strip()
                if text:
                    self.compare_summarization_methods(text)
            
            elif choice == '3':
                print("è¯·è¾“å…¥å¤šä¸ªæ–‡æ¡£ï¼Œæ¯è¡Œä¸€ä¸ª (è¾“å…¥ç©ºè¡Œç»“æŸ):")
                texts = []
                while True:
                    line = input().strip()
                    if not line:
                        break
                    texts.append(line)
                
                if texts:
                    self.analyze_document_collection(texts)
                    
                    visualize = input("æ˜¯å¦è¿›è¡Œå¯è§†åŒ–åˆ†æ? (y/n): ").strip().lower()
                    if visualize == 'y':
                        self.visualize_summarization_analysis(texts)
            
            elif choice == '4':
                original = input("è¯·è¾“å…¥åŸæ–‡: ").strip()
                summary = input("è¯·è¾“å…¥æ‘˜è¦: ").strip()
                
                if original and summary:
                    metrics = self.evaluate_summary_quality(original, summary)
                    print("\næ‘˜è¦è´¨é‡è¯„ä¼°:")
                    for metric, value in metrics.items():
                        print(f"{metric}: {value:.3f}")
            
            elif choice == '5':
                try:
                    documents = load_documents('data/sample_documents.json')
                    texts = [doc['content'] for doc in documents[:5]]
                    titles = [doc.get('title', f"æ–‡æ¡£{i+1}") for i, doc in enumerate(documents[:5])]
                    
                    print(f"åŠ è½½äº† {len(texts)} ä¸ªæ–‡æ¡£")
                    self.analyze_document_collection(texts, titles)
                    
                    visualize = input("æ˜¯å¦è¿›è¡Œå¯è§†åŒ–åˆ†æ? (y/n): ").strip().lower()
                    if visualize == 'y':
                        self.visualize_summarization_analysis(texts, titles)
                        
                except Exception as e:
                    print(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("åˆå§‹åŒ–æ–‡æœ¬æ‘˜è¦åº”ç”¨...")
    
    app = SummarizationApp()
    
    # ç¤ºä¾‹æ–‡æœ¬
    sample_text = """
    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œ
    å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼ä½œå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€
    è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œ
    åº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ï¼Œå¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚
    
    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›æ€§èƒ½ï¼Œ
    è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚
    è¿™äº›æŠ€æœ¯åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚
    
    éšç€å¤§æ•°æ®å’Œäº‘è®¡ç®—æŠ€æœ¯çš„å‘å±•ï¼Œäººå·¥æ™ºèƒ½çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œ
    ä»æ™ºèƒ½å®¶å±…åˆ°åŒ»ç–—è¯Šæ–­ï¼Œäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚ç„¶è€Œï¼Œäººå·¥æ™ºèƒ½çš„å‘å±•ä¹Ÿå¸¦æ¥äº†
    ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚å°±ä¸šå½±å“ã€éšç§ä¿æŠ¤ã€ç®—æ³•åè§ç­‰é—®é¢˜éœ€è¦æˆ‘ä»¬è®¤çœŸå¯¹å¾…ã€‚
    """
    
    print("\nğŸ¯ æ¼”ç¤º: å¤šæ–¹æ³•æ–‡æœ¬æ‘˜è¦")
    print("=" * 40)
    
    # æ¯”è¾ƒä¸åŒæ‘˜è¦æ–¹æ³•
    app.compare_summarization_methods(sample_text)
    
    # è¿è¡Œäº¤äº’å¼æ¼”ç¤º
    app.run_interactive_demo()

if __name__ == "__main__":
    main()
