{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1325ab2e",
   "metadata": {},
   "source": [
    "# å‘é‡æœç´¢åŸºç¡€æ•™ç¨‹\n",
    "# Vector Search Basics Tutorial\n",
    "\n",
    "è¿™ä¸ªç¬”è®°æœ¬å°†ä»‹ç»å‘é‡æœç´¢çš„åŸºæœ¬æ¦‚å¿µå’Œæ“ä½œã€‚\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£å‘é‡çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- å­¦ä¹ å‘é‡æ“ä½œå’Œç›¸ä¼¼åº¦è®¡ç®—\n",
    "- ä½¿ç”¨åŸºæœ¬å‘é‡æœç´¢åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c75841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.basic_vector_search import BasicVectorSearch\n",
    "from src.utils import visualize_vectors, plot_similarity_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ca3b2",
   "metadata": {},
   "source": [
    "## 1. å‘é‡åŸºç¡€\n",
    "\n",
    "å‘é‡æ˜¯æœºå™¨å­¦ä¹ å’Œä¿¡æ¯æ£€ç´¢çš„åŸºç¡€ã€‚è®©æˆ‘ä»¬ä»ç®€å•çš„å‘é‡å¼€å§‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€äº›ç¤ºä¾‹å‘é‡\n",
    "vector1 = np.array([1, 2, 3])\n",
    "vector2 = np.array([4, 5, 6])\n",
    "vector3 = np.array([1, 1, 1])\n",
    "\n",
    "print(\"å‘é‡1:\", vector1)\n",
    "print(\"å‘é‡2:\", vector2)\n",
    "print(\"å‘é‡3:\", vector3)\n",
    "\n",
    "# å‘é‡çš„ç»´åº¦\n",
    "print(f\"\\nå‘é‡ç»´åº¦: {len(vector1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f443eb",
   "metadata": {},
   "source": [
    "## 2. åˆå§‹åŒ–å‘é‡æœç´¢ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå‘é‡æœç´¢å®ä¾‹\n",
    "search_engine = BasicVectorSearch()\n",
    "\n",
    "# æ·»åŠ å‘é‡åˆ°ç´¢å¼•\n",
    "vectors = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [1, 1, 1],\n",
    "    [2, 3, 4],\n",
    "    [0, 1, 2]\n",
    "])\n",
    "\n",
    "search_engine.add_vectors(vectors)\n",
    "print(f\"å·²æ·»åŠ  {len(vectors)} ä¸ªå‘é‡åˆ°æœç´¢å¼•æ“\")\n",
    "print(f\"ç´¢å¼•å¤§å°: {search_engine.get_index_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27178b14",
   "metadata": {},
   "source": [
    "## 3. ç›¸ä¼¼åº¦è®¡ç®—\n",
    "\n",
    "è®©æˆ‘ä»¬æ¢ç´¢ä¸åŒçš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥è¯¢å‘é‡\n",
    "query_vector = np.array([2, 2, 2])\n",
    "\n",
    "# ä½¿ç”¨ä¸åŒç›¸ä¼¼åº¦åº¦é‡è¿›è¡Œæœç´¢\n",
    "print(\"æŸ¥è¯¢å‘é‡:\", query_vector)\n",
    "print(\"\\n=== ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢ ===\")\n",
    "cosine_results = search_engine.search(query_vector, k=3, metric='cosine')\n",
    "for i, (idx, similarity) in enumerate(cosine_results):\n",
    "    print(f\"{i+1}. å‘é‡ç´¢å¼• {idx}, ç›¸ä¼¼åº¦: {similarity:.3f}, å‘é‡: {vectors[idx]}\")\n",
    "\n",
    "print(\"\\n=== æ¬§å‡ é‡Œå¾—è·ç¦»æœç´¢ ===\")\n",
    "euclidean_results = search_engine.search(query_vector, k=3, metric='euclidean')\n",
    "for i, (idx, distance) in enumerate(euclidean_results):\n",
    "    print(f\"{i+1}. å‘é‡ç´¢å¼• {idx}, è·ç¦»: {distance:.3f}, å‘é‡: {vectors[idx]}\")\n",
    "\n",
    "print(\"\\n=== æ›¼å“ˆé¡¿è·ç¦»æœç´¢ ===\")\n",
    "manhattan_results = search_engine.search(query_vector, k=3, metric='manhattan')\n",
    "for i, (idx, distance) in enumerate(manhattan_results):\n",
    "    print(f\"{i+1}. å‘é‡ç´¢å¼• {idx}, è·ç¦»: {distance:.3f}, å‘é‡: {vectors[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b2ac4",
   "metadata": {},
   "source": [
    "## 4. å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a35e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å‘é‡ï¼ˆä»…ä½¿ç”¨å‰ä¸¤ä¸ªç»´åº¦ï¼‰\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ç»˜åˆ¶ç´¢å¼•ä¸­çš„å‘é‡\n",
    "plt.scatter(vectors[:, 0], vectors[:, 1], c='blue', s=100, label='ç´¢å¼•å‘é‡', alpha=0.7)\n",
    "for i, vec in enumerate(vectors):\n",
    "    plt.annotate(f'V{i}', (vec[0], vec[1]), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# ç»˜åˆ¶æŸ¥è¯¢å‘é‡\n",
    "plt.scatter(query_vector[0], query_vector[1], c='red', s=150, label='æŸ¥è¯¢å‘é‡', marker='*')\n",
    "plt.annotate('Query', (query_vector[0], query_vector[1]), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('ç»´åº¦ 1')\n",
    "plt.ylabel('ç»´åº¦ 2')\n",
    "plt.title('å‘é‡åˆ†å¸ƒå¯è§†åŒ–')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6dea6c",
   "metadata": {},
   "source": [
    "## 5. æ€§èƒ½æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ›´å¤§çš„æ•°æ®é›†è¿›è¡Œæ€§èƒ½æµ‹è¯•\n",
    "large_vectors = np.random.rand(1000, 10)  # 1000ä¸ª10ç»´å‘é‡\n",
    "large_search_engine = BasicVectorSearch()\n",
    "large_search_engine.add_vectors(large_vectors)\n",
    "\n",
    "# æ€§èƒ½åŸºå‡†æµ‹è¯•\n",
    "query = np.random.rand(10)\n",
    "benchmark_results = large_search_engine.benchmark_search(query, k=10, num_queries=100)\n",
    "\n",
    "print(\"æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:\")\n",
    "for metric, stats in benchmark_results.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  å¹³å‡æ—¶é—´: {stats['avg_time']:.4f}ç§’\")\n",
    "    print(f\"  æœ€å°æ—¶é—´: {stats['min_time']:.4f}ç§’\")\n",
    "    print(f\"  æœ€å¤§æ—¶é—´: {stats['max_time']:.4f}ç§’\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c77b0e",
   "metadata": {},
   "source": [
    "## 6. æ€»ç»“\n",
    "\n",
    "åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š\n",
    "\n",
    "1. **å‘é‡åŸºç¡€**: ä»€ä¹ˆæ˜¯å‘é‡ä»¥åŠå¦‚ä½•è¡¨ç¤º\n",
    "2. **å‘é‡æœç´¢**: å¦‚ä½•æ„å»ºå’Œä½¿ç”¨åŸºæœ¬å‘é‡æœç´¢ç³»ç»Ÿ\n",
    "3. **ç›¸ä¼¼åº¦åº¦é‡**: ä¸åŒç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•çš„å·®å¼‚\n",
    "4. **å¯è§†åŒ–**: å¦‚ä½•å¯è§†åŒ–å‘é‡æ•°æ®\n",
    "5. **æ€§èƒ½**: æœç´¢æ€§èƒ½çš„æµ‹é‡å’Œæ¯”è¾ƒ\n",
    "\n",
    "ä¸‹ä¸€ä¸ªç¬”è®°æœ¬å°†æ·±å…¥æ¢è®¨å„ç§ç›¸ä¼¼åº¦åº¦é‡çš„æ•°å­¦åŸç†å’Œåº”ç”¨åœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b362614",
   "metadata": {},
   "source": [
    "# å‘é‡æœç´¢åŸºç¡€æ•™ç¨‹\n",
    "# Vector Search Basics Tutorial\n",
    "\n",
    "æœ¬æ•™ç¨‹å°†å¸¦æ‚¨äº†è§£å‘é‡æœç´¢çš„åŸºç¡€æ¦‚å¿µï¼ŒåŒ…æ‹¬:\n",
    "- å‘é‡è¡¨ç¤º\n",
    "- ç›¸ä¼¼åº¦åº¦é‡\n",
    "- åŸºç¡€æœç´¢ç®—æ³•\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19ec77",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# æ·»åŠ æºä»£ç è·¯å¾„\n",
    "sys.path.append('../src')\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾æ ·å¼\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bf382",
   "metadata": {},
   "source": [
    "## 2. ä»€ä¹ˆæ˜¯å‘é‡ï¼Ÿ\n",
    "\n",
    "å‘é‡æ˜¯æ•°å­¦ä¸­çš„åŸºç¡€æ¦‚å¿µï¼Œåœ¨æœºå™¨å­¦ä¹ å’Œä¿¡æ¯æ£€ç´¢ä¸­ï¼Œæˆ‘ä»¬ç”¨å‘é‡æ¥è¡¨ç¤ºæ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç®€å•çš„2Då‘é‡ç¤ºä¾‹\n",
    "vector_a = np.array([3, 4])\n",
    "vector_b = np.array([1, 2])\n",
    "vector_c = np.array([-2, 3])\n",
    "\n",
    "print(f\"å‘é‡ A: {vector_a}\")\n",
    "print(f\"å‘é‡ B: {vector_b}\")\n",
    "print(f\"å‘é‡ C: {vector_c}\")\n",
    "\n",
    "# å¯è§†åŒ–å‘é‡\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.quiver(0, 0, vector_a[0], vector_a[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='å‘é‡ A')\n",
    "plt.quiver(0, 0, vector_b[0], vector_b[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.005, label='å‘é‡ B')\n",
    "plt.quiver(0, 0, vector_c[0], vector_c[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.005, label='å‘é‡ C')\n",
    "\n",
    "plt.xlim(-3, 5)\n",
    "plt.ylim(-1, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.title('2D å‘é‡å¯è§†åŒ–')\n",
    "plt.xlabel('X è½´')\n",
    "plt.ylabel('Y è½´')\n",
    "plt.show()\n",
    "\n",
    "# è®¡ç®—å‘é‡çš„é•¿åº¦ï¼ˆL2èŒƒæ•°ï¼‰\n",
    "print(f\"\\nå‘é‡é•¿åº¦:\")\n",
    "print(f\"||A|| = {np.linalg.norm(vector_a):.3f}\")\n",
    "print(f\"||B|| = {np.linalg.norm(vector_b):.3f}\")\n",
    "print(f\"||C|| = {np.linalg.norm(vector_c):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9802d",
   "metadata": {},
   "source": [
    "## 3. ç›¸ä¼¼åº¦åº¦é‡\n",
    "\n",
    "ç›¸ä¼¼åº¦åº¦é‡æ˜¯å‘é‡æœç´¢çš„æ ¸å¿ƒã€‚è®©æˆ‘ä»¬å­¦ä¹ å‡ ç§å¸¸ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "def euclidean_distance(v1, v2):\n",
    "    \"\"\"è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»\"\"\"\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "\n",
    "def manhattan_distance(v1, v2):\n",
    "    \"\"\"è®¡ç®—æ›¼å“ˆé¡¿è·ç¦»\"\"\"\n",
    "    return np.sum(np.abs(v1 - v2))\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    \"\"\"è®¡ç®—ç‚¹ç§¯\"\"\"\n",
    "    return np.dot(v1, v2)\n",
    "\n",
    "# è®¡ç®—å‘é‡é—´çš„ç›¸ä¼¼åº¦/è·ç¦»\n",
    "print(\"å‘é‡ A å’Œ B ä¹‹é—´çš„ç›¸ä¼¼åº¦/è·ç¦»:\")\n",
    "print(f\"ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_similarity(vector_a, vector_b):.4f}\")\n",
    "print(f\"æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_distance(vector_a, vector_b):.4f}\")\n",
    "print(f\"æ›¼å“ˆé¡¿è·ç¦»: {manhattan_distance(vector_a, vector_b):.4f}\")\n",
    "print(f\"ç‚¹ç§¯: {dot_product(vector_a, vector_b):.4f}\")\n",
    "\n",
    "print(\"\\nå‘é‡ A å’Œ C ä¹‹é—´çš„ç›¸ä¼¼åº¦/è·ç¦»:\")\n",
    "print(f\"ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_similarity(vector_a, vector_c):.4f}\")\n",
    "print(f\"æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_distance(vector_a, vector_c):.4f}\")\n",
    "print(f\"æ›¼å“ˆé¡¿è·ç¦»: {manhattan_distance(vector_a, vector_c):.4f}\")\n",
    "print(f\"ç‚¹ç§¯: {dot_product(vector_a, vector_c):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962e11e",
   "metadata": {},
   "source": [
    "## 4. ç›¸ä¼¼åº¦åº¦é‡çš„å‡ ä½•è§£é‡Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49cb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ›´å¤šå‘é‡æ¥å±•ç¤ºä¸åŒçš„ç›¸ä¼¼åº¦\n",
    "angles = np.linspace(0, 2*np.pi, 8, endpoint=False)\n",
    "vectors = np.array([[np.cos(angle), np.sin(angle)] for angle in angles])\n",
    "reference_vector = np.array([1, 0])  # å‚è€ƒå‘é‡\n",
    "\n",
    "# è®¡ç®—æ‰€æœ‰å‘é‡ä¸å‚è€ƒå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "similarities = [cosine_similarity(reference_vector, v) for v in vectors]\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# å·¦å›¾: å‘é‡å¯è§†åŒ–\n",
    "ax1.quiver(0, 0, reference_vector[0], reference_vector[1], \n",
    "           angles='xy', scale_units='xy', scale=1, color='red', width=0.01, label='å‚è€ƒå‘é‡')\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(vectors)))\n",
    "for i, (v, sim, color) in enumerate(zip(vectors, similarities, colors)):\n",
    "    ax1.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=color, width=0.005, label=f'å‘é‡ {i+1} (sim={sim:.3f})')\n",
    "\n",
    "ax1.set_xlim(-1.5, 1.5)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('å‘é‡ä¸å‚è€ƒå‘é‡çš„è§’åº¦å…³ç³»')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# å³å›¾: ç›¸ä¼¼åº¦åˆ†å¸ƒ\n",
    "ax2.bar(range(1, len(similarities)+1), similarities, color=colors)\n",
    "ax2.set_xlabel('å‘é‡ç¼–å·')\n",
    "ax2.set_ylabel('ä½™å¼¦ç›¸ä¼¼åº¦')\n",
    "ax2.set_title('ä½™å¼¦ç›¸ä¼¼åº¦åˆ†å¸ƒ')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"è§‚å¯Ÿ:\")\n",
    "print(\"- ä½™å¼¦ç›¸ä¼¼åº¦è¡¡é‡å‘é‡ä¹‹é—´çš„è§’åº¦\")\n",
    "print(\"- ç›¸ä¼¼åº¦ä¸º1è¡¨ç¤ºå‘é‡æ–¹å‘å®Œå…¨ç›¸åŒ\")\n",
    "print(\"- ç›¸ä¼¼åº¦ä¸º0è¡¨ç¤ºå‘é‡å‚ç›´\")\n",
    "print(\"- ç›¸ä¼¼åº¦ä¸º-1è¡¨ç¤ºå‘é‡æ–¹å‘å®Œå…¨ç›¸å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3da51f",
   "metadata": {},
   "source": [
    "## 5. é«˜ç»´å‘é‡ç¤ºä¾‹\n",
    "\n",
    "åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¤„ç†é«˜ç»´å‘é‡ï¼ˆå‡ ååˆ°å‡ åƒç»´ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45625a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆé«˜ç»´å‘é‡ç¤ºä¾‹\n",
    "np.random.seed(42)  # ä¿è¯ç»“æœå¯é‡ç°\n",
    "\n",
    "# åˆ›å»ºä¸€äº›é«˜ç»´å‘é‡\n",
    "vector_dim = 100\n",
    "n_vectors = 1000\n",
    "\n",
    "# æ–¹æ³•1: éšæœºå‘é‡\n",
    "random_vectors = np.random.rand(n_vectors, vector_dim)\n",
    "\n",
    "# æ–¹æ³•2: æ­£æ€åˆ†å¸ƒå‘é‡\n",
    "normal_vectors = np.random.randn(n_vectors, vector_dim)\n",
    "\n",
    "# æ–¹æ³•3: ç¨€ç–å‘é‡ï¼ˆå¤§éƒ¨åˆ†å…ƒç´ ä¸º0ï¼‰\n",
    "sparse_vectors = np.zeros((n_vectors, vector_dim))\n",
    "for i in range(n_vectors):\n",
    "    # åªæœ‰10%çš„å…ƒç´ éé›¶\n",
    "    non_zero_indices = np.random.choice(vector_dim, size=vector_dim//10, replace=False)\n",
    "    sparse_vectors[i, non_zero_indices] = np.random.randn(len(non_zero_indices))\n",
    "\n",
    "print(f\"åˆ›å»ºäº† {n_vectors} ä¸ª {vector_dim} ç»´å‘é‡\")\n",
    "print(f\"éšæœºå‘é‡èŒƒå›´: [{np.min(random_vectors):.3f}, {np.max(random_vectors):.3f}]\")\n",
    "print(f\"æ­£æ€åˆ†å¸ƒå‘é‡èŒƒå›´: [{np.min(normal_vectors):.3f}, {np.max(normal_vectors):.3f}]\")\n",
    "print(f\"ç¨€ç–å‘é‡éé›¶å…ƒç´ æ¯”ä¾‹: {np.count_nonzero(sparse_vectors) / sparse_vectors.size:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯”è¾ƒä¸åŒå‘é‡ç±»å‹çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ\n",
    "def compute_pairwise_similarities(vectors, sample_size=100):\n",
    "    \"\"\"è®¡ç®—æˆå¯¹ç›¸ä¼¼åº¦\"\"\"\n",
    "    # ä¸ºäº†æ•ˆç‡ï¼Œåªè®¡ç®—å‰sample_sizeä¸ªå‘é‡\n",
    "    sample_vectors = vectors[:sample_size]\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        for j in range(i+1, sample_size):\n",
    "            sim = cosine_similarity(sample_vectors[i], sample_vectors[j])\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# è®¡ç®—ç›¸ä¼¼åº¦åˆ†å¸ƒ\n",
    "random_sims = compute_pairwise_similarities(random_vectors)\n",
    "normal_sims = compute_pairwise_similarities(normal_vectors)\n",
    "sparse_sims = compute_pairwise_similarities(sparse_vectors)\n",
    "\n",
    "# å¯è§†åŒ–ç›¸ä¼¼åº¦åˆ†å¸ƒ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(random_sims, bins=30, alpha=0.7, color='blue')\n",
    "axes[0].set_title('éšæœºå‘é‡ç›¸ä¼¼åº¦åˆ†å¸ƒ')\n",
    "axes[0].set_xlabel('ä½™å¼¦ç›¸ä¼¼åº¦')\n",
    "axes[0].set_ylabel('é¢‘æ¬¡')\n",
    "axes[0].axvline(np.mean(random_sims), color='red', linestyle='--', label=f'å‡å€¼: {np.mean(random_sims):.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(normal_sims, bins=30, alpha=0.7, color='green')\n",
    "axes[1].set_title('æ­£æ€åˆ†å¸ƒå‘é‡ç›¸ä¼¼åº¦åˆ†å¸ƒ')\n",
    "axes[1].set_xlabel('ä½™å¼¦ç›¸ä¼¼åº¦')\n",
    "axes[1].set_ylabel('é¢‘æ¬¡')\n",
    "axes[1].axvline(np.mean(normal_sims), color='red', linestyle='--', label=f'å‡å€¼: {np.mean(normal_sims):.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(sparse_sims, bins=30, alpha=0.7, color='orange')\n",
    "axes[2].set_title('ç¨€ç–å‘é‡ç›¸ä¼¼åº¦åˆ†å¸ƒ')\n",
    "axes[2].set_xlabel('ä½™å¼¦ç›¸ä¼¼åº¦')\n",
    "axes[2].set_ylabel('é¢‘æ¬¡')\n",
    "axes[2].axvline(np.mean(sparse_sims), color='red', linestyle='--', label=f'å‡å€¼: {np.mean(sparse_sims):.3f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"è§‚å¯Ÿ:\")\n",
    "print(f\"- éšæœºå‘é‡å¹³å‡ç›¸ä¼¼åº¦: {np.mean(random_sims):.4f}\")\n",
    "print(f\"- æ­£æ€åˆ†å¸ƒå‘é‡å¹³å‡ç›¸ä¼¼åº¦: {np.mean(normal_sims):.4f}\")\n",
    "print(f\"- ç¨€ç–å‘é‡å¹³å‡ç›¸ä¼¼åº¦: {np.mean(sparse_sims):.4f}\")\n",
    "print(\"\\néšç€ç»´åº¦å¢åŠ ï¼Œéšæœºå‘é‡è¶‹å‘äºæ­£äº¤ï¼ˆç›¸ä¼¼åº¦æ¥è¿‘0ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380cb181",
   "metadata": {},
   "source": [
    "## 6. åŸºç¡€å‘é‡æœç´¢å®ç°\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„å‘é‡æœç´¢ç®—æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09202a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorSearch:\n",
    "    def __init__(self):\n",
    "        self.vectors = None\n",
    "        self.labels = None\n",
    "    \n",
    "    def add_vectors(self, vectors, labels=None):\n",
    "        \"\"\"æ·»åŠ å‘é‡åˆ°æœç´¢åº“\"\"\"\n",
    "        self.vectors = np.array(vectors)\n",
    "        if labels is None:\n",
    "            self.labels = [f\"å‘é‡_{i}\" for i in range(len(vectors))]\n",
    "        else:\n",
    "            self.labels = labels\n",
    "        print(f\"æ·»åŠ äº† {len(vectors)} ä¸ªå‘é‡\")\n",
    "    \n",
    "    def search(self, query_vector, top_k=5):\n",
    "        \"\"\"æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡\"\"\"\n",
    "        if self.vectors is None:\n",
    "            raise ValueError(\"è¯·å…ˆæ·»åŠ å‘é‡\")\n",
    "        \n",
    "        # è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            sim = cosine_similarity(query_vector, vector)\n",
    "            similarities.append((i, sim, self.labels[i]))\n",
    "        \n",
    "        # æŒ‰ç›¸ä¼¼åº¦æ’åº\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "\n",
    "# æµ‹è¯•æœç´¢åŠŸèƒ½\n",
    "search_engine = SimpleVectorSearch()\n",
    "\n",
    "# ä½¿ç”¨å‰é¢ç”Ÿæˆçš„éšæœºå‘é‡\n",
    "sample_vectors = random_vectors[:100]  # ä½¿ç”¨100ä¸ªå‘é‡ä½œä¸ºæœç´¢åº“\n",
    "sample_labels = [f\"æ–‡æ¡£_{i:03d}\" for i in range(100)]\n",
    "\n",
    "search_engine.add_vectors(sample_vectors, sample_labels)\n",
    "\n",
    "# åˆ›å»ºæŸ¥è¯¢å‘é‡\n",
    "query = np.random.rand(vector_dim)\n",
    "print(f\"\\næŸ¥è¯¢å‘é‡ç»´åº¦: {len(query)}\")\n",
    "\n",
    "# æ‰§è¡Œæœç´¢\n",
    "results = search_engine.search(query, top_k=5)\n",
    "\n",
    "print(\"\\næœç´¢ç»“æœ (Top 5):\")\n",
    "for rank, (idx, similarity, label) in enumerate(results, 1):\n",
    "    print(f\"{rank:2d}. {label}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2b68a",
   "metadata": {},
   "source": [
    "## 7. å‘é‡å½’ä¸€åŒ–çš„é‡è¦æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5276a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸åŒå°ºåº¦çš„å‘é‡\n",
    "vector_small = np.array([0.1, 0.2])\n",
    "vector_large = np.array([10, 20])\n",
    "vector_medium = np.array([1, 2])\n",
    "\n",
    "print(\"åŸå§‹å‘é‡:\")\n",
    "print(f\"å°å‘é‡: {vector_small}, é•¿åº¦: {np.linalg.norm(vector_small):.3f}\")\n",
    "print(f\"ä¸­å‘é‡: {vector_medium}, é•¿åº¦: {np.linalg.norm(vector_medium):.3f}\")\n",
    "print(f\"å¤§å‘é‡: {vector_large}, é•¿åº¦: {np.linalg.norm(vector_large):.3f}\")\n",
    "\n",
    "# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ³¨æ„å®ƒä»¬æ–¹å‘ç›¸åŒï¼‰\n",
    "print(\"\\nä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ–¹å‘ç›¸åŒçš„å‘é‡ï¼‰:\")\n",
    "print(f\"å°-ä¸­: {cosine_similarity(vector_small, vector_medium):.4f}\")\n",
    "print(f\"å°-å¤§: {cosine_similarity(vector_small, vector_large):.4f}\")\n",
    "print(f\"ä¸­-å¤§: {cosine_similarity(vector_medium, vector_large):.4f}\")\n",
    "\n",
    "# è®¡ç®—ç‚¹ç§¯ï¼ˆå—å‘é‡é•¿åº¦å½±å“ï¼‰\n",
    "print(\"\\nç‚¹ç§¯ï¼ˆå—é•¿åº¦å½±å“ï¼‰:\")\n",
    "print(f\"å°-ä¸­: {np.dot(vector_small, vector_medium):.4f}\")\n",
    "print(f\"å°-å¤§: {np.dot(vector_small, vector_large):.4f}\")\n",
    "print(f\"ä¸­-å¤§: {np.dot(vector_medium, vector_large):.4f}\")\n",
    "\n",
    "# å½’ä¸€åŒ–å‘é‡\n",
    "def normalize(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "vector_small_norm = normalize(vector_small)\n",
    "vector_medium_norm = normalize(vector_medium)\n",
    "vector_large_norm = normalize(vector_large)\n",
    "\n",
    "print(\"\\nå½’ä¸€åŒ–åçš„å‘é‡:\")\n",
    "print(f\"å°å‘é‡: {vector_small_norm}, é•¿åº¦: {np.linalg.norm(vector_small_norm):.3f}\")\n",
    "print(f\"ä¸­å‘é‡: {vector_medium_norm}, é•¿åº¦: {np.linalg.norm(vector_medium_norm):.3f}\")\n",
    "print(f\"å¤§å‘é‡: {vector_large_norm}, é•¿åº¦: {np.linalg.norm(vector_large_norm):.3f}\")\n",
    "\n",
    "print(\"\\nå½’ä¸€åŒ–åçš„ç‚¹ç§¯ï¼ˆç­‰äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰:\")\n",
    "print(f\"å°-ä¸­: {np.dot(vector_small_norm, vector_medium_norm):.4f}\")\n",
    "print(f\"å°-å¤§: {np.dot(vector_small_norm, vector_large_norm):.4f}\")\n",
    "print(f\"ä¸­-å¤§: {np.dot(vector_medium_norm, vector_large_norm):.4f}\")\n",
    "\n",
    "print(\"\\nç»“è®º: å¯¹äºæ–¹å‘ç›¸åŒä½†é•¿åº¦ä¸åŒçš„å‘é‡ï¼Œå½’ä¸€åŒ–åç‚¹ç§¯ç­‰äºä½™å¼¦ç›¸ä¼¼åº¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37402a30",
   "metadata": {},
   "source": [
    "## 8. å®è·µç»ƒä¹ \n",
    "\n",
    "è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…çš„ä¾‹å­æ¥åº”ç”¨å­¦åˆ°çš„çŸ¥è¯†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c182822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿæ–‡æ¡£å‘é‡æœç´¢\n",
    "# å‡è®¾æˆ‘ä»¬æœ‰ä¸€äº›æ–‡æ¡£ï¼Œæ¯ä¸ªæ–‡æ¡£ç”¨ç‰¹å¾å‘é‡è¡¨ç¤º\n",
    "\n",
    "# åˆ›å»ºæ¨¡æ‹Ÿæ–‡æ¡£å‘é‡ï¼ˆæ¯ä¸ªç»´åº¦ä»£è¡¨ä¸€ä¸ªè¯çš„é‡è¦æ€§ï¼‰\n",
    "np.random.seed(123)\n",
    "n_docs = 50\n",
    "n_features = 20  # ç®€åŒ–çš„ç‰¹å¾ç©ºé—´\n",
    "\n",
    "# æ–‡æ¡£ç±»å‹: ç§‘æŠ€ã€ä½“è‚²ã€è‰ºæœ¯\n",
    "doc_types = ['ç§‘æŠ€', 'ä½“è‚²', 'è‰ºæœ¯']\n",
    "doc_labels = []\n",
    "doc_vectors = []\n",
    "\n",
    "for i in range(n_docs):\n",
    "    doc_type = doc_types[i % 3]\n",
    "    \n",
    "    if doc_type == 'ç§‘æŠ€':\n",
    "        # ç§‘æŠ€æ–‡æ¡£åœ¨å‰7ä¸ªç‰¹å¾ä¸Šæœ‰è¾ƒé«˜å€¼\n",
    "        vector = np.concatenate([\n",
    "            np.random.uniform(0.5, 1.0, 7),  # ç§‘æŠ€ç›¸å…³ç‰¹å¾\n",
    "            np.random.uniform(0.0, 0.3, 13)  # å…¶ä»–ç‰¹å¾\n",
    "        ])\n",
    "    elif doc_type == 'ä½“è‚²':\n",
    "        # ä½“è‚²æ–‡æ¡£åœ¨ä¸­é—´7ä¸ªç‰¹å¾ä¸Šæœ‰è¾ƒé«˜å€¼\n",
    "        vector = np.concatenate([\n",
    "            np.random.uniform(0.0, 0.3, 7),   # ç§‘æŠ€ç‰¹å¾\n",
    "            np.random.uniform(0.5, 1.0, 7),   # ä½“è‚²ç›¸å…³ç‰¹å¾\n",
    "            np.random.uniform(0.0, 0.3, 6)    # è‰ºæœ¯ç‰¹å¾\n",
    "        ])\n",
    "    else:  # è‰ºæœ¯\n",
    "        # è‰ºæœ¯æ–‡æ¡£åœ¨å6ä¸ªç‰¹å¾ä¸Šæœ‰è¾ƒé«˜å€¼\n",
    "        vector = np.concatenate([\n",
    "            np.random.uniform(0.0, 0.3, 14),  # å…¶ä»–ç‰¹å¾\n",
    "            np.random.uniform(0.5, 1.0, 6)    # è‰ºæœ¯ç›¸å…³ç‰¹å¾\n",
    "        ])\n",
    "    \n",
    "    doc_vectors.append(vector)\n",
    "    doc_labels.append(f\"{doc_type}æ–‡æ¡£_{i:02d}\")\n",
    "\n",
    "doc_vectors = np.array(doc_vectors)\n",
    "\n",
    "print(f\"åˆ›å»ºäº† {n_docs} ä¸ªæ–‡æ¡£å‘é‡ï¼Œæ¯ä¸ª {n_features} ç»´\")\n",
    "print(f\"æ–‡æ¡£ç±»å‹åˆ†å¸ƒ: {len([l for l in doc_labels if 'ç§‘æŠ€' in l])} ç§‘æŠ€, \"\n",
    "      f\"{len([l for l in doc_labels if 'ä½“è‚²' in l])} ä½“è‚², \"\n",
    "      f\"{len([l for l in doc_labels if 'è‰ºæœ¯' in l])} è‰ºæœ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ–‡æ¡£å‘é‡çš„ç‰¹å¾åˆ†å¸ƒ\n",
    "tech_docs = [i for i, label in enumerate(doc_labels) if 'ç§‘æŠ€' in label]\n",
    "sports_docs = [i for i, label in enumerate(doc_labels) if 'ä½“è‚²' in label]\n",
    "art_docs = [i for i, label in enumerate(doc_labels) if 'è‰ºæœ¯' in label]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# è®¡ç®—æ¯ç±»æ–‡æ¡£çš„å¹³å‡ç‰¹å¾å€¼\n",
    "tech_mean = np.mean(doc_vectors[tech_docs], axis=0)\n",
    "sports_mean = np.mean(doc_vectors[sports_docs], axis=0)\n",
    "art_mean = np.mean(doc_vectors[art_docs], axis=0)\n",
    "\n",
    "plt.plot(tech_mean, 'o-', label='ç§‘æŠ€æ–‡æ¡£å¹³å‡', color='blue')\n",
    "plt.plot(sports_mean, 's-', label='ä½“è‚²æ–‡æ¡£å¹³å‡', color='green')\n",
    "plt.plot(art_mean, '^-', label='è‰ºæœ¯æ–‡æ¡£å¹³å‡', color='red')\n",
    "\n",
    "plt.xlabel('ç‰¹å¾ç»´åº¦')\n",
    "plt.ylabel('ç‰¹å¾å€¼')\n",
    "plt.title('ä¸åŒç±»å‹æ–‡æ¡£çš„ç‰¹å¾åˆ†å¸ƒ')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# åˆ›å»ºæœç´¢å¼•æ“å¹¶æ·»åŠ æ–‡æ¡£\n",
    "doc_search = SimpleVectorSearch()\n",
    "doc_search.add_vectors(doc_vectors, doc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä¸åŒç±»å‹çš„æŸ¥è¯¢\n",
    "def test_query(query_vector, query_name):\n",
    "    print(f\"\\n=== {query_name} ===\")\n",
    "    results = doc_search.search(query_vector, top_k=5)\n",
    "    \n",
    "    for rank, (idx, similarity, label) in enumerate(results, 1):\n",
    "        print(f\"{rank}. {label}: {similarity:.4f}\")\n",
    "    \n",
    "    # ç»Ÿè®¡ç»“æœä¸­å„ç±»å‹æ–‡æ¡£çš„æ•°é‡\n",
    "    tech_count = sum(1 for _, _, label in results if 'ç§‘æŠ€' in label)\n",
    "    sports_count = sum(1 for _, _, label in results if 'ä½“è‚²' in label)\n",
    "    art_count = sum(1 for _, _, label in results if 'è‰ºæœ¯' in label)\n",
    "    \n",
    "    print(f\"ç»“æœåˆ†å¸ƒ: ç§‘æŠ€={tech_count}, ä½“è‚²={sports_count}, è‰ºæœ¯={art_count}\")\n",
    "\n",
    "# æŸ¥è¯¢1: ç§‘æŠ€ç›¸å…³æŸ¥è¯¢\n",
    "tech_query = np.concatenate([\n",
    "    np.ones(7) * 0.8,      # ç§‘æŠ€ç‰¹å¾é«˜\n",
    "    np.ones(13) * 0.1      # å…¶ä»–ç‰¹å¾ä½\n",
    "])\n",
    "test_query(tech_query, \"ç§‘æŠ€æŸ¥è¯¢\")\n",
    "\n",
    "# æŸ¥è¯¢2: ä½“è‚²ç›¸å…³æŸ¥è¯¢\n",
    "sports_query = np.concatenate([\n",
    "    np.ones(7) * 0.1,      # ç§‘æŠ€ç‰¹å¾ä½\n",
    "    np.ones(7) * 0.8,      # ä½“è‚²ç‰¹å¾é«˜\n",
    "    np.ones(6) * 0.1       # è‰ºæœ¯ç‰¹å¾ä½\n",
    "])\n",
    "test_query(sports_query, \"ä½“è‚²æŸ¥è¯¢\")\n",
    "\n",
    "# æŸ¥è¯¢3: è‰ºæœ¯ç›¸å…³æŸ¥è¯¢\n",
    "art_query = np.concatenate([\n",
    "    np.ones(14) * 0.1,     # å…¶ä»–ç‰¹å¾ä½\n",
    "    np.ones(6) * 0.8       # è‰ºæœ¯ç‰¹å¾é«˜\n",
    "])\n",
    "test_query(art_query, \"è‰ºæœ¯æŸ¥è¯¢\")\n",
    "\n",
    "# æŸ¥è¯¢4: æ··åˆæŸ¥è¯¢\n",
    "mixed_query = np.ones(20) * 0.5  # æ‰€æœ‰ç‰¹å¾ä¸­ç­‰\n",
    "test_query(mixed_query, \"æ··åˆæŸ¥è¯¢\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761fe91",
   "metadata": {},
   "source": [
    "## 9. æ€»ç»“\n",
    "\n",
    "åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†:\n",
    "\n",
    "1. **å‘é‡è¡¨ç¤º**: å¦‚ä½•ç”¨æ•°å€¼å‘é‡è¡¨ç¤ºæ•°æ®\n",
    "2. **ç›¸ä¼¼åº¦åº¦é‡**: ä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§å‡ é‡Œå¾—è·ç¦»ã€æ›¼å“ˆé¡¿è·ç¦»ç­‰\n",
    "3. **é«˜ç»´å‘é‡**: å®é™…åº”ç”¨ä¸­çš„å‘é‡ç‰¹æ€§\n",
    "4. **å‘é‡æœç´¢**: åŸºç¡€çš„çº¿æ€§æœç´¢ç®—æ³•\n",
    "5. **å½’ä¸€åŒ–**: å‘é‡å½’ä¸€åŒ–çš„é‡è¦æ€§\n",
    "6. **å®è·µåº”ç”¨**: æ–‡æ¡£æœç´¢çš„ç®€å•ç¤ºä¾‹\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "åœ¨æ¥ä¸‹æ¥çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ :\n",
    "- æ›´é«˜çº§çš„ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•\n",
    "- æ–‡æœ¬å‘é‡åŒ–æŠ€æœ¯\n",
    "- é«˜æ•ˆçš„è¿‘ä¼¼æœç´¢ç®—æ³•\n",
    "- å®é™…åº”ç”¨æ¡ˆä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c26740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹ : å°è¯•ä¿®æ”¹ä¸Šé¢çš„ä»£ç ï¼Œå®éªŒä¸åŒçš„å‚æ•°è®¾ç½®\n",
    "# 1. æ”¹å˜æ–‡æ¡£ç‰¹å¾åˆ†å¸ƒ\n",
    "# 2. å°è¯•ä¸åŒçš„ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•\n",
    "# 3. è§‚å¯Ÿå½’ä¸€åŒ–å¯¹æœç´¢ç»“æœçš„å½±å“\n",
    "\n",
    "print(\"ğŸ‰ æ•™ç¨‹å®Œæˆï¼\")\n",
    "print(\"ğŸ’¡ å»ºè®®: å°è¯•ä¿®æ”¹ä»£ç å‚æ•°ï¼Œè§‚å¯Ÿç»“æœå˜åŒ–\")\n",
    "print(\"ğŸ“š ä¸‹ä¸€æ­¥: å­¦ä¹  02_similarity_metrics.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
