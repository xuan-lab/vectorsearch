{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457d15cf",
   "metadata": {},
   "source": [
    "# 文本嵌入和向量化\n",
    "# Text Embeddings and Vectorization\n",
    "\n",
    "这个笔记本演示如何将文本转换为向量，包括TF-IDF、Word2Vec和Sentence Transformers等方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea959c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.text_vectorizer import TextVectorizer\n",
    "from src.utils import load_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e9234",
   "metadata": {},
   "source": [
    "## 1. 加载示例文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e763228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载示例文档\n",
    "documents = load_json('../data/sample_documents.json')\n",
    "texts = [doc['content'] for doc in documents]\n",
    "\n",
    "print(f\"加载了 {len(texts)} 个文档\")\n",
    "print(\"\\n前3个文档内容:\")\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    print(f\"文档{i+1}: {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f2389",
   "metadata": {},
   "source": [
    "## 2. TF-IDF 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4abf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建TF-IDF向量化器\n",
    "tfidf_vectorizer = TextVectorizer(method='tfidf')\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "print(f\"TF-IDF向量形状: {tfidf_vectors.shape}\")\n",
    "print(f\"词汇表大小: {len(tfidf_vectorizer.vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# 显示特征词\n",
    "feature_names = tfidf_vectorizer.vectorizer.get_feature_names_out()\n",
    "print(f\"\\n前20个特征词: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76b644",
   "metadata": {},
   "source": [
    "## 3. 语义向量化 (Sentence Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建语义向量化器\n",
    "try:\n",
    "    semantic_vectorizer = TextVectorizer(method='semantic')\n",
    "    semantic_vectors = semantic_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    print(f\"语义向量形状: {semantic_vectors.shape}\")\n",
    "    print(\"语义向量维度更高，能捕获更丰富的语义信息\")\n",
    "except Exception as e:\n",
    "    print(f\"语义向量化需要安装sentence-transformers: {e}\")\n",
    "    semantic_vectors = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cef737",
   "metadata": {},
   "source": [
    "## 4. 向量化方法比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced90b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同向量化方法\n",
    "if semantic_vectors is not None:\n",
    "    methods = ['TF-IDF', 'Semantic']\n",
    "    vectors = [tfidf_vectors, semantic_vectors]\n",
    "else:\n",
    "    methods = ['TF-IDF']\n",
    "    vectors = [tfidf_vectors]\n",
    "\n",
    "for method, vec in zip(methods, vectors):\n",
    "    print(f\"\\n{method} 向量统计:\")\n",
    "    print(f\"  形状: {vec.shape}\")\n",
    "    print(f\"  稀疏度: {np.mean(vec == 0)*100:.1f}%\")\n",
    "    print(f\"  平均值: {np.mean(vec):.4f}\")\n",
    "    print(f\"  标准差: {np.std(vec):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479fef6",
   "metadata": {},
   "source": [
    "## 5. 可视化向量分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf15694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PCA降维可视化\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# PCA降维到2D\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_2d = pca.fit_transform(tfidf_vectors.toarray() if hasattr(tfidf_vectors, 'toarray') else tfidf_vectors)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# TF-IDF可视化\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1], alpha=0.7)\n",
    "for i, doc in enumerate(documents):\n",
    "    plt.annotate(f\"Doc{i+1}\", (tfidf_2d[i, 0], tfidf_2d[i, 1]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.title('TF-IDF向量分布 (PCA)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "# 语义向量可视化\n",
    "if semantic_vectors is not None:\n",
    "    semantic_2d = pca.fit_transform(semantic_vectors)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(semantic_2d[:, 0], semantic_2d[:, 1], alpha=0.7, color='red')\n",
    "    for i, doc in enumerate(documents):\n",
    "        plt.annotate(f\"Doc{i+1}\", (semantic_2d[i, 0], semantic_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    plt.title('语义向量分布 (PCA)')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744aa5db",
   "metadata": {},
   "source": [
    "## 6. 文本预处理效果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较有无预处理的效果\n",
    "sample_text = \"这是一个测试文档！包含各种标点符号...和数字123。\"\n",
    "\n",
    "# 原始文本\n",
    "print(\"原始文本:\", sample_text)\n",
    "\n",
    "# 预处理后\n",
    "processed = tfidf_vectorizer.preprocess_text(sample_text)\n",
    "print(\"预处理后:\", processed)\n",
    "\n",
    "# 分析预处理步骤\n",
    "print(\"\\n预处理步骤分析:\")\n",
    "print(\"1. 转小写\")\n",
    "print(\"2. 移除标点符号\")\n",
    "print(\"3. 移除数字\")\n",
    "print(\"4. 移除停用词\")\n",
    "print(\"5. 词干提取/词形还原\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981a337",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本笔记本介绍了主要的文本向量化方法:\n",
    "- TF-IDF: 基于词频的统计方法\n",
    "- Semantic Embeddings: 基于深度学习的语义方法\n",
    "\n",
    "每种方法都有其适用场景，选择时需要考虑精度、速度和资源需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f15693",
   "metadata": {},
   "source": [
    "# 文本嵌入与向量化技术\n",
    "\n",
    "本notebook将深入探讨文本向量化的各种技术，包括传统的TF-IDF方法和现代的神经网络嵌入方法。\n",
    "\n",
    "## 学习目标\n",
    "- 理解不同文本向量化方法的原理\n",
    "- 学习TF-IDF、Word2Vec、Sentence Transformers的实现\n",
    "- 比较不同方法的优缺点\n",
    "- 在中文文本上实践各种嵌入技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17861ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import jieba\n",
    "import json\n",
    "\n",
    "from src.text_vectorizer import TextVectorizer\n",
    "from src.utils import load_documents, plot_vector_2d\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False   # 用来正常显示负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b308993",
   "metadata": {},
   "source": [
    "## 1. 加载示例数据\n",
    "\n",
    "首先让我们加载一些中文文档来进行实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba34809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载示例文档\n",
    "documents = load_documents('../data/sample_documents.json')\n",
    "\n",
    "print(f\"加载了 {len(documents)} 个文档\")\n",
    "print(\"\\n前3个文档:\")\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"文档 {i+1}: {doc['title']}\")\n",
    "    print(f\"内容: {doc['content'][:100]}...\")\n",
    "    print(f\"类别: {doc['category']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abf6fd",
   "metadata": {},
   "source": [
    "## 2. TF-IDF向量化\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) 是最经典的文本向量化方法之一。\n",
    "\n",
    "### 原理\n",
    "- **TF (词频)**: 词在文档中出现的频率\n",
    "- **IDF (逆文档频率)**: 衡量词的重要性，常见词的IDF值较低\n",
    "- **TF-IDF = TF × IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a97152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化文本向量化器\n",
    "vectorizer = TextVectorizer()\n",
    "\n",
    "# 提取文档内容\n",
    "texts = [doc['content'] for doc in documents]\n",
    "\n",
    "# 使用TF-IDF向量化\n",
    "print(\"正在进行TF-IDF向量化...\")\n",
    "tfidf_vectors = vectorizer.tfidf_vectorize(texts)\n",
    "\n",
    "print(f\"TF-IDF向量维度: {tfidf_vectors.shape}\")\n",
    "print(f\"向量稀疏度: {(tfidf_vectors == 0).sum() / tfidf_vectors.size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化TF-IDF向量的特征分布\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 每个文档的向量长度分布\n",
    "plt.subplot(1, 2, 1)\n",
    "vector_norms = np.linalg.norm(tfidf_vectors, axis=1)\n",
    "plt.hist(vector_norms, bins=15, alpha=0.7, color='skyblue')\n",
    "plt.title('TF-IDF向量长度分布')\n",
    "plt.xlabel('向量长度')\n",
    "plt.ylabel('文档数量')\n",
    "\n",
    "# 特征值分布\n",
    "plt.subplot(1, 2, 2)\n",
    "non_zero_values = tfidf_vectors[tfidf_vectors > 0]\n",
    "plt.hist(non_zero_values, bins=30, alpha=0.7, color='lightcoral')\n",
    "plt.title('TF-IDF值分布(非零值)')\n",
    "plt.xlabel('TF-IDF值')\n",
    "plt.ylabel('频次')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725cccaa",
   "metadata": {},
   "source": [
    "## 3. Word2Vec嵌入\n",
    "\n",
    "Word2Vec是一种将词映射到稠密向量空间的神经网络方法，能够捕获词之间的语义关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbeee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec向量化\n",
    "print(\"正在训练Word2Vec模型...\")\n",
    "word2vec_vectors = vectorizer.word2vec_vectorize(texts, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "print(f\"Word2Vec向量维度: {word2vec_vectors.shape}\")\n",
    "print(f\"向量稠密度: {(word2vec_vectors != 0).sum() / word2vec_vectors.size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示Word2Vec词向量的语义相似性\n",
    "if hasattr(vectorizer, 'word2vec_model') and vectorizer.word2vec_model:\n",
    "    model = vectorizer.word2vec_model\n",
    "    \n",
    "    # 找一些测试词\n",
    "    test_words = ['技术', '人工智能', '体育', '艺术', '音乐']\n",
    "    available_words = [word for word in test_words if word in model.wv.key_to_index]\n",
    "    \n",
    "    if available_words:\n",
    "        print(\"词语相似性测试:\")\n",
    "        for word in available_words[:3]:\n",
    "            try:\n",
    "                similar_words = model.wv.most_similar(word, topn=3)\n",
    "                print(f\"与'{word}'最相似的词:\")\n",
    "                for similar_word, similarity in similar_words:\n",
    "                    print(f\"  {similar_word}: {similarity:.3f}\")\n",
    "                print()\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3201ed",
   "metadata": {},
   "source": [
    "## 4. Sentence Transformers嵌入\n",
    "\n",
    "Sentence Transformers基于预训练的BERT等模型，能够生成高质量的句子级别嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36302c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformers向量化\n",
    "print(\"正在使用Sentence Transformers进行向量化...\")\n",
    "try:\n",
    "    sentence_vectors = vectorizer.sentence_transformer_vectorize(texts, model_name='all-MiniLM-L6-v2')\n",
    "    print(f\"Sentence Transformers向量维度: {sentence_vectors.shape}\")\n",
    "    print(f\"向量稠密度: {(sentence_vectors != 0).sum() / sentence_vectors.size * 100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"Sentence Transformers加载失败: {e}\")\n",
    "    print(\"将使用模拟向量进行演示\")\n",
    "    sentence_vectors = np.random.randn(len(texts), 384)  # 模拟384维向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7e833",
   "metadata": {},
   "source": [
    "## 5. 向量化方法比较\n",
    "\n",
    "让我们比较三种方法生成的向量的特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb27b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同方法的向量特性\n",
    "methods = {\n",
    "    'TF-IDF': tfidf_vectors,\n",
    "    'Word2Vec': word2vec_vectors,\n",
    "    'Sentence Transformers': sentence_vectors\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (method_name, vectors) in enumerate(methods.items()):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    \n",
    "    # 计算向量长度\n",
    "    norms = np.linalg.norm(vectors, axis=1)\n",
    "    plt.hist(norms, bins=15, alpha=0.7, label=method_name)\n",
    "    plt.title(f'{method_name}\\n向量长度分布')\n",
    "    plt.xlabel('向量长度')\n",
    "    plt.ylabel('文档数量')\n",
    "    \n",
    "    # 添加统计信息\n",
    "    plt.axvline(np.mean(norms), color='red', linestyle='--', \n",
    "                label=f'均值: {np.mean(norms):.3f}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543749ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PCA降维可视化不同方法的向量分布\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "categories = [doc['category'] for doc in documents]\n",
    "unique_categories = list(set(categories))\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "category_color_map = dict(zip(unique_categories, colors))\n",
    "\n",
    "for i, (method_name, vectors) in enumerate(methods.items()):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    \n",
    "    # PCA降维到2D\n",
    "    if vectors.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        vectors_2d = pca.fit_transform(vectors)\n",
    "    else:\n",
    "        vectors_2d = vectors\n",
    "    \n",
    "    # 按类别绘制散点图\n",
    "    for category in unique_categories:\n",
    "        mask = np.array(categories) == category\n",
    "        plt.scatter(vectors_2d[mask, 0], vectors_2d[mask, 1], \n",
    "                   c=[category_color_map[category]], \n",
    "                   label=category, alpha=0.7, s=50)\n",
    "    \n",
    "    plt.title(f'{method_name}\\nPCA可视化')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    if i == 0:  # 只在第一个图上显示图例\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90f9b3",
   "metadata": {},
   "source": [
    "## 6. 相似度计算实验\n",
    "\n",
    "让我们测试不同向量化方法在文档相似度计算上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 选择一个查询文档\n",
    "query_idx = 0\n",
    "query_doc = documents[query_idx]\n",
    "\n",
    "print(f\"查询文档: {query_doc['title']}\")\n",
    "print(f\"内容: {query_doc['content'][:100]}...\")\n",
    "print(f\"类别: {query_doc['category']}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 对每种方法计算相似度\n",
    "for method_name, vectors in methods.items():\n",
    "    print(f\"\\n{method_name} 相似度排序:\")\n",
    "    \n",
    "    # 计算查询向量与所有文档的相似度\n",
    "    query_vector = vectors[query_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(query_vector, vectors)[0]\n",
    "    \n",
    "    # 排序（除了查询文档本身）\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:6]  # 前5个最相似的\n",
    "    \n",
    "    for i, idx in enumerate(similar_indices):\n",
    "        sim_score = similarities[idx]\n",
    "        doc = documents[idx]\n",
    "        print(f\"  {i+1}. {doc['title']} (相似度: {sim_score:.3f}) [{doc['category']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3a83a",
   "metadata": {},
   "source": [
    "## 7. 嵌入质量评估\n",
    "\n",
    "评估不同向量化方法的质量，通过类内相似度和类间相似度来衡量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embedding_quality(vectors, categories):\n",
    "    \"\"\"评估嵌入质量\"\"\"\n",
    "    similarities = cosine_similarity(vectors)\n",
    "    \n",
    "    # 计算类内和类间相似度\n",
    "    intra_class_sims = []\n",
    "    inter_class_sims = []\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        for j in range(i+1, len(categories)):\n",
    "            sim = similarities[i, j]\n",
    "            if categories[i] == categories[j]:\n",
    "                intra_class_sims.append(sim)\n",
    "            else:\n",
    "                inter_class_sims.append(sim)\n",
    "    \n",
    "    return {\n",
    "        'intra_class_mean': np.mean(intra_class_sims) if intra_class_sims else 0,\n",
    "        'inter_class_mean': np.mean(inter_class_sims) if inter_class_sims else 0,\n",
    "        'separation': np.mean(intra_class_sims) - np.mean(inter_class_sims) if intra_class_sims and inter_class_sims else 0\n",
    "    }\n",
    "\n",
    "# 评估所有方法\n",
    "print(\"嵌入质量评估 (类内相似度 vs 类间相似度):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method_name, vectors in methods.items():\n",
    "    quality = evaluate_embedding_quality(vectors, categories)\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  类内平均相似度: {quality['intra_class_mean']:.3f}\")\n",
    "    print(f\"  类间平均相似度: {quality['inter_class_mean']:.3f}\")\n",
    "    print(f\"  分离度 (越大越好): {quality['separation']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1475bff",
   "metadata": {},
   "source": [
    "## 8. 实际应用场景\n",
    "\n",
    "让我们演示如何在实际场景中选择合适的向量化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 场景1: 关键词搜索 - TF-IDF通常表现良好\n",
    "print(\"场景1: 关键词搜索\")\n",
    "print(\"查询: '人工智能技术发展'\")\n",
    "\n",
    "query_text = \"人工智能技术发展\"\n",
    "query_tfidf = vectorizer.tfidf_vectorize([query_text])\n",
    "\n",
    "# 计算与所有文档的相似度\n",
    "similarities = cosine_similarity(query_tfidf, tfidf_vectors)[0]\n",
    "top_indices = np.argsort(similarities)[::-1][:3]\n",
    "\n",
    "print(\"\\nTF-IDF搜索结果:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"  {i+1}. {documents[idx]['title']} (相似度: {similarities[idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 场景2: 语义搜索 - Sentence Transformers通常更好\n",
    "print(\"\\n场景2: 语义搜索\")\n",
    "print(\"查询: '创新科技应用'\")\n",
    "\n",
    "try:\n",
    "    query_semantic = vectorizer.sentence_transformer_vectorize([query_text])\n",
    "    similarities_semantic = cosine_similarity(query_semantic, sentence_vectors)[0]\n",
    "    top_indices_semantic = np.argsort(similarities_semantic)[::-1][:3]\n",
    "    \n",
    "    print(\"\\nSentence Transformers搜索结果:\")\n",
    "    for i, idx in enumerate(top_indices_semantic):\n",
    "        print(f\"  {i+1}. {documents[idx]['title']} (相似度: {similarities_semantic[idx]:.3f})\")\n",
    "except:\n",
    "    print(\"Sentence Transformers不可用，跳过语义搜索演示\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207491d5",
   "metadata": {},
   "source": [
    "## 9. 总结与建议\n",
    "\n",
    "### 方法选择指南:\n",
    "\n",
    "1. **TF-IDF**:\n",
    "   - 优点: 简单快速，适合关键词匹配\n",
    "   - 缺点: 无法捕获语义关系\n",
    "   - 适用: 传统信息检索，关键词搜索\n",
    "\n",
    "2. **Word2Vec**:\n",
    "   - 优点: 捕获词语语义关系\n",
    "   - 缺点: 需要聚合词向量到文档级别\n",
    "   - 适用: 词语相似度分析，语义理解\n",
    "\n",
    "3. **Sentence Transformers**:\n",
    "   - 优点: 直接生成句子/文档级别嵌入，语义理解能力强\n",
    "   - 缺点: 计算资源需求大，模型较大\n",
    "   - 适用: 语义搜索，文档相似度，推荐系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存实验结果\n",
    "experiment_results = {\n",
    "    'document_count': len(documents),\n",
    "    'vector_dimensions': {\n",
    "        'tfidf': tfidf_vectors.shape[1],\n",
    "        'word2vec': word2vec_vectors.shape[1],\n",
    "        'sentence_transformers': sentence_vectors.shape[1]\n",
    "    },\n",
    "    'quality_metrics': {}\n",
    "}\n",
    "\n",
    "for method_name, vectors in methods.items():\n",
    "    quality = evaluate_embedding_quality(vectors, categories)\n",
    "    experiment_results['quality_metrics'][method_name] = quality\n",
    "\n",
    "print(\"实验完成！\")\n",
    "print(f\"处理了{len(documents)}个文档\")\n",
    "print(\"各方法向量维度:\", experiment_results['vector_dimensions'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
