{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385c0ff4",
   "metadata": {},
   "source": [
    "# ç›¸ä¼¼åº¦åº¦é‡æ·±å…¥åˆ†æ\n",
    "# Similarity Metrics Deep Dive\n",
    "\n",
    "è¿™ä¸ªç¬”è®°æœ¬æ·±å…¥æ¢è®¨å„ç§ç›¸ä¼¼åº¦åº¦é‡çš„æ•°å­¦åŸç†å’Œå®é™…åº”ç”¨ã€‚\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "- ç†è§£ä¸åŒç›¸ä¼¼åº¦åº¦é‡çš„æ•°å­¦åŸç†\n",
    "- æ¯”è¾ƒå„ç§åº¦é‡æ–¹æ³•çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯\n",
    "- å¯è§†åŒ–ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ae51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from src.basic_vector_search import BasicVectorSearch\n",
    "from src.utils import plot_similarity_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91c481",
   "metadata": {},
   "source": [
    "## 1. æ•°æ®å‡†å¤‡\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ›å»ºä¸€äº›æµ‹è¯•å‘é‡æ¥æ¼”ç¤ºä¸åŒçš„ç›¸ä¼¼åº¦åº¦é‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæµ‹è¯•å‘é‡\n",
    "test_vectors = np.array([\n",
    "    [1, 0, 0],      # å•ä½å‘é‡ X\n",
    "    [0, 1, 0],      # å•ä½å‘é‡ Y\n",
    "    [0, 0, 1],      # å•ä½å‘é‡ Z\n",
    "    [1, 1, 0],      # å¯¹è§’çº¿å‘é‡\n",
    "    [2, 0, 0],      # 2å€Xå‘é‡\n",
    "    [1, 1, 1],      # ä¸‰ç»´å¯¹è§’çº¿\n",
    "    [-1, 0, 0],     # è´ŸXå‘é‡\n",
    "    [0.5, 0.5, 0.7] # æ··åˆå‘é‡\n",
    "])\n",
    "\n",
    "# å‘é‡æ ‡ç­¾\n",
    "vector_labels = ['X', 'Y', 'Z', 'XY', '2X', 'XYZ', '-X', 'Mixed']\n",
    "\n",
    "print(\"æµ‹è¯•å‘é‡:\")\n",
    "for i, (vec, label) in enumerate(zip(test_vectors, vector_labels)):\n",
    "    print(f\"{label}: {vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f86b63",
   "metadata": {},
   "source": [
    "## 2. ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)\n",
    "\n",
    "ä½™å¼¦ç›¸ä¼¼åº¦æµ‹é‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è§’åº¦ï¼ŒèŒƒå›´ä»-1åˆ°1ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4be66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "cosine_sim_matrix = cosine_similarity(test_vectors)\n",
    "\n",
    "# å¯è§†åŒ–ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cosine_sim_matrix, \n",
    "            xticklabels=vector_labels, \n",
    "            yticklabels=vector_labels,\n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0,\n",
    "            fmt='.3f')\n",
    "plt.title('ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# åˆ†æå…³é”®è§‚å¯Ÿ\n",
    "print(\"å…³é”®è§‚å¯Ÿ:\")\n",
    "print(f\"X å’Œ 2X çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim_matrix[0, 4]:.3f} (å®Œå…¨ç›¸åŒæ–¹å‘)\")\n",
    "print(f\"X å’Œ -X çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim_matrix[0, 6]:.3f} (å®Œå…¨ç›¸åæ–¹å‘)\")\n",
    "print(f\"X å’Œ Y çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim_matrix[0, 1]:.3f} (å‚ç›´)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9796b",
   "metadata": {},
   "source": [
    "## 3. æ¬§å‡ é‡Œå¾—è·ç¦» (Euclidean Distance)\n",
    "\n",
    "æ¬§å‡ é‡Œå¾—è·ç¦»æ˜¯æœ€ç›´è§‚çš„è·ç¦»åº¦é‡ï¼Œè®¡ç®—å‘é‡ä¹‹é—´çš„ç›´çº¿è·ç¦»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»çŸ©é˜µ\n",
    "euclidean_dist_matrix = euclidean_distances(test_vectors)\n",
    "\n",
    "# å¯è§†åŒ–æ¬§å‡ é‡Œå¾—è·ç¦»çŸ©é˜µ\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(euclidean_dist_matrix, \n",
    "            xticklabels=vector_labels, \n",
    "            yticklabels=vector_labels,\n",
    "            annot=True, \n",
    "            cmap='viridis_r',\n",
    "            fmt='.3f')\n",
    "plt.title('æ¬§å‡ é‡Œå¾—è·ç¦»çŸ©é˜µ')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"å…³é”®è§‚å¯Ÿ:\")\n",
    "print(f\"X å’Œ 2X çš„æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_dist_matrix[0, 4]:.3f} (é•¿åº¦å·®å¼‚)\")\n",
    "print(f\"X å’Œ -X çš„æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_dist_matrix[0, 6]:.3f} (æ–¹å‘ç›¸å)\")\n",
    "print(f\"X å’Œ Y çš„æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_dist_matrix[0, 1]:.3f} (å‚ç›´å‘é‡)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5cdf9c",
   "metadata": {},
   "source": [
    "## 4. æ›¼å“ˆé¡¿è·ç¦» (Manhattan Distance)\n",
    "\n",
    "æ›¼å“ˆé¡¿è·ç¦»ï¼ˆä¹Ÿç§°ä¸ºL1è·ç¦»ï¼‰è®¡ç®—å„ç»´åº¦å·®å€¼çš„ç»å¯¹å€¼ä¹‹å’Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d03e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—æ›¼å“ˆé¡¿è·ç¦»çŸ©é˜µ\n",
    "manhattan_dist_matrix = manhattan_distances(test_vectors)\n",
    "\n",
    "# å¯è§†åŒ–æ›¼å“ˆé¡¿è·ç¦»çŸ©é˜µ\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(manhattan_dist_matrix, \n",
    "            xticklabels=vector_labels, \n",
    "            yticklabels=vector_labels,\n",
    "            annot=True, \n",
    "            cmap='plasma_r',\n",
    "            fmt='.3f')\n",
    "plt.title('æ›¼å“ˆé¡¿è·ç¦»çŸ©é˜µ')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"å…³é”®è§‚å¯Ÿ:\")\n",
    "print(f\"X å’Œ 2X çš„æ›¼å“ˆé¡¿è·ç¦»: {manhattan_dist_matrix[0, 4]:.3f}\")\n",
    "print(f\"X å’Œ -X çš„æ›¼å“ˆé¡¿è·ç¦»: {manhattan_dist_matrix[0, 6]:.3f}\")\n",
    "print(f\"X å’Œ Y çš„æ›¼å“ˆé¡¿è·ç¦»: {manhattan_dist_matrix[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddf871",
   "metadata": {},
   "source": [
    "## 5. åº¦é‡æ¯”è¾ƒå’Œåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed491aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©ä¸€ä¸ªæŸ¥è¯¢å‘é‡è¿›è¡Œæ¯”è¾ƒ\n",
    "query_idx = 0  # ä½¿ç”¨Xå‘é‡ä½œä¸ºæŸ¥è¯¢\n",
    "query_vector = test_vectors[query_idx]\n",
    "\n",
    "print(f\"æŸ¥è¯¢å‘é‡: {vector_labels[query_idx]} = {query_vector}\")\n",
    "print(\"\\nä¸å…¶ä»–å‘é‡çš„ç›¸ä¼¼åº¦/è·ç¦»æ¯”è¾ƒ:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'å‘é‡':<8} {'ä½™å¼¦ç›¸ä¼¼åº¦':<12} {'æ¬§å‡ é‡Œå¾—è·ç¦»':<12} {'æ›¼å“ˆé¡¿è·ç¦»':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, label in enumerate(vector_labels):\n",
    "    if i != query_idx:\n",
    "        cosine_sim = cosine_sim_matrix[query_idx, i]\n",
    "        euclidean_dist = euclidean_dist_matrix[query_idx, i]\n",
    "        manhattan_dist = manhattan_dist_matrix[query_idx, i]\n",
    "        \n",
    "        print(f\"{label:<8} {cosine_sim:<12.3f} {euclidean_dist:<12.3f} {manhattan_dist:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3b448",
   "metadata": {},
   "source": [
    "## 6. å®é™…åº”ç”¨åœºæ™¯åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91deb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸åŒç‰¹å¾çš„å‘é‡æ¥æ¼”ç¤ºåº”ç”¨åœºæ™¯\n",
    "# åœºæ™¯1: æ–‡æ¡£å‘é‡ï¼ˆç¨€ç–ç‰¹å¾ï¼‰\n",
    "doc_vectors = np.array([\n",
    "    [1, 0, 2, 0, 3],    # æ–‡æ¡£1: åŒ…å«è¯1å’Œè¯3å’Œè¯5\n",
    "    [0, 1, 0, 2, 0],    # æ–‡æ¡£2: åŒ…å«è¯2å’Œè¯4\n",
    "    [2, 0, 4, 0, 6],    # æ–‡æ¡£3: æ–‡æ¡£1çš„æ‰©å±•ç‰ˆæœ¬\n",
    "    [1, 1, 1, 1, 1],    # æ–‡æ¡£4: åŒ…å«æ‰€æœ‰è¯\n",
    "])\n",
    "\n",
    "print(\"=== æ–‡æ¡£ç›¸ä¼¼åº¦åˆ†æ ===\")\n",
    "print(\"æ–‡æ¡£å‘é‡ (è¯é¢‘è¡¨ç¤º):\")\n",
    "for i, vec in enumerate(doc_vectors):\n",
    "    print(f\"æ–‡æ¡£{i+1}: {vec}\")\n",
    "\n",
    "# ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦åˆ†ææ–‡æ¡£ç›¸ä¼¼æ€§\n",
    "doc_cosine_sim = cosine_similarity(doc_vectors)\n",
    "print(\"\\næ–‡æ¡£é—´ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "for i in range(len(doc_vectors)):\n",
    "    for j in range(i+1, len(doc_vectors)):\n",
    "        sim = doc_cosine_sim[i, j]\n",
    "        print(f\"æ–‡æ¡£{i+1} vs æ–‡æ¡£{j+1}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nè§‚å¯Ÿ: æ–‡æ¡£1å’Œæ–‡æ¡£3çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾ˆé«˜(1.000)ï¼Œå› ä¸ºå®ƒä»¬æœ‰ç›¸åŒçš„è¯æ±‡æ¯”ä¾‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœºæ™¯2: å›¾åƒç‰¹å¾å‘é‡ï¼ˆå¯†é›†ç‰¹å¾ï¼‰\n",
    "np.random.seed(42)\n",
    "image_vectors = np.array([\n",
    "    [0.8, 0.2, 0.1],    # å›¾åƒ1: ä¸»è¦æ˜¯çº¢è‰²\n",
    "    [0.1, 0.8, 0.2],    # å›¾åƒ2: ä¸»è¦æ˜¯ç»¿è‰²\n",
    "    [0.2, 0.1, 0.8],    # å›¾åƒ3: ä¸»è¦æ˜¯è“è‰²\n",
    "    [0.5, 0.5, 0.1],    # å›¾åƒ4: çº¢ç»¿æ··åˆ\n",
    "])\n",
    "\n",
    "print(\"\\n=== å›¾åƒé¢œè‰²ç›¸ä¼¼åº¦åˆ†æ ===\")\n",
    "print(\"å›¾åƒé¢œè‰²å‘é‡ (RGBæ¯”ä¾‹):\")\n",
    "colors = ['ä¸»è¦çº¢è‰²', 'ä¸»è¦ç»¿è‰²', 'ä¸»è¦è“è‰²', 'çº¢ç»¿æ··åˆ']\n",
    "for i, (vec, color) in enumerate(zip(image_vectors, colors)):\n",
    "    print(f\"å›¾åƒ{i+1} ({color}): {vec}\")\n",
    "\n",
    "# æ¯”è¾ƒä¸åŒåº¦é‡åœ¨å›¾åƒç›¸ä¼¼åº¦ä¸Šçš„è¡¨ç°\n",
    "img_cosine = cosine_similarity(image_vectors)\n",
    "img_euclidean = euclidean_distances(image_vectors)\n",
    "\n",
    "print(\"\\nä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "for i in range(len(image_vectors)):\n",
    "    for j in range(i+1, len(image_vectors)):\n",
    "        sim = img_cosine[i, j]\n",
    "        print(f\"{colors[i]} vs {colors[j]}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»:\")\n",
    "for i in range(len(image_vectors)):\n",
    "    for j in range(i+1, len(image_vectors)):\n",
    "        dist = img_euclidean[i, j]\n",
    "        print(f\"{colors[i]} vs {colors[j]}: {dist:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3e43d",
   "metadata": {},
   "source": [
    "## 7. åº¦é‡é€‰æ‹©æŒ‡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå†³ç­–è¡¨\n",
    "import pandas as pd\n",
    "\n",
    "decision_data = {\n",
    "    'åº¦é‡æ–¹æ³•': ['ä½™å¼¦ç›¸ä¼¼åº¦', 'æ¬§å‡ é‡Œå¾—è·ç¦»', 'æ›¼å“ˆé¡¿è·ç¦»'],\n",
    "    'èŒƒå›´': ['[-1, 1]', '[0, +âˆ)', '[0, +âˆ)'],\n",
    "    'å¯¹é•¿åº¦æ•æ„Ÿ': ['å¦', 'æ˜¯', 'æ˜¯'],\n",
    "    'é€‚ç”¨æ•°æ®ç±»å‹': ['ç¨€ç–å‘é‡', 'å¯†é›†å‘é‡', 'é«˜ç»´ç¨€ç–'],\n",
    "    'è®¡ç®—å¤æ‚åº¦': ['O(d)', 'O(d)', 'O(d)'],\n",
    "    'ä¸»è¦åº”ç”¨': ['æ–‡æœ¬æ£€ç´¢', 'å›¾åƒåŒ¹é…', 'æ¨èç³»ç»Ÿ']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(decision_data)\n",
    "print(\"åº¦é‡æ–¹æ³•é€‰æ‹©æŒ‡å—:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nä½¿ç”¨å»ºè®®:\")\n",
    "print(\"1. ä½™å¼¦ç›¸ä¼¼åº¦: å½“å‘é‡é•¿åº¦ä¸é‡è¦ï¼Œåªå…³å¿ƒæ–¹å‘æ—¶ï¼ˆå¦‚æ–‡æ¡£ç›¸ä¼¼æ€§ï¼‰\")\n",
    "print(\"2. æ¬§å‡ é‡Œå¾—è·ç¦»: å½“å‘é‡é•¿åº¦å’Œæ–¹å‘éƒ½é‡è¦æ—¶ï¼ˆå¦‚åæ ‡ç‚¹è·ç¦»ï¼‰\")\n",
    "print(\"3. æ›¼å“ˆé¡¿è·ç¦»: å½“ç‰¹å¾ç‹¬ç«‹ä¸”æœ‰æ˜ç¡®ç‰©ç†æ„ä¹‰æ—¶ï¼ˆå¦‚åŸå¸‚è¡—åŒºè·ç¦»ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a9296",
   "metadata": {},
   "source": [
    "## 8. æ€§èƒ½æ¯”è¾ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®è¿›è¡Œæ€§èƒ½æµ‹è¯•\n",
    "import time\n",
    "\n",
    "# åˆ›å»ºä¸åŒå¤§å°çš„æ•°æ®é›†\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "dimensions = [10, 50, 100]\n",
    "\n",
    "print(\"ä¸åŒç»´åº¦å’Œæ•°æ®è§„æ¨¡ä¸‹çš„æ€§èƒ½æ¯”è¾ƒ:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"\\nç»´åº¦: {dim}\")\n",
    "    print(f\"{'æ•°æ®è§„æ¨¡':<10} {'ä½™å¼¦ç›¸ä¼¼åº¦':<15} {'æ¬§å‡ é‡Œå¾—è·ç¦»':<15} {'æ›¼å“ˆé¡¿è·ç¦»':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # ç”Ÿæˆéšæœºæ•°æ®\n",
    "        data = np.random.rand(size, dim)\n",
    "        query = np.random.rand(dim)\n",
    "        \n",
    "        # æµ‹è¯•ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        start_time = time.time()\n",
    "        _ = cosine_similarity([query], data)\n",
    "        cosine_time = time.time() - start_time\n",
    "        \n",
    "        # æµ‹è¯•æ¬§å‡ é‡Œå¾—è·ç¦»\n",
    "        start_time = time.time()\n",
    "        _ = euclidean_distances([query], data)\n",
    "        euclidean_time = time.time() - start_time\n",
    "        \n",
    "        # æµ‹è¯•æ›¼å“ˆé¡¿è·ç¦»\n",
    "        start_time = time.time()\n",
    "        _ = manhattan_distances([query], data)\n",
    "        manhattan_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"{size:<10} {cosine_time:<15.4f} {euclidean_time:<15.4f} {manhattan_time:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed79cdb",
   "metadata": {},
   "source": [
    "## 9. æ€»ç»“\n",
    "\n",
    "åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†ï¼š\n",
    "\n",
    "### ä¸»è¦ç›¸ä¼¼åº¦åº¦é‡:\n",
    "1. **ä½™å¼¦ç›¸ä¼¼åº¦**: æµ‹é‡è§’åº¦ï¼Œå¿½ç•¥å‘é‡é•¿åº¦\n",
    "2. **æ¬§å‡ é‡Œå¾—è·ç¦»**: ç›´çº¿è·ç¦»ï¼Œè€ƒè™‘æ‰€æœ‰ç»´åº¦\n",
    "3. **æ›¼å“ˆé¡¿è·ç¦»**: å„ç»´åº¦å·®å€¼çš„ç»å¯¹å€¼ä¹‹å’Œ\n",
    "\n",
    "### é€‰æ‹©åŸåˆ™:\n",
    "- **æ–‡æ¡£/æ–‡æœ¬**: ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "- **å›¾åƒ/åæ ‡**: ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»\n",
    "- **é«˜ç»´ç¨€ç–**: è€ƒè™‘æ›¼å“ˆé¡¿è·ç¦»\n",
    "\n",
    "### æ€§èƒ½è€ƒè™‘:\n",
    "- æ‰€æœ‰åº¦é‡çš„æ—¶é—´å¤æ‚åº¦éƒ½æ˜¯O(d)\n",
    "- å®é™…æ€§èƒ½å¯èƒ½å› å®ç°å’Œæ•°æ®ç‰¹å¾è€Œå¼‚\n",
    "\n",
    "ä¸‹ä¸€ä¸ªç¬”è®°æœ¬å°†æ¢è®¨æ–‡æœ¬å‘é‡åŒ–æŠ€æœ¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53c177",
   "metadata": {},
   "source": [
    "# ç›¸ä¼¼åº¦åº¦é‡è¯¦è§£\n",
    "# Similarity Metrics Deep Dive\n",
    "\n",
    "æœ¬æ•™ç¨‹æ·±å…¥æ¢è®¨å„ç§ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼ŒåŒ…æ‹¬:\n",
    "- ä½™å¼¦ç›¸ä¼¼åº¦ vs ç‚¹ç§¯\n",
    "- è·ç¦»åº¦é‡çš„é€‰æ‹©\n",
    "- åº¦é‡æ–¹æ³•çš„é€‚ç”¨åœºæ™¯\n",
    "- æ€§èƒ½å¯¹æ¯”åˆ†æ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df4707",
   "metadata": {},
   "source": [
    "## 1. ç›¸ä¼¼åº¦åº¦é‡æ¦‚è§ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityMetrics:\n",
    "    \"\"\"ç›¸ä¼¼åº¦åº¦é‡å·¥å…·ç±»\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity(v1, v2):\n",
    "        \"\"\"ä½™å¼¦ç›¸ä¼¼åº¦: è¡¡é‡å‘é‡å¤¹è§’\"\"\"\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dot_product(v1, v2):\n",
    "        \"\"\"ç‚¹ç§¯: è€ƒè™‘å‘é‡å¤§å°å’Œæ–¹å‘\"\"\"\n",
    "        return np.dot(v1, v2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(v1, v2):\n",
    "        \"\"\"æ¬§å‡ é‡Œå¾—è·ç¦»: L2è·ç¦»\"\"\"\n",
    "        return np.sqrt(np.sum((v1 - v2) ** 2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def manhattan_distance(v1, v2):\n",
    "        \"\"\"æ›¼å“ˆé¡¿è·ç¦»: L1è·ç¦»\"\"\"\n",
    "        return np.sum(np.abs(v1 - v2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def chebyshev_distance(v1, v2):\n",
    "        \"\"\"åˆ‡æ¯”é›ªå¤«è·ç¦»: Lâˆè·ç¦»\"\"\"\n",
    "        return np.max(np.abs(v1 - v2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def hamming_distance(v1, v2):\n",
    "        \"\"\"æ±‰æ˜è·ç¦»: ä¸åŒä½ç½®çš„æ•°é‡\"\"\"\n",
    "        return np.sum(v1 != v2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard_similarity(v1, v2):\n",
    "        \"\"\"Jaccardç›¸ä¼¼åº¦: é€‚ç”¨äºäºŒè¿›åˆ¶å‘é‡\"\"\"\n",
    "        intersection = np.sum(np.minimum(v1, v2))\n",
    "        union = np.sum(np.maximum(v1, v2))\n",
    "        return intersection / union if union > 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def pearson_correlation(v1, v2):\n",
    "        \"\"\"çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
    "        return np.corrcoef(v1, v2)[0, 1]\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•å‘é‡\n",
    "np.random.seed(42)\n",
    "v1 = np.array([1, 2, 3, 4, 5])\n",
    "v2 = np.array([2, 4, 6, 8, 10])  # v1çš„2å€\n",
    "v3 = np.array([5, 4, 3, 2, 1])   # v1çš„åå‘\n",
    "v4 = np.array([1, 1, 1, 1, 1])   # å¸¸æ•°å‘é‡\n",
    "v5 = np.random.randint(0, 2, 5)  # äºŒè¿›åˆ¶å‘é‡\n",
    "\n",
    "print(\"æµ‹è¯•å‘é‡:\")\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"v2: {v2} (v1çš„2å€)\")\n",
    "print(f\"v3: {v3} (v1çš„åå‘)\")\n",
    "print(f\"v4: {v4} (å¸¸æ•°)\")\n",
    "print(f\"v5: {v5} (äºŒè¿›åˆ¶)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659019b4",
   "metadata": {},
   "source": [
    "## 2. ç›¸ä¼¼åº¦åº¦é‡æ¯”è¾ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d689507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—æ‰€æœ‰å‘é‡å¯¹ä¹‹é—´çš„ç›¸ä¼¼åº¦/è·ç¦»\n",
    "vectors = [v1, v2, v3, v4, v5]\n",
    "vector_names = ['v1', 'v2 (2Ã—v1)', 'v3 (åå‘)', 'v4 (å¸¸æ•°)', 'v5 (äºŒè¿›åˆ¶)']\n",
    "\n",
    "metrics = {\n",
    "    'ä½™å¼¦ç›¸ä¼¼åº¦': SimilarityMetrics.cosine_similarity,\n",
    "    'ç‚¹ç§¯': SimilarityMetrics.dot_product,\n",
    "    'æ¬§å‡ é‡Œå¾—è·ç¦»': SimilarityMetrics.euclidean_distance,\n",
    "    'æ›¼å“ˆé¡¿è·ç¦»': SimilarityMetrics.manhattan_distance,\n",
    "    'åˆ‡æ¯”é›ªå¤«è·ç¦»': SimilarityMetrics.chebyshev_distance,\n",
    "    'Jaccardç›¸ä¼¼åº¦': SimilarityMetrics.jaccard_similarity\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for metric_name, metric_func in metrics.items():\n",
    "    matrix = np.zeros((len(vectors), len(vectors)))\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(len(vectors)):\n",
    "            try:\n",
    "                if metric_name == 'Jaccardç›¸ä¼¼åº¦':\n",
    "                    # å¯¹äºJaccardï¼Œéœ€è¦äºŒè¿›åˆ¶åŒ–\n",
    "                    v_i = (vectors[i] > 0).astype(int)\n",
    "                    v_j = (vectors[j] > 0).astype(int)\n",
    "                    value = metric_func(v_i, v_j)\n",
    "                else:\n",
    "                    value = metric_func(vectors[i], vectors[j])\n",
    "                matrix[i, j] = value\n",
    "            except:\n",
    "                matrix[i, j] = np.nan\n",
    "    results[metric_name] = matrix\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric_name, matrix) in enumerate(results.items()):\n",
    "    if idx < len(axes):\n",
    "        sns.heatmap(matrix, annot=True, fmt='.3f', \n",
    "                   xticklabels=vector_names, yticklabels=vector_names,\n",
    "                   ax=axes[idx], cmap='viridis')\n",
    "        axes[idx].set_title(metric_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a044231",
   "metadata": {},
   "source": [
    "## 3. ä½™å¼¦ç›¸ä¼¼åº¦ vs ç‚¹ç§¯çš„è¯¦ç»†åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d690090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸åŒé•¿åº¦ä½†æ–¹å‘ç›¸åŒçš„å‘é‡\n",
    "base_direction = np.array([1, 1, 1, 1])\n",
    "scales = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "scaled_vectors = [scale * base_direction for scale in scales]\n",
    "\n",
    "print(\"å‘é‡ç¼©æ”¾å®éªŒ:\")\n",
    "print(\"åŸºç¡€æ–¹å‘:\", base_direction)\n",
    "print(\"\\nç¼©æ”¾å› å­ | å‘é‡ | é•¿åº¦ | ä¸åŸºç¡€å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ | ç‚¹ç§¯\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "cosine_similarities = []\n",
    "dot_products = []\n",
    "vector_norms = []\n",
    "\n",
    "for scale, vector in zip(scales, scaled_vectors):\n",
    "    norm = np.linalg.norm(vector)\n",
    "    cosine_sim = SimilarityMetrics.cosine_similarity(base_direction, vector)\n",
    "    dot_prod = SimilarityMetrics.dot_product(base_direction, vector)\n",
    "    \n",
    "    vector_norms.append(norm)\n",
    "    cosine_similarities.append(cosine_sim)\n",
    "    dot_products.append(dot_prod)\n",
    "    \n",
    "    print(f\"{scale:8.1f} | {vector} | {norm:5.2f} | {cosine_sim:16.3f} | {dot_prod:6.1f}\")\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# å·¦å›¾: ä½™å¼¦ç›¸ä¼¼åº¦å’Œç‚¹ç§¯ vs ç¼©æ”¾å› å­\n",
    "ax1.plot(scales, cosine_similarities, 'o-', label='ä½™å¼¦ç›¸ä¼¼åº¦', linewidth=2)\n",
    "ax1.plot(scales, np.array(dot_products)/max(dot_products), 's-', label='ç‚¹ç§¯ (å½’ä¸€åŒ–)', linewidth=2)\n",
    "ax1.set_xlabel('ç¼©æ”¾å› å­')\n",
    "ax1.set_ylabel('ç›¸ä¼¼åº¦å€¼')\n",
    "ax1.set_title('ä½™å¼¦ç›¸ä¼¼åº¦ vs ç‚¹ç§¯ (æ–¹å‘ç›¸åŒçš„å‘é‡)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# å³å›¾: ç‚¹ç§¯ vs å‘é‡é•¿åº¦\n",
    "ax2.scatter(vector_norms, dot_products, s=100, alpha=0.7)\n",
    "ax2.set_xlabel('å‘é‡é•¿åº¦')\n",
    "ax2.set_ylabel('ç‚¹ç§¯å€¼')\n",
    "ax2.set_title('ç‚¹ç§¯ä¸å‘é‡é•¿åº¦çš„å…³ç³»')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# æ·»åŠ è¶‹åŠ¿çº¿\n",
    "z = np.polyfit(vector_norms, dot_products, 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(vector_norms, p(vector_norms), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\né‡è¦è§‚å¯Ÿ:\")\n",
    "print(\"- ä½™å¼¦ç›¸ä¼¼åº¦å¯¹å‘é‡é•¿åº¦ä¸æ•æ„Ÿï¼Œåªå…³æ³¨æ–¹å‘\")\n",
    "print(\"- ç‚¹ç§¯åŒæ—¶è€ƒè™‘æ–¹å‘å’Œå¤§å°\")\n",
    "print(\"- å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ç­‰äºç‚¹ç§¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4a87c",
   "metadata": {},
   "source": [
    "## 4. è·ç¦»åº¦é‡çš„å‡ ä½•ç›´è§‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Dç¤ºä¾‹ï¼šä¸åŒè·ç¦»åº¦é‡çš„å¯è§†åŒ–\n",
    "point_a = np.array([1, 1])\n",
    "point_b = np.array([4, 3])\n",
    "\n",
    "# è®¡ç®—å„ç§è·ç¦»\n",
    "euclidean_dist = SimilarityMetrics.euclidean_distance(point_a, point_b)\n",
    "manhattan_dist = SimilarityMetrics.manhattan_distance(point_a, point_b)\n",
    "chebyshev_dist = SimilarityMetrics.chebyshev_distance(point_a, point_b)\n",
    "\n",
    "print(f\"ç‚¹A: {point_a}, ç‚¹B: {point_b}\")\n",
    "print(f\"æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_dist:.3f}\")\n",
    "print(f\"æ›¼å“ˆé¡¿è·ç¦»: {manhattan_dist:.3f}\")\n",
    "print(f\"åˆ‡æ¯”é›ªå¤«è·ç¦»: {chebyshev_dist:.3f}\")\n",
    "\n",
    "# å¯è§†åŒ–è·ç¦»\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# æ¬§å‡ é‡Œå¾—è·ç¦»\n",
    "axes[0].plot([point_a[0], point_b[0]], [point_a[1], point_b[1]], 'r-', linewidth=3, label='ç›´çº¿è·ç¦»')\n",
    "axes[0].plot(point_a[0], point_a[1], 'bo', markersize=10, label='ç‚¹A')\n",
    "axes[0].plot(point_b[0], point_b[1], 'ro', markersize=10, label='ç‚¹B')\n",
    "axes[0].set_title(f'æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_dist:.3f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# æ›¼å“ˆé¡¿è·ç¦»\n",
    "axes[1].plot([point_a[0], point_b[0]], [point_a[1], point_a[1]], 'g-', linewidth=3, label='æ°´å¹³ç§»åŠ¨')\n",
    "axes[1].plot([point_b[0], point_b[0]], [point_a[1], point_b[1]], 'g-', linewidth=3, label='å‚ç›´ç§»åŠ¨')\n",
    "axes[1].plot(point_a[0], point_a[1], 'bo', markersize=10, label='ç‚¹A')\n",
    "axes[1].plot(point_b[0], point_b[1], 'ro', markersize=10, label='ç‚¹B')\n",
    "axes[1].set_title(f'æ›¼å“ˆé¡¿è·ç¦»: {manhattan_dist:.3f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# åˆ‡æ¯”é›ªå¤«è·ç¦»\n",
    "diff_x = abs(point_b[0] - point_a[0])\n",
    "diff_y = abs(point_b[1] - point_a[1])\n",
    "max_diff = max(diff_x, diff_y)\n",
    "\n",
    "# ç”»æ­£æ–¹å½¢è¡¨ç¤ºåˆ‡æ¯”é›ªå¤«è·ç¦»\n",
    "square_x = [point_a[0] - max_diff, point_a[0] + max_diff, point_a[0] + max_diff, point_a[0] - max_diff, point_a[0] - max_diff]\n",
    "square_y = [point_a[1] - max_diff, point_a[1] - max_diff, point_a[1] + max_diff, point_a[1] + max_diff, point_a[1] - max_diff]\n",
    "axes[2].plot(square_x, square_y, 'purple', linewidth=2, alpha=0.7, label='åˆ‡æ¯”é›ªå¤«è·ç¦»è¾¹ç•Œ')\n",
    "axes[2].plot(point_a[0], point_a[1], 'bo', markersize=10, label='ç‚¹A')\n",
    "axes[2].plot(point_b[0], point_b[1], 'ro', markersize=10, label='ç‚¹B')\n",
    "axes[2].set_title(f'åˆ‡æ¯”é›ªå¤«è·ç¦»: {chebyshev_dist:.3f}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677b127",
   "metadata": {},
   "source": [
    "## 5. é«˜ç»´ç©ºé—´ä¸­çš„è·ç¦»åº¦é‡è¡Œä¸º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eea7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»´åº¦è¯…å’’æ¼”ç¤º\n",
    "dimensions = [2, 5, 10, 20, 50, 100, 200]\n",
    "n_samples = 1000\n",
    "\n",
    "euclidean_stats = []\n",
    "cosine_stats = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"å¤„ç† {dim} ç»´...\")\n",
    "    \n",
    "    # ç”Ÿæˆéšæœºå‘é‡\n",
    "    vectors = np.random.randn(n_samples, dim)\n",
    "    \n",
    "    # è®¡ç®—æˆå¯¹è·ç¦»å’Œç›¸ä¼¼åº¦\n",
    "    euclidean_distances = []\n",
    "    cosine_similarities = []\n",
    "    \n",
    "    # é‡‡æ ·ä»¥å‡å°‘è®¡ç®—é‡\n",
    "    sample_size = min(100, n_samples)\n",
    "    sample_indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        for j in range(i+1, sample_size):\n",
    "            v1, v2 = vectors[sample_indices[i]], vectors[sample_indices[j]]\n",
    "            \n",
    "            euclidean_distances.append(SimilarityMetrics.euclidean_distance(v1, v2))\n",
    "            cosine_similarities.append(SimilarityMetrics.cosine_similarity(v1, v2))\n",
    "    \n",
    "    euclidean_stats.append({\n",
    "        'mean': np.mean(euclidean_distances),\n",
    "        'std': np.std(euclidean_distances),\n",
    "        'min': np.min(euclidean_distances),\n",
    "        'max': np.max(euclidean_distances)\n",
    "    })\n",
    "    \n",
    "    cosine_stats.append({\n",
    "        'mean': np.mean(cosine_similarities),\n",
    "        'std': np.std(cosine_similarities),\n",
    "        'min': np.min(cosine_similarities),\n",
    "        'max': np.max(cosine_similarities)\n",
    "    })\n",
    "\n",
    "# å¯è§†åŒ–ç»´åº¦æ•ˆåº”\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# æ¬§å‡ é‡Œå¾—è·ç¦»ç»Ÿè®¡\n",
    "euclidean_means = [stats['mean'] for stats in euclidean_stats]\n",
    "euclidean_stds = [stats['std'] for stats in euclidean_stats]\n",
    "\n",
    "ax1.plot(dimensions, euclidean_means, 'o-', linewidth=2, label='å‡å€¼')\n",
    "ax1.fill_between(dimensions, \n",
    "                np.array(euclidean_means) - np.array(euclidean_stds),\n",
    "                np.array(euclidean_means) + np.array(euclidean_stds),\n",
    "                alpha=0.3, label='Â±1æ ‡å‡†å·®')\n",
    "ax1.set_xlabel('ç»´åº¦')\n",
    "ax1.set_ylabel('æ¬§å‡ é‡Œå¾—è·ç¦»')\n",
    "ax1.set_title('æ¬§å‡ é‡Œå¾—è·ç¦» vs ç»´åº¦')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ä½™å¼¦ç›¸ä¼¼åº¦ç»Ÿè®¡\n",
    "cosine_means = [stats['mean'] for stats in cosine_stats]\n",
    "cosine_stds = [stats['std'] for stats in cosine_stats]\n",
    "\n",
    "ax2.plot(dimensions, cosine_means, 's-', linewidth=2, color='green', label='å‡å€¼')\n",
    "ax2.fill_between(dimensions, \n",
    "                np.array(cosine_means) - np.array(cosine_stds),\n",
    "                np.array(cosine_means) + np.array(cosine_stds),\n",
    "                alpha=0.3, color='green', label='Â±1æ ‡å‡†å·®')\n",
    "ax2.set_xlabel('ç»´åº¦')\n",
    "ax2.set_ylabel('ä½™å¼¦ç›¸ä¼¼åº¦')\n",
    "ax2.set_title('ä½™å¼¦ç›¸ä¼¼åº¦ vs ç»´åº¦')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# è·ç¦»åˆ†å¸ƒçš„å˜åŒ–\n",
    "euclidean_ranges = [stats['max'] - stats['min'] for stats in euclidean_stats]\n",
    "cosine_ranges = [stats['max'] - stats['min'] for stats in cosine_stats]\n",
    "\n",
    "ax3.plot(dimensions, euclidean_stds, 'o-', linewidth=2, label='æ¬§å‡ é‡Œå¾—è·ç¦»æ ‡å‡†å·®')\n",
    "ax3.plot(dimensions, cosine_stds, 's-', linewidth=2, label='ä½™å¼¦ç›¸ä¼¼åº¦æ ‡å‡†å·®')\n",
    "ax3.set_xlabel('ç»´åº¦')\n",
    "ax3.set_ylabel('æ ‡å‡†å·®')\n",
    "ax3.set_title('åˆ†å¸ƒç¦»æ•£ç¨‹åº¦ vs ç»´åº¦')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# ç›¸å¯¹æ ‡å‡†å·®ï¼ˆå˜å¼‚ç³»æ•°ï¼‰\n",
    "euclidean_cv = [std/mean for mean, std in zip(euclidean_means, euclidean_stds)]\n",
    "cosine_cv = [std/abs(mean) if mean != 0 else 0 for mean, std in zip(cosine_means, cosine_stds)]\n",
    "\n",
    "ax4.plot(dimensions, euclidean_cv, 'o-', linewidth=2, label='æ¬§å‡ é‡Œå¾—è·ç¦»CV')\n",
    "ax4.plot(dimensions, cosine_cv, 's-', linewidth=2, label='ä½™å¼¦ç›¸ä¼¼åº¦CV')\n",
    "ax4.set_xlabel('ç»´åº¦')\n",
    "ax4.set_ylabel('å˜å¼‚ç³»æ•° (CV)')\n",
    "ax4.set_title('ç›¸å¯¹å˜å¼‚æ€§ vs ç»´åº¦')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nç»´åº¦è¯…å’’è§‚å¯Ÿ:\")\n",
    "print(\"- é«˜ç»´ç©ºé—´ä¸­ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»è¶‹å‘äºç›¸åŒå€¼\")\n",
    "print(\"- ä½™å¼¦ç›¸ä¼¼åº¦åœ¨é«˜ç»´ä¸‹è¡¨ç°æ›´ç¨³å®š\")\n",
    "print(\"- è·ç¦»çš„åŒºåˆ†èƒ½åŠ›éšç»´åº¦å¢åŠ è€Œä¸‹é™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbb507",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½åŸºå‡†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292cec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½æµ‹è¯•\n",
    "def benchmark_metrics(n_vectors=1000, vector_dim=128, n_queries=100):\n",
    "    \"\"\"åŸºå‡†æµ‹è¯•ä¸åŒç›¸ä¼¼åº¦åº¦é‡çš„æ€§èƒ½\"\"\"\n",
    "    \n",
    "    # ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "    vectors = np.random.randn(n_vectors, vector_dim).astype(np.float32)\n",
    "    queries = np.random.randn(n_queries, vector_dim).astype(np.float32)\n",
    "    \n",
    "    # å½’ä¸€åŒ–ï¼ˆå¯¹æŸäº›åº¦é‡å¾ˆé‡è¦ï¼‰\n",
    "    vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    queries_norm = queries / np.linalg.norm(queries, axis=1, keepdims=True)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ‰‹åŠ¨å®ç°ï¼‰\n",
    "    start_time = time.time()\n",
    "    for query in queries_norm:\n",
    "        similarities = []\n",
    "        for vector in vectors_norm:\n",
    "            sim = np.dot(query, vector)  # å½’ä¸€åŒ–åçš„ç‚¹ç§¯ç­‰äºä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            similarities.append(sim)\n",
    "    manual_cosine_time = time.time() - start_time\n",
    "    results['æ‰‹åŠ¨ä½™å¼¦ç›¸ä¼¼åº¦'] = manual_cosine_time\n",
    "    \n",
    "    # 2. å‘é‡åŒ–ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    start_time = time.time()\n",
    "    similarities_matrix = np.dot(queries_norm, vectors_norm.T)\n",
    "    vectorized_cosine_time = time.time() - start_time\n",
    "    results['å‘é‡åŒ–ä½™å¼¦ç›¸ä¼¼åº¦'] = vectorized_cosine_time\n",
    "    \n",
    "    # 3. sklearnä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    start_time = time.time()\n",
    "    sklearn_similarities = cosine_similarity(queries, vectors)\n",
    "    sklearn_cosine_time = time.time() - start_time\n",
    "    results['sklearnä½™å¼¦ç›¸ä¼¼åº¦'] = sklearn_cosine_time\n",
    "    \n",
    "    # 4. æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆæ‰‹åŠ¨ï¼‰\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        distances = []\n",
    "        for vector in vectors:\n",
    "            dist = np.sqrt(np.sum((query - vector) ** 2))\n",
    "            distances.append(dist)\n",
    "    manual_euclidean_time = time.time() - start_time\n",
    "    results['æ‰‹åŠ¨æ¬§å‡ é‡Œå¾—è·ç¦»'] = manual_euclidean_time\n",
    "    \n",
    "    # 5. sklearnæ¬§å‡ é‡Œå¾—è·ç¦»\n",
    "    start_time = time.time()\n",
    "    sklearn_distances = euclidean_distances(queries, vectors)\n",
    "    sklearn_euclidean_time = time.time() - start_time\n",
    "    results['sklearnæ¬§å‡ é‡Œå¾—è·ç¦»'] = sklearn_euclidean_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æ‰§è¡ŒåŸºå‡†æµ‹è¯•\n",
    "print(\"æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...\")\n",
    "perf_results = benchmark_metrics(n_vectors=1000, vector_dim=128, n_queries=100)\n",
    "\n",
    "print(\"\\næ€§èƒ½ç»“æœ (ç§’):\")\n",
    "for method, time_taken in sorted(perf_results.items(), key=lambda x: x[1]):\n",
    "    print(f\"{method:20s}: {time_taken:.4f}s\")\n",
    "\n",
    "# å¯è§†åŒ–æ€§èƒ½ç»“æœ\n",
    "methods = list(perf_results.keys())\n",
    "times = list(perf_results.values())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(methods)), times, color=['red', 'green', 'blue', 'orange', 'purple'])\n",
    "plt.xlabel('æ–¹æ³•')\n",
    "plt.ylabel('æ—¶é—´ (ç§’)')\n",
    "plt.title('ç›¸ä¼¼åº¦åº¦é‡æ€§èƒ½æ¯”è¾ƒ')\n",
    "plt.xticks(range(len(methods)), methods, rotation=45, ha='right')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{time_val:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è®¡ç®—åŠ é€Ÿæ¯”\n",
    "baseline_time = perf_results['æ‰‹åŠ¨ä½™å¼¦ç›¸ä¼¼åº¦']\n",
    "print(\"\\nåŠ é€Ÿæ¯” (ç›¸å¯¹äºæ‰‹åŠ¨ä½™å¼¦ç›¸ä¼¼åº¦):\")\n",
    "for method, time_taken in perf_results.items():\n",
    "    speedup = baseline_time / time_taken\n",
    "    print(f\"{method:20s}: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd90566",
   "metadata": {},
   "source": [
    "## 7. åº¦é‡æ–¹æ³•é€‰æ‹©æŒ‡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºé€‰æ‹©æŒ‡å—è¡¨æ ¼\n",
    "import pandas as pd\n",
    "\n",
    "guidance_data = {\n",
    "    'åº¦é‡æ–¹æ³•': [\n",
    "        'ä½™å¼¦ç›¸ä¼¼åº¦',\n",
    "        'ç‚¹ç§¯',\n",
    "        'æ¬§å‡ é‡Œå¾—è·ç¦»',\n",
    "        'æ›¼å“ˆé¡¿è·ç¦»',\n",
    "        'åˆ‡æ¯”é›ªå¤«è·ç¦»',\n",
    "        'Jaccardç›¸ä¼¼åº¦'\n",
    "    ],\n",
    "    'é€‚ç”¨åœºæ™¯': [\n",
    "        'æ–‡æœ¬ç›¸ä¼¼åº¦ã€æ¨èç³»ç»Ÿ',\n",
    "        'å½’ä¸€åŒ–å‘é‡çš„å¿«é€Ÿç›¸ä¼¼åº¦',\n",
    "        'å‡ ä½•è·ç¦»ã€èšç±»',\n",
    "        'é«˜ç»´ç¨€ç–æ•°æ®',\n",
    "        'æ¸¸æˆAIã€è·¯å¾„è§„åˆ’',\n",
    "        'é›†åˆç›¸ä¼¼åº¦ã€äºŒè¿›åˆ¶ç‰¹å¾'\n",
    "    ],\n",
    "    'ä¼˜ç‚¹': [\n",
    "        'ä¸å—å‘é‡é•¿åº¦å½±å“',\n",
    "        'è®¡ç®—å¿«é€Ÿ',\n",
    "        'ç›´è§‚çš„å‡ ä½•æ„ä¹‰',\n",
    "        'å¯¹å¼‚å¸¸å€¼é²æ£’',\n",
    "        'å…³æ³¨æœ€å¤§å·®å¼‚',\n",
    "        'é€‚åˆç¨€ç–æ•°æ®'\n",
    "    ],\n",
    "    'ç¼ºç‚¹': [\n",
    "        'è®¡ç®—ç›¸å¯¹å¤æ‚',\n",
    "        'éœ€è¦å½’ä¸€åŒ–',\n",
    "        'ç»´åº¦è¯…å’’æ•æ„Ÿ',\n",
    "        'ä¸¢å¤±æ–¹å‘ä¿¡æ¯',\n",
    "        'åªè€ƒè™‘å•ä¸ªç»´åº¦',\n",
    "        'ä»…é€‚ç”¨äºäºŒè¿›åˆ¶æ•°æ®'\n",
    "    ],\n",
    "    'è®¡ç®—å¤æ‚åº¦': [\n",
    "        'O(d)',\n",
    "        'O(d)',\n",
    "        'O(d)',\n",
    "        'O(d)',\n",
    "        'O(d)',\n",
    "        'O(d)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "guidance_df = pd.DataFrame(guidance_data)\n",
    "print(\"ç›¸ä¼¼åº¦åº¦é‡é€‰æ‹©æŒ‡å—:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in guidance_df.iterrows():\n",
    "    print(f\"\\n{row['åº¦é‡æ–¹æ³•']}:\")\n",
    "    print(f\"  é€‚ç”¨åœºæ™¯: {row['é€‚ç”¨åœºæ™¯']}\")\n",
    "    print(f\"  ä¼˜ç‚¹: {row['ä¼˜ç‚¹']}\")\n",
    "    print(f\"  ç¼ºç‚¹: {row['ç¼ºç‚¹']}\")\n",
    "    print(f\"  å¤æ‚åº¦: {row['è®¡ç®—å¤æ‚åº¦']}\")\n",
    "\n",
    "# åˆ›å»ºå†³ç­–æ ‘å¯è§†åŒ–\n",
    "print(\"\\n\\nåº¦é‡æ–¹æ³•é€‰æ‹©å†³ç­–æ ‘:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "æ•°æ®ç±»å‹ï¼Ÿ\n",
    "â”œâ”€â”€ äºŒè¿›åˆ¶/é›†åˆ â†’ Jaccardç›¸ä¼¼åº¦\n",
    "â”œâ”€â”€ è¿ç»­æ•°å€¼\n",
    "â”‚   â”œâ”€â”€ å…³æ³¨æ–¹å‘ï¼ˆä¸å…³æ³¨å¤§å°ï¼‰â†’ ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "â”‚   â”œâ”€â”€ å…³æ³¨å¤§å°å’Œæ–¹å‘ â†’ ç‚¹ç§¯ï¼ˆå½’ä¸€åŒ–å‘é‡ï¼‰\n",
    "â”‚   â”œâ”€â”€ å‡ ä½•è·ç¦»\n",
    "â”‚   â”‚   â”œâ”€â”€ ä½ç»´åº¦ â†’ æ¬§å‡ é‡Œå¾—è·ç¦»\n",
    "â”‚   â”‚   â”œâ”€â”€ é«˜ç»´åº¦/ç¨€ç– â†’ æ›¼å“ˆé¡¿è·ç¦»\n",
    "â”‚   â”‚   â””â”€â”€ æœ€å¤§å·®å¼‚é‡è¦ â†’ åˆ‡æ¯”é›ªå¤«è·ç¦»\n",
    "â”‚   â””â”€â”€ æ–‡æœ¬/è¯­ä¹‰ç›¸ä¼¼åº¦ â†’ ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nå®ç”¨å»ºè®®:\")\n",
    "print(\"1. æ–‡æœ¬ç›¸ä¼¼åº¦: ä¼˜å…ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦\")\n",
    "print(\"2. æ¨èç³»ç»Ÿ: ä½™å¼¦ç›¸ä¼¼åº¦æˆ–ç‚¹ç§¯\")\n",
    "print(\"3. å›¾åƒç›¸ä¼¼åº¦: æ¬§å‡ é‡Œå¾—è·ç¦»æˆ–ä½™å¼¦ç›¸ä¼¼åº¦\")\n",
    "print(\"4. é«˜ç»´ç¨€ç–æ•°æ®: æ›¼å“ˆé¡¿è·ç¦»\")\n",
    "print(\"5. æ€§èƒ½è¦æ±‚é«˜: ç‚¹ç§¯ï¼ˆå½’ä¸€åŒ–å‘é‡ï¼‰\")\n",
    "print(\"6. éœ€è¦å‘é‡åŒ–è®¡ç®—: numpyæˆ–sklearnå®ç°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e67c9b",
   "metadata": {},
   "source": [
    "## 8. æ€»ç»“\n",
    "\n",
    "æœ¬æ•™ç¨‹æ·±å…¥æ¢è®¨äº†å„ç§ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•:\n",
    "\n",
    "### ä¸»è¦æ”¶è·\n",
    "1. **ä½™å¼¦ç›¸ä¼¼åº¦**: æœ€å¸¸ç”¨äºæ–‡æœ¬å’Œé«˜ç»´æ•°æ®ï¼Œä¸å—å‘é‡é•¿åº¦å½±å“\n",
    "2. **ç‚¹ç§¯**: è€ƒè™‘å‘é‡å¤§å°ï¼Œå½’ä¸€åŒ–åç­‰äºä½™å¼¦ç›¸ä¼¼åº¦\n",
    "3. **è·ç¦»åº¦é‡**: å„æœ‰ç‰¹ç‚¹ï¼Œé€‰æ‹©å–å†³äºæ•°æ®ç±»å‹å’Œåº”ç”¨åœºæ™¯\n",
    "4. **ç»´åº¦è¯…å’’**: é«˜ç»´ç©ºé—´ä¸­æ¬§å‡ é‡Œå¾—è·ç¦»å¤±æ•ˆï¼Œä½™å¼¦ç›¸ä¼¼åº¦æ›´ç¨³å®š\n",
    "5. **æ€§èƒ½ä¼˜åŒ–**: å‘é‡åŒ–è®¡ç®—æ¯”å¾ªç¯å¿«æ•°åå€\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "- æ–‡æœ¬æ•°æ®: ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "- å›¾åƒæ•°æ®: æ¬§å‡ é‡Œå¾—è·ç¦»æˆ–ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "- ç¨€ç–æ•°æ®: æ›¼å“ˆé¡¿è·ç¦»æˆ–Jaccardç›¸ä¼¼åº¦\n",
    "- æ€§èƒ½ä¼˜å…ˆ: ä½¿ç”¨å‘é‡åŒ–å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ ç›¸ä¼¼åº¦åº¦é‡æ•™ç¨‹å®Œæˆï¼\")\n",
    "print(\"ğŸ“š ä¸‹ä¸€æ­¥: å­¦ä¹  03_text_embeddings.ipynb\")\n",
    "print(\"ğŸ’¡ ç»ƒä¹ : å°è¯•åœ¨è‡ªå·±çš„æ•°æ®ä¸Šæ¯”è¾ƒä¸åŒåº¦é‡æ–¹æ³•\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
