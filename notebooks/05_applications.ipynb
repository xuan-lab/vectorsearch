{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2be8d44",
   "metadata": {},
   "source": [
    "# 向量搜索应用集成\n",
    "# Vector Search Applications Integration\n",
    "\n",
    "这个笔记本展示如何将所有组件集成为完整的向量搜索应用系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8634cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所有必要的库\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from src.basic_vector_search import BasicVectorSearch\n",
    "from src.text_vectorizer import TextVectorizer\n",
    "from src.utils import load_json, save_json\n",
    "from examples.document_search import DocumentSearchSystem\n",
    "from examples.semantic_search import SemanticSearchSystem\n",
    "from examples.recommendation import ContentRecommendationSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eeab99",
   "metadata": {},
   "source": [
    "## 1. 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060796a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载示例数据\n",
    "documents = load_json('../data/sample_documents.json')\n",
    "print(f\"加载了 {len(documents)} 个文档\")\n",
    "\n",
    "# 显示文档结构\n",
    "if documents:\n",
    "    print(\"\\n文档结构示例:\")\n",
    "    sample_doc = documents[0]\n",
    "    for key, value in sample_doc.items():\n",
    "        if isinstance(value, str) and len(value) > 50:\n",
    "            print(f\"{key}: {value[:50]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c93e8",
   "metadata": {},
   "source": [
    "## 2. 文档搜索系统演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化文档搜索系统\n",
    "print(\"=== 文档搜索系统演示 ===\")\n",
    "\n",
    "doc_search = DocumentSearchSystem(documents)\n",
    "print(f\"文档搜索系统已初始化，包含 {len(documents)} 个文档\")\n",
    "\n",
    "# 测试不同类型的搜索\n",
    "test_queries = [\n",
    "    \"人工智能\",\n",
    "    \"机器学习算法\",\n",
    "    \"数据分析\"\n",
    "]\n",
    "\n",
    "search_types = ['keyword', 'semantic', 'hybrid']\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n查询: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for search_type in search_types:\n",
    "        print(f\"\\n{search_type.upper()} 搜索结果:\")\n",
    "        try:\n",
    "            results = doc_search.search(query, method=search_type, top_k=3)\n",
    "            \n",
    "            if results:\n",
    "                for i, result in enumerate(results):\n",
    "                    title = result.get('title', f\"文档{result.get('doc_id', 'N/A')}\")\n",
    "                    score = result.get('score', 0)\n",
    "                    print(f\"  {i+1}. {title} (得分: {score:.3f})\")\n",
    "            else:\n",
    "                print(\"  未找到相关结果\")\n",
    "        except Exception as e:\n",
    "            print(f\"  搜索出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd0ec5",
   "metadata": {},
   "source": [
    "## 3. 语义搜索系统演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化语义搜索系统\n",
    "print(\"\\n=== 语义搜索系统演示 ===\")\n",
    "\n",
    "try:\n",
    "    semantic_search = SemanticSearchSystem()\n",
    "    \n",
    "    # 添加文档\n",
    "    for doc in documents:\n",
    "        semantic_search.add_document(\n",
    "            content=doc['content'],\n",
    "            metadata={\n",
    "                'title': doc.get('title', ''),\n",
    "                'category': doc.get('category', ''),\n",
    "                'doc_id': doc.get('id', '')\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"语义搜索系统已初始化，包含 {len(documents)} 个文档\")\n",
    "    \n",
    "    # 测试语义搜索\n",
    "    semantic_queries = [\n",
    "        \"深度学习模型训练\",\n",
    "        \"自然语言处理技术\",\n",
    "        \"计算机视觉应用\"\n",
    "    ]\n",
    "    \n",
    "    for query in semantic_queries:\n",
    "        print(f\"\\n语义查询: '{query}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        results = semantic_search.search(query, top_k=3)\n",
    "        \n",
    "        if results:\n",
    "            for i, result in enumerate(results):\n",
    "                title = result['metadata'].get('title', f\"文档{i+1}\")\n",
    "                similarity = result['similarity']\n",
    "                content_preview = result['content'][:100] + \"...\"\n",
    "                print(f\"  {i+1}. {title}\")\n",
    "                print(f\"     相似度: {similarity:.3f}\")\n",
    "                print(f\"     内容: {content_preview}\")\n",
    "        else:\n",
    "            print(\"  未找到相关结果\")\n",
    "    \n",
    "    # 文档聚类分析\n",
    "    print(\"\\n=== 文档聚类分析 ===\")\n",
    "    try:\n",
    "        clusters = semantic_search.cluster_documents(n_clusters=3)\n",
    "        \n",
    "        print(f\"将文档分为 {len(clusters)} 个聚类:\")\n",
    "        for cluster_id, cluster_docs in clusters.items():\n",
    "            print(f\"\\n聚类 {cluster_id} ({len(cluster_docs)} 个文档):\")\n",
    "            for doc_idx in cluster_docs[:3]:  # 显示前3个\n",
    "                if doc_idx < len(documents):\n",
    "                    title = documents[doc_idx].get('title', f\"文档{doc_idx}\")\n",
    "                    print(f\"  - {title}\")\n",
    "    except Exception as e:\n",
    "        print(f\"聚类分析出错: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"语义搜索系统初始化失败: {e}\")\n",
    "    print(\"可能需要安装sentence-transformers: pip install sentence-transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89176ff",
   "metadata": {},
   "source": [
    "## 4. 推荐系统演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0cb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化推荐系统\n",
    "print(\"\\n=== 内容推荐系统演示 ===\")\n",
    "\n",
    "try:\n",
    "    rec_system = ContentRecommendationSystem(documents, vector_type='tfidf')\n",
    "    print(f\"推荐系统已初始化，包含 {len(documents)} 个文档\")\n",
    "    \n",
    "    # 模拟用户行为\n",
    "    print(\"\\n模拟用户行为...\")\n",
    "    \n",
    "    # 用户1: 对AI和机器学习感兴趣\n",
    "    user1_id = \"user_ai_enthusiast\"\n",
    "    ai_related_docs = [i for i, doc in enumerate(documents) \n",
    "                      if any(keyword in doc.get('content', '').lower() \n",
    "                            for keyword in ['人工智能', '机器学习', 'ai', '算法'])]\n",
    "    \n",
    "    for doc_id in ai_related_docs[:3]:  # 与前3个AI相关文档交互\n",
    "        rec_system.add_user_interaction(user1_id, doc_id, 'like')\n",
    "    \n",
    "    # 用户2: 对数据分析感兴趣\n",
    "    user2_id = \"user_data_analyst\"\n",
    "    data_related_docs = [i for i, doc in enumerate(documents) \n",
    "                        if any(keyword in doc.get('content', '').lower() \n",
    "                              for keyword in ['数据', '分析', '统计', '可视化'])]\n",
    "    \n",
    "    for doc_id in data_related_docs[:3]:  # 与前3个数据相关文档交互\n",
    "        rec_system.add_user_interaction(user2_id, doc_id, 'like')\n",
    "    \n",
    "    print(f\"为用户 {user1_id} 和 {user2_id} 模拟了交互行为\")\n",
    "    \n",
    "    # 生成推荐\n",
    "    users = [user1_id, user2_id]\n",
    "    strategies = ['content', 'popularity', 'hybrid']\n",
    "    \n",
    "    for user_id in users:\n",
    "        print(f\"\\n=== {user_id} 的推荐结果 ===\")\n",
    "        \n",
    "        # 显示用户统计\n",
    "        user_stats = rec_system.get_user_statistics(user_id)\n",
    "        if 'error' not in user_stats:\n",
    "            print(f\"用户统计: {user_stats['total_interactions']} 次交互, \"\n",
    "                  f\"{user_stats['liked_items_count']} 个喜欢的物品\")\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            print(f\"\\n{strategy.upper()} 策略推荐:\")\n",
    "            try:\n",
    "                recommendations = rec_system.recommend_items(user_id, 3, strategy)\n",
    "                \n",
    "                if recommendations:\n",
    "                    for i, rec in enumerate(recommendations):\n",
    "                        doc = rec['document']\n",
    "                        title = doc.get('title', f\"文档{rec['item_id']}\")\n",
    "                        similarity = rec['similarity']\n",
    "                        reason = rec['reason']\n",
    "                        print(f\"  {i+1}. {title}\")\n",
    "                        print(f\"     相似度: {similarity:.3f}\")\n",
    "                        print(f\"     推荐原因: {reason}\")\n",
    "                else:\n",
    "                    print(\"  暂无推荐\")\n",
    "            except Exception as e:\n",
    "                print(f\"  推荐生成出错: {e}\")\n",
    "    \n",
    "    # 推荐解释\n",
    "    print(f\"\\n=== 推荐解释示例 ===\")\n",
    "    if len(documents) > 0:\n",
    "        item_to_explain = 0  # 解释第一个文档的推荐\n",
    "        explanation = rec_system.explain_recommendation(user1_id, item_to_explain)\n",
    "        \n",
    "        print(f\"为用户 {user1_id} 推荐文档 {item_to_explain} 的原因:\")\n",
    "        print(json.dumps(explanation, indent=2, ensure_ascii=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"推荐系统初始化失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa12b9",
   "metadata": {},
   "source": [
    "## 5. 系统性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 综合性能分析\n",
    "print(\"\\n=== 系统性能分析 ===\")\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def measure_system_performance():\n",
    "    \"\"\"测量系统资源使用情况\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    return {\n",
    "        'memory_mb': process.memory_info().rss / 1024 / 1024,\n",
    "        'cpu_percent': process.cpu_percent(),\n",
    "        'num_threads': process.num_threads()\n",
    "    }\n",
    "\n",
    "# 测试各系统的初始化时间和资源使用\n",
    "systems_performance = {}\n",
    "\n",
    "# 1. 基础向量搜索\n",
    "print(\"测试基础向量搜索性能...\")\n",
    "start_time = time.time()\n",
    "start_perf = measure_system_performance()\n",
    "\n",
    "basic_search = BasicVectorSearch()\n",
    "# 生成测试向量\n",
    "test_vectors = np.random.randn(1000, 128)\n",
    "basic_search.add_vectors(test_vectors)\n",
    "\n",
    "# 执行搜索\n",
    "query = np.random.randn(128)\n",
    "_ = basic_search.search(query, k=10)\n",
    "\n",
    "end_time = time.time()\n",
    "end_perf = measure_system_performance()\n",
    "\n",
    "systems_performance['基础搜索'] = {\n",
    "    'init_time': end_time - start_time,\n",
    "    'memory_usage': end_perf['memory_mb'] - start_perf['memory_mb'],\n",
    "    'total_memory': end_perf['memory_mb']\n",
    "}\n",
    "\n",
    "# 2. 文本向量化\n",
    "print(\"测试文本向量化性能...\")\n",
    "start_time = time.time()\n",
    "start_perf = measure_system_performance()\n",
    "\n",
    "vectorizer = TextVectorizer(method='tfidf')\n",
    "sample_texts = [doc['content'] for doc in documents[:100]]  # 使用前100个文档\n",
    "vectors = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "end_time = time.time()\n",
    "end_perf = measure_system_performance()\n",
    "\n",
    "systems_performance['文本向量化'] = {\n",
    "    'init_time': end_time - start_time,\n",
    "    'memory_usage': end_perf['memory_mb'] - start_perf['memory_mb'],\n",
    "    'total_memory': end_perf['memory_mb']\n",
    "}\n",
    "\n",
    "# 打印性能报告\n",
    "print(\"\\n性能报告:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'系统':<15} {'初始化时间(秒)':<15} {'内存增量(MB)':<15} {'总内存(MB)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for system_name, perf in systems_performance.items():\n",
    "    print(f\"{system_name:<15} {perf['init_time']:<15.3f} {perf['memory_usage']:<15.1f} {perf['total_memory']:<15.1f}\")\n",
    "\n",
    "# 可视化性能数据\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "systems = list(systems_performance.keys())\n",
    "init_times = [systems_performance[s]['init_time'] for s in systems]\n",
    "memory_usage = [systems_performance[s]['memory_usage'] for s in systems]\n",
    "\n",
    "# 初始化时间\n",
    "ax1.bar(systems, init_times)\n",
    "ax1.set_ylabel('初始化时间 (秒)')\n",
    "ax1.set_title('系统初始化时间比较')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 内存使用\n",
    "ax2.bar(systems, memory_usage)\n",
    "ax2.set_ylabel('内存增量 (MB)')\n",
    "ax2.set_title('系统内存使用比较')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec7851",
   "metadata": {},
   "source": [
    "## 6. 端到端应用演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个简单的端到端应用\n",
    "print(\"\\n=== 端到端应用演示 ===\")\n",
    "\n",
    "class IntegratedSearchSystem:\n",
    "    \"\"\"集成搜索系统\"\"\"\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.doc_search = DocumentSearchSystem(documents)\n",
    "        \n",
    "        # 尝试初始化语义搜索\n",
    "        try:\n",
    "            self.semantic_search = SemanticSearchSystem()\n",
    "            for doc in documents:\n",
    "                self.semantic_search.add_document(\n",
    "                    content=doc['content'],\n",
    "                    metadata={'title': doc.get('title', ''), 'id': doc.get('id', '')}\n",
    "                )\n",
    "            self.has_semantic = True\n",
    "        except:\n",
    "            self.has_semantic = False\n",
    "            print(\"语义搜索不可用，仅使用关键词搜索\")\n",
    "        \n",
    "        # 初始化推荐系统\n",
    "        try:\n",
    "            self.rec_system = ContentRecommendationSystem(documents)\n",
    "            self.has_recommendation = True\n",
    "        except:\n",
    "            self.has_recommendation = False\n",
    "            print(\"推荐系统不可用\")\n",
    "    \n",
    "    def unified_search(self, query, search_type='auto', top_k=5):\n",
    "        \"\"\"统一搜索接口\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        if search_type == 'auto':\n",
    "            # 自动选择搜索方式\n",
    "            if self.has_semantic:\n",
    "                search_type = 'semantic'\n",
    "            else:\n",
    "                search_type = 'keyword'\n",
    "        \n",
    "        if search_type == 'keyword':\n",
    "            results = self.doc_search.search(query, method='keyword', top_k=top_k)\n",
    "        elif search_type == 'semantic' and self.has_semantic:\n",
    "            semantic_results = self.semantic_search.search(query, top_k=top_k)\n",
    "            results = [{\n",
    "                'title': r['metadata'].get('title', ''),\n",
    "                'score': r['similarity'],\n",
    "                'content': r['content'][:200] + '...',\n",
    "                'type': 'semantic'\n",
    "            } for r in semantic_results]\n",
    "        elif search_type == 'hybrid':\n",
    "            results = self.doc_search.search(query, method='hybrid', top_k=top_k)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_recommendations(self, user_id, num_recs=5):\n",
    "        \"\"\"获取推荐\"\"\"\n",
    "        if not self.has_recommendation:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            return self.rec_system.recommend_items(user_id, num_recs, 'hybrid')\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def add_user_feedback(self, user_id, doc_id, feedback_type):\n",
    "        \"\"\"添加用户反馈\"\"\"\n",
    "        if self.has_recommendation:\n",
    "            self.rec_system.add_user_interaction(user_id, doc_id, feedback_type)\n",
    "\n",
    "# 初始化集成系统\n",
    "integrated_system = IntegratedSearchSystem(documents)\n",
    "print(\"集成搜索系统已初始化\")\n",
    "\n",
    "# 演示统一搜索\n",
    "demo_queries = [\"机器学习\", \"数据分析\", \"人工智能应用\"]\n",
    "\n",
    "for query in demo_queries:\n",
    "    print(f\"\\n搜索查询: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = integrated_system.unified_search(query, top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results):\n",
    "            title = result.get('title', '无标题')\n",
    "            score = result.get('score', 0)\n",
    "            result_type = result.get('type', 'keyword')\n",
    "            print(f\"  {i+1}. {title} [{result_type}]\")\n",
    "            print(f\"     得分: {score:.3f}\")\n",
    "            if 'content' in result:\n",
    "                print(f\"     摘要: {result['content'][:100]}...\")\n",
    "    else:\n",
    "        print(\"  未找到相关结果\")\n",
    "\n",
    "# 演示推荐功能\n",
    "if integrated_system.has_recommendation:\n",
    "    print(\"\\n=== 推荐演示 ===\")\n",
    "    \n",
    "    # 模拟用户交互\n",
    "    demo_user = \"demo_user\"\n",
    "    integrated_system.add_user_feedback(demo_user, 0, 'like')\n",
    "    integrated_system.add_user_feedback(demo_user, 1, 'view')\n",
    "    \n",
    "    recommendations = integrated_system.get_recommendations(demo_user, 3)\n",
    "    \n",
    "    if recommendations:\n",
    "        print(f\"为用户 {demo_user} 的推荐:\")\n",
    "        for i, rec in enumerate(recommendations):\n",
    "            doc = rec['document']\n",
    "            title = doc.get('title', f\"文档{rec['item_id']}\")\n",
    "            print(f\"  {i+1}. {title} (相似度: {rec['similarity']:.3f})\")\n",
    "    else:\n",
    "        print(\"暂无推荐\")\n",
    "else:\n",
    "    print(\"\\n推荐功能不可用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd53941",
   "metadata": {},
   "source": [
    "## 7. 总结和部署建议\n",
    "\n",
    "### 系统特点:\n",
    "- **模块化设计**: 各组件可独立使用\n",
    "- **灵活配置**: 支持多种搜索和推荐策略\n",
    "- **可扩展**: 易于添加新功能和算法\n",
    "\n",
    "### 部署建议:\n",
    "1. **开发环境**: 使用基础搜索和TF-IDF\n",
    "2. **生产环境**: 集成FAISS和语义搜索\n",
    "3. **大规模部署**: 考虑分布式架构\n",
    "\n",
    "### 性能优化:\n",
    "- 使用向量缓存\n",
    "- 批量处理\n",
    "- 异步搜索\n",
    "- 索引预热\n",
    "\n",
    "### 监控指标:\n",
    "- 搜索延迟\n",
    "- 内存使用\n",
    "- 索引大小\n",
    "- 用户满意度\n",
    "\n",
    "这个向量搜索项目提供了完整的基础架构，可以根据具体需求进行定制和扩展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6440240",
   "metadata": {},
   "source": [
    "# 向量搜索实际应用案例\n",
    "\n",
    "本notebook展示向量搜索在实际场景中的应用，包括文档搜索、推荐系统、语义检索等。\n",
    "\n",
    "## 学习目标\n",
    "- 掌握文档搜索系统的构建\n",
    "- 理解推荐系统中的向量化应用\n",
    "- 学习语义搜索的实现方法\n",
    "- 了解向量搜索的评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1873e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from src.basic_vector_search import BasicVectorSearch\n",
    "from src.advanced_search import FAISSSearch\n",
    "from src.text_vectorizer import TextVectorizer\n",
    "from src.utils import load_documents, VectorSearchBenchmark\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False   # 用来正常显示负号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfc57e",
   "metadata": {},
   "source": [
    "## 1. 文档搜索系统\n",
    "\n",
    "构建一个完整的文档搜索系统，支持关键词搜索和语义搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06154ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSearchSystem:\n",
    "    \"\"\"文档搜索系统\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.vectorizer = TextVectorizer()\n",
    "        self.tfidf_vectors = None\n",
    "        self.semantic_vectors = None\n",
    "        self.basic_search = None\n",
    "        self.faiss_search = None\n",
    "        \n",
    "    def load_documents(self, documents):\n",
    "        \"\"\"加载文档\"\"\"\n",
    "        self.documents = documents\n",
    "        texts = [doc['content'] for doc in documents]\n",
    "        \n",
    "        print(\"正在构建TF-IDF索引...\")\n",
    "        self.tfidf_vectors = self.vectorizer.tfidf_vectorize(texts)\n",
    "        self.basic_search = BasicVectorSearch()\n",
    "        self.basic_search.add_vectors(self.tfidf_vectors.toarray())\n",
    "        \n",
    "        print(\"正在构建语义向量索引...\")\n",
    "        try:\n",
    "            self.semantic_vectors = self.vectorizer.sentence_transformer_vectorize(texts)\n",
    "            self.faiss_search = FAISSSearch(vector_dim=self.semantic_vectors.shape[1])\n",
    "            self.faiss_search.add_vectors(self.semantic_vectors)\n",
    "        except Exception as e:\n",
    "            print(f\"语义向量构建失败: {e}\")\n",
    "            print(\"将使用TF-IDF作为备选\")\n",
    "            normalized_tfidf = normalize(self.tfidf_vectors.toarray().astype('float32'))\n",
    "            self.faiss_search = FAISSSearch(vector_dim=normalized_tfidf.shape[1])\n",
    "            self.faiss_search.add_vectors(normalized_tfidf)\n",
    "            self.semantic_vectors = normalized_tfidf\n",
    "    \n",
    "    def keyword_search(self, query, top_k=5):\n",
    "        \"\"\"关键词搜索\"\"\"\n",
    "        query_vector = self.vectorizer.tfidf_vectorize([query])\n",
    "        similarities, indices = self.basic_search.search(query_vector.toarray()[0], top_k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (idx, sim) in enumerate(zip(indices, similarities)):\n",
    "            if idx < len(self.documents):\n",
    "                results.append({\n",
    "                    'rank': i + 1,\n",
    "                    'document': self.documents[idx],\n",
    "                    'similarity': sim,\n",
    "                    'type': 'keyword'\n",
    "                })\n",
    "        return results\n",
    "    \n",
    "    def semantic_search(self, query, top_k=5):\n",
    "        \"\"\"语义搜索\"\"\"\n",
    "        try:\n",
    "            query_vector = self.vectorizer.sentence_transformer_vectorize([query])\n",
    "        except:\n",
    "            query_vector = self.vectorizer.tfidf_vectorize([query])\n",
    "            query_vector = normalize(query_vector.toarray().astype('float32'))\n",
    "        \n",
    "        distances, indices = self.faiss_search.search(query_vector, top_k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "            if idx < len(self.documents):\n",
    "                # 转换距离为相似度\n",
    "                similarity = 1 / (1 + dist)\n",
    "                results.append({\n",
    "                    'rank': i + 1,\n",
    "                    'document': self.documents[idx],\n",
    "                    'similarity': similarity,\n",
    "                    'distance': dist,\n",
    "                    'type': 'semantic'\n",
    "                })\n",
    "        return results\n",
    "    \n",
    "    def hybrid_search(self, query, top_k=5, alpha=0.7):\n",
    "        \"\"\"混合搜索：结合关键词和语义搜索\"\"\"\n",
    "        keyword_results = self.keyword_search(query, top_k*2)\n",
    "        semantic_results = self.semantic_search(query, top_k*2)\n",
    "        \n",
    "        # 合并结果并重新排序\n",
    "        combined_scores = defaultdict(float)\n",
    "        doc_info = {}\n",
    "        \n",
    "        for result in keyword_results:\n",
    "            doc_id = result['document']['id']\n",
    "            combined_scores[doc_id] += alpha * result['similarity']\n",
    "            doc_info[doc_id] = result['document']\n",
    "        \n",
    "        for result in semantic_results:\n",
    "            doc_id = result['document']['id']\n",
    "            combined_scores[doc_id] += (1 - alpha) * result['similarity']\n",
    "            doc_info[doc_id] = result['document']\n",
    "        \n",
    "        # 排序并返回top_k\n",
    "        sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for i, (doc_id, score) in enumerate(sorted_docs):\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'document': doc_info[doc_id],\n",
    "                'similarity': score,\n",
    "                'type': 'hybrid'\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# 初始化搜索系统\n",
    "search_system = DocumentSearchSystem()\n",
    "documents = load_documents('../data/sample_documents.json')\n",
    "search_system.load_documents(documents)\n",
    "\n",
    "print(f\"\\n文档搜索系统已准备就绪，共加载 {len(documents)} 个文档\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ccab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不同的搜索方法\n",
    "test_queries = [\n",
    "    \"人工智能发展\",\n",
    "    \"体育运动健康\",\n",
    "    \"艺术创作灵感\",\n",
    "    \"科技创新应用\"\n",
    "]\n",
    "\n",
    "def display_search_results(results, query, method_name):\n",
    "    \"\"\"显示搜索结果\"\"\"\n",
    "    print(f\"\\n{method_name} - 查询: '{query}'\")\n",
    "    print(\"=\"*60)\n",
    "    for result in results:\n",
    "        doc = result['document']\n",
    "        print(f\"{result['rank']}. {doc['title']} (相似度: {result['similarity']:.3f})\")\n",
    "        print(f\"   类别: {doc['category']}\")\n",
    "        print(f\"   内容: {doc['content'][:80]}...\")\n",
    "        print()\n",
    "\n",
    "# 对每个查询测试三种搜索方法\n",
    "query = test_queries[0]\n",
    "print(f\"测试查询: '{query}'\")\n",
    "\n",
    "# 关键词搜索\n",
    "keyword_results = search_system.keyword_search(query, top_k=3)\n",
    "display_search_results(keyword_results, query, \"关键词搜索\")\n",
    "\n",
    "# 语义搜索\n",
    "semantic_results = search_system.semantic_search(query, top_k=3)\n",
    "display_search_results(semantic_results, query, \"语义搜索\")\n",
    "\n",
    "# 混合搜索\n",
    "hybrid_results = search_system.hybrid_search(query, top_k=3)\n",
    "display_search_results(hybrid_results, query, \"混合搜索\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3de53",
   "metadata": {},
   "source": [
    "## 2. 推荐系统\n",
    "\n",
    "基于向量相似度的内容推荐系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae67a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentRecommendationSystem:\n",
    "    \"\"\"基于内容的推荐系统\"\"\"\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.vectorizer = TextVectorizer()\n",
    "        self.item_vectors = None\n",
    "        self.user_profiles = {}\n",
    "        self._build_item_vectors()\n",
    "    \n",
    "    def _build_item_vectors(self):\n",
    "        \"\"\"构建物品向量\"\"\"\n",
    "        print(\"构建物品向量...\")\n",
    "        texts = [doc['content'] for doc in self.documents]\n",
    "        \n",
    "        try:\n",
    "            self.item_vectors = self.vectorizer.sentence_transformer_vectorize(texts)\n",
    "        except:\n",
    "            tfidf_vectors = self.vectorizer.tfidf_vectorize(texts)\n",
    "            self.item_vectors = normalize(tfidf_vectors.toarray().astype('float32'))\n",
    "        \n",
    "        print(f\"物品向量维度: {self.item_vectors.shape}\")\n",
    "    \n",
    "    def create_user_profile(self, user_id, liked_items, disliked_items=None):\n",
    "        \"\"\"创建用户画像\"\"\"\n",
    "        liked_vectors = [self.item_vectors[i] for i in liked_items if i < len(self.item_vectors)]\n",
    "        \n",
    "        if not liked_vectors:\n",
    "            return None\n",
    "        \n",
    "        # 用户画像为喜欢物品的平均向量\n",
    "        user_vector = np.mean(liked_vectors, axis=0)\n",
    "        \n",
    "        # 如果有不喜欢的物品，从用户画像中减去\n",
    "        if disliked_items:\n",
    "            disliked_vectors = [self.item_vectors[i] for i in disliked_items if i < len(self.item_vectors)]\n",
    "            if disliked_vectors:\n",
    "                disliked_mean = np.mean(disliked_vectors, axis=0)\n",
    "                user_vector = user_vector - 0.3 * disliked_mean  # 负反馈权重较小\n",
    "        \n",
    "        # 归一化\n",
    "        user_vector = user_vector / np.linalg.norm(user_vector)\n",
    "        \n",
    "        self.user_profiles[user_id] = {\n",
    "            'vector': user_vector,\n",
    "            'liked_items': liked_items,\n",
    "            'disliked_items': disliked_items or []\n",
    "        }\n",
    "        \n",
    "        return user_vector\n",
    "    \n",
    "    def recommend_items(self, user_id, top_k=5, exclude_seen=True):\n",
    "        \"\"\"为用户推荐物品\"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            return []\n",
    "        \n",
    "        user_vector = self.user_profiles[user_id]['vector']\n",
    "        liked_items = set(self.user_profiles[user_id]['liked_items'])\n",
    "        disliked_items = set(self.user_profiles[user_id]['disliked_items'])\n",
    "        \n",
    "        # 计算与所有物品的相似度\n",
    "        similarities = cosine_similarity([user_vector], self.item_vectors)[0]\n",
    "        \n",
    "        # 创建推荐列表\n",
    "        recommendations = []\n",
    "        for i, sim in enumerate(similarities):\n",
    "            # 排除已经交互过的物品\n",
    "            if exclude_seen and (i in liked_items or i in disliked_items):\n",
    "                continue\n",
    "            \n",
    "            recommendations.append({\n",
    "                'item_id': i,\n",
    "                'document': self.documents[i],\n",
    "                'similarity': sim,\n",
    "                'score': sim\n",
    "            })\n",
    "        \n",
    "        # 按相似度排序\n",
    "        recommendations.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        return recommendations[:top_k]\n",
    "    \n",
    "    def explain_recommendation(self, user_id, item_id):\n",
    "        \"\"\"解释推荐原因\"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            return \"用户画像不存在\"\n",
    "        \n",
    "        user_profile = self.user_profiles[user_id]\n",
    "        user_vector = user_profile['vector']\n",
    "        item_vector = self.item_vectors[item_id]\n",
    "        \n",
    "        similarity = cosine_similarity([user_vector], [item_vector])[0][0]\n",
    "        \n",
    "        # 找到与推荐物品最相似的用户喜欢的物品\n",
    "        liked_similarities = []\n",
    "        for liked_id in user_profile['liked_items']:\n",
    "            liked_vector = self.item_vectors[liked_id]\n",
    "            liked_sim = cosine_similarity([item_vector], [liked_vector])[0][0]\n",
    "            liked_similarities.append((liked_id, liked_sim))\n",
    "        \n",
    "        liked_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        most_similar_liked = liked_similarities[0] if liked_similarities else None\n",
    "        \n",
    "        explanation = f\"推荐评分: {similarity:.3f}\\n\"\n",
    "        if most_similar_liked:\n",
    "            similar_doc = self.documents[most_similar_liked[0]]\n",
    "            explanation += f\"因为与您喜欢的'{similar_doc['title']}'相似 (相似度: {most_similar_liked[1]:.3f})\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# 初始化推荐系统\n",
    "rec_system = ContentRecommendationSystem(documents)\n",
    "print(\"推荐系统初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟用户行为数据\n",
    "print(\"创建模拟用户...\")\n",
    "\n",
    "# 用户1: 喜欢科技类文档\n",
    "tech_docs = [i for i, doc in enumerate(documents) if doc['category'] == '科技']\n",
    "user1_liked = tech_docs[:2] if len(tech_docs) >= 2 else tech_docs\n",
    "rec_system.create_user_profile('user1', user1_liked)\n",
    "\n",
    "# 用户2: 喜欢体育类文档\n",
    "sports_docs = [i for i, doc in enumerate(documents) if doc['category'] == '体育']\n",
    "user2_liked = sports_docs[:2] if len(sports_docs) >= 2 else sports_docs\n",
    "rec_system.create_user_profile('user2', user2_liked)\n",
    "\n",
    "# 用户3: 混合偏好\n",
    "mixed_docs = tech_docs[:1] + sports_docs[:1]\n",
    "rec_system.create_user_profile('user3', mixed_docs)\n",
    "\n",
    "print(f\"用户1 喜欢的文档: {[documents[i]['title'] for i in user1_liked]}\")\n",
    "print(f\"用户2 喜欢的文档: {[documents[i]['title'] for i in user2_liked]}\")\n",
    "print(f\"用户3 喜欢的文档: {[documents[i]['title'] for i in mixed_docs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18124542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为用户生成推荐\n",
    "def display_recommendations(user_id, recommendations):\n",
    "    \"\"\"显示推荐结果\"\"\"\n",
    "    print(f\"\\n为 {user_id} 的推荐:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, rec in enumerate(recommendations):\n",
    "        doc = rec['document']\n",
    "        print(f\"{i+1}. {doc['title']} (评分: {rec['score']:.3f})\")\n",
    "        print(f\"   类别: {doc['category']}\")\n",
    "        print(f\"   内容: {doc['content'][:60]}...\")\n",
    "        \n",
    "        # 显示推荐解释\n",
    "        explanation = rec_system.explain_recommendation(user_id, rec['item_id'])\n",
    "        print(f\"   推荐原因: {explanation.split('因为')[1] if '因为' in explanation else '基于用户偏好'}\")\n",
    "        print()\n",
    "\n",
    "# 为每个用户生成推荐\n",
    "for user_id in ['user1', 'user2', 'user3']:\n",
    "    recommendations = rec_system.recommend_items(user_id, top_k=3)\n",
    "    display_recommendations(user_id, recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b8b7f",
   "metadata": {},
   "source": [
    "## 3. 语义相似度分析\n",
    "\n",
    "分析文档之间的语义关系和聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算文档间的相似度矩阵\n",
    "print(\"计算文档相似度矩阵...\")\n",
    "similarity_matrix = cosine_similarity(rec_system.item_vectors)\n",
    "\n",
    "# 可视化相似度矩阵\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(similarity_matrix, dtype=bool))\n",
    "sns.heatmap(similarity_matrix, mask=mask, annot=False, cmap='YlOrRd', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('文档相似度矩阵')\n",
    "plt.xlabel('文档索引')\n",
    "plt.ylabel('文档索引')\n",
    "plt.show()\n",
    "\n",
    "# 找出最相似的文档对\n",
    "print(\"\\n最相似的文档对:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 去除对角线和重复的对\n",
    "n_docs = len(documents)\n",
    "similar_pairs = []\n",
    "for i in range(n_docs):\n",
    "    for j in range(i+1, n_docs):\n",
    "        similar_pairs.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "# 排序并显示前5对\n",
    "similar_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "for i, (doc1_idx, doc2_idx, sim) in enumerate(similar_pairs[:5]):\n",
    "    doc1 = documents[doc1_idx]\n",
    "    doc2 = documents[doc2_idx]\n",
    "    print(f\"{i+1}. 相似度: {sim:.3f}\")\n",
    "    print(f\"   文档1: {doc1['title']} [{doc1['category']}]\")\n",
    "    print(f\"   文档2: {doc2['title']} [{doc2['category']}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别内部和类别间的相似度分析\n",
    "def analyze_category_similarities():\n",
    "    \"\"\"分析类别间的相似度\"\"\"\n",
    "    categories = [doc['category'] for doc in documents]\n",
    "    unique_categories = list(set(categories))\n",
    "    \n",
    "    intra_category_sims = defaultdict(list)  # 类别内相似度\n",
    "    inter_category_sims = defaultdict(list)  # 类别间相似度\n",
    "    \n",
    "    for i in range(len(documents)):\n",
    "        for j in range(i+1, len(documents)):\n",
    "            sim = similarity_matrix[i, j]\n",
    "            cat1, cat2 = categories[i], categories[j]\n",
    "            \n",
    "            if cat1 == cat2:\n",
    "                intra_category_sims[cat1].append(sim)\n",
    "            else:\n",
    "                pair_key = tuple(sorted([cat1, cat2]))\n",
    "                inter_category_sims[pair_key].append(sim)\n",
    "    \n",
    "    # 显示类别内相似度\n",
    "    print(\"类别内平均相似度:\")\n",
    "    print(\"-\" * 30)\n",
    "    for cat in unique_categories:\n",
    "        if cat in intra_category_sims:\n",
    "            avg_sim = np.mean(intra_category_sims[cat])\n",
    "            print(f\"{cat}: {avg_sim:.3f} (样本数: {len(intra_category_sims[cat])})\")\n",
    "    \n",
    "    # 显示类别间相似度\n",
    "    print(\"\\n类别间平均相似度:\")\n",
    "    print(\"-\" * 30)\n",
    "    for pair, sims in inter_category_sims.items():\n",
    "        avg_sim = np.mean(sims)\n",
    "        print(f\"{pair[0]} vs {pair[1]}: {avg_sim:.3f} (样本数: {len(sims)})\")\n",
    "    \n",
    "    return intra_category_sims, inter_category_sims\n",
    "\n",
    "intra_sims, inter_sims = analyze_category_similarities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f09886",
   "metadata": {},
   "source": [
    "## 4. 搜索系统评估\n",
    "\n",
    "评估不同搜索方法的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEvaluator:\n",
    "    \"\"\"搜索系统评估器\"\"\"\n",
    "    \n",
    "    def __init__(self, search_system, documents):\n",
    "        self.search_system = search_system\n",
    "        self.documents = documents\n",
    "    \n",
    "    def create_test_queries(self):\n",
    "        \"\"\"创建测试查询及其相关文档\"\"\"\n",
    "        # 基于文档类别创建查询和期望结果\n",
    "        test_cases = [\n",
    "            {\n",
    "                'query': '人工智能机器学习',\n",
    "                'relevant_categories': ['科技'],\n",
    "                'relevant_keywords': ['人工智能', '机器学习', '算法', '技术']\n",
    "            },\n",
    "            {\n",
    "                'query': '体育运动健身',\n",
    "                'relevant_categories': ['体育'],\n",
    "                'relevant_keywords': ['体育', '运动', '健身', '锻炼']\n",
    "            },\n",
    "            {\n",
    "                'query': '艺术创作文化',\n",
    "                'relevant_categories': ['艺术'],\n",
    "                'relevant_keywords': ['艺术', '创作', '文化', '音乐']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # 为每个测试用例找到相关文档\n",
    "        for test_case in test_cases:\n",
    "            relevant_docs = []\n",
    "            for i, doc in enumerate(self.documents):\n",
    "                # 基于类别匹配\n",
    "                if doc['category'] in test_case['relevant_categories']:\n",
    "                    relevant_docs.append(i)\n",
    "                # 基于关键词匹配\n",
    "                else:\n",
    "                    content_lower = doc['content'].lower()\n",
    "                    if any(keyword in content_lower for keyword in test_case['relevant_keywords']):\n",
    "                        relevant_docs.append(i)\n",
    "            \n",
    "            test_case['relevant_docs'] = relevant_docs\n",
    "        \n",
    "        return test_cases\n",
    "    \n",
    "    def calculate_metrics(self, retrieved_docs, relevant_docs, k=5):\n",
    "        \"\"\"计算评估指标\"\"\"\n",
    "        retrieved_set = set(retrieved_docs[:k])\n",
    "        relevant_set = set(relevant_docs)\n",
    "        \n",
    "        # 计算交集\n",
    "        intersection = retrieved_set.intersection(relevant_set)\n",
    "        \n",
    "        # Precision@K: 检索到的文档中相关文档的比例\n",
    "        precision = len(intersection) / len(retrieved_set) if retrieved_set else 0\n",
    "        \n",
    "        # Recall@K: 相关文档中被检索到的比例\n",
    "        recall = len(intersection) / len(relevant_set) if relevant_set else 0\n",
    "        \n",
    "        # F1-Score\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'relevant_retrieved': len(intersection)\n",
    "        }\n",
    "    \n",
    "    def evaluate_search_methods(self, test_cases, k=5):\n",
    "        \"\"\"评估不同搜索方法\"\"\"\n",
    "        methods = {\n",
    "            '关键词搜索': self.search_system.keyword_search,\n",
    "            '语义搜索': self.search_system.semantic_search,\n",
    "            '混合搜索': self.search_system.hybrid_search\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for method_name, search_func in methods.items():\n",
    "            method_metrics = []\n",
    "            \n",
    "            for test_case in test_cases:\n",
    "                query = test_case['query']\n",
    "                relevant_docs = test_case['relevant_docs']\n",
    "                \n",
    "                # 执行搜索\n",
    "                search_results = search_func(query, top_k=k)\n",
    "                retrieved_docs = []\n",
    "                \n",
    "                for result in search_results:\n",
    "                    # 找到文档在原始列表中的索引\n",
    "                    doc_id = result['document']['id']\n",
    "                    doc_idx = next((i for i, d in enumerate(self.documents) if d['id'] == doc_id), -1)\n",
    "                    if doc_idx != -1:\n",
    "                        retrieved_docs.append(doc_idx)\n",
    "                \n",
    "                # 计算指标\n",
    "                metrics = self.calculate_metrics(retrieved_docs, relevant_docs, k)\n",
    "                metrics['query'] = query\n",
    "                method_metrics.append(metrics)\n",
    "            \n",
    "            results[method_name] = method_metrics\n",
    "        \n",
    "        return results\n",
    "\n",
    "# 初始化评估器\n",
    "evaluator = SearchEvaluator(search_system, documents)\n",
    "test_cases = evaluator.create_test_queries()\n",
    "\n",
    "print(\"创建的测试用例:\")\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"{i+1}. 查询: '{test_case['query']}'\")\n",
    "    print(f\"   相关文档数: {len(test_case['relevant_docs'])}\")\n",
    "    print(f\"   相关类别: {test_case['relevant_categories']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06836e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行评估\n",
    "print(\"执行搜索系统评估...\")\n",
    "evaluation_results = evaluator.evaluate_search_methods(test_cases, k=5)\n",
    "\n",
    "# 显示评估结果\n",
    "print(\"\\n评估结果:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method_name, method_results in evaluation_results.items():\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    all_precision = [r['precision'] for r in method_results]\n",
    "    all_recall = [r['recall'] for r in method_results]\n",
    "    all_f1 = [r['f1'] for r in method_results]\n",
    "    \n",
    "    print(f\"平均 Precision@5: {np.mean(all_precision):.3f}\")\n",
    "    print(f\"平均 Recall@5: {np.mean(all_recall):.3f}\")\n",
    "    print(f\"平均 F1-Score: {np.mean(all_f1):.3f}\")\n",
    "    \n",
    "    # 详细结果\n",
    "    for result in method_results:\n",
    "        print(f\"  查询: '{result['query']}' - P:{result['precision']:.2f} R:{result['recall']:.2f} F1:{result['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd176c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化评估结果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "metrics = ['precision', 'recall', 'f1']\n",
    "metric_names = ['Precision@5', 'Recall@5', 'F1-Score']\n",
    "\n",
    "for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "    method_names = list(evaluation_results.keys())\n",
    "    method_scores = []\n",
    "    \n",
    "    for method_name in method_names:\n",
    "        scores = [r[metric] for r in evaluation_results[method_name]]\n",
    "        method_scores.append(np.mean(scores))\n",
    "    \n",
    "    bars = axes[i].bar(method_names, method_scores, \n",
    "                      color=['blue', 'orange', 'green'], alpha=0.7)\n",
    "    axes[i].set_title(f'{metric_name} 比较')\n",
    "    axes[i].set_ylabel(metric_name)\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar, score in zip(bars, method_scores):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a1a5e",
   "metadata": {},
   "source": [
    "## 5. 性能优化实验\n",
    "\n",
    "测试不同参数和方法对性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试混合搜索中不同alpha值的影响\n",
    "print(\"测试混合搜索参数优化...\")\n",
    "\n",
    "alpha_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "alpha_results = []\n",
    "\n",
    "test_query = test_cases[0]['query']\n",
    "relevant_docs = test_cases[0]['relevant_docs']\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # 修改混合搜索的alpha参数\n",
    "    hybrid_results = search_system.hybrid_search(test_query, top_k=5, alpha=alpha)\n",
    "    \n",
    "    # 提取文档索引\n",
    "    retrieved_docs = []\n",
    "    for result in hybrid_results:\n",
    "        doc_id = result['document']['id']\n",
    "        doc_idx = next((i for i, d in enumerate(documents) if d['id'] == doc_id), -1)\n",
    "        if doc_idx != -1:\n",
    "            retrieved_docs.append(doc_idx)\n",
    "    \n",
    "    # 计算指标\n",
    "    metrics = evaluator.calculate_metrics(retrieved_docs, relevant_docs, 5)\n",
    "    metrics['alpha'] = alpha\n",
    "    alpha_results.append(metrics)\n",
    "\n",
    "# 可视化alpha值的影响\n",
    "alphas = [r['alpha'] for r in alpha_results]\n",
    "precisions = [r['precision'] for r in alpha_results]\n",
    "recalls = [r['recall'] for r in alpha_results]\n",
    "f1s = [r['f1'] for r in alpha_results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, precisions, 'o-', label='Precision', linewidth=2)\n",
    "plt.plot(alphas, recalls, 's-', label='Recall', linewidth=2)\n",
    "plt.plot(alphas, f1s, '^-', label='F1-Score', linewidth=2)\n",
    "plt.xlabel('Alpha值 (关键词搜索权重)')\n",
    "plt.ylabel('评估指标')\n",
    "plt.title('混合搜索Alpha参数对性能的影响')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 找到最佳alpha值\n",
    "best_alpha_idx = np.argmax(f1s)\n",
    "best_alpha = alphas[best_alpha_idx]\n",
    "print(f\"\\n最佳Alpha值: {best_alpha} (F1-Score: {f1s[best_alpha_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182bb9b9",
   "metadata": {},
   "source": [
    "## 6. 总结和应用建议\n",
    "\n",
    "基于实验结果的应用建议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"向量搜索应用总结:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 分析评估结果\n",
    "method_performance = {}\n",
    "for method_name, method_results in evaluation_results.items():\n",
    "    avg_f1 = np.mean([r['f1'] for r in method_results])\n",
    "    method_performance[method_name] = avg_f1\n",
    "\n",
    "best_method = max(method_performance, key=method_performance.get)\n",
    "print(f\"\\n最佳搜索方法: {best_method} (平均F1: {method_performance[best_method]:.3f})\")\n",
    "\n",
    "print(\"\\n应用场景建议:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "scenarios = {\n",
    "    \"关键词精确匹配\": {\n",
    "        \"推荐方法\": \"TF-IDF + 关键词搜索\",\n",
    "        \"适用场景\": \"法律文档检索、产品规格查询\",\n",
    "        \"优点\": \"快速、精确匹配关键词\",\n",
    "        \"缺点\": \"无法理解语义关系\"\n",
    "    },\n",
    "    \"语义理解搜索\": {\n",
    "        \"推荐方法\": \"Sentence Transformers + FAISS\",\n",
    "        \"适用场景\": \"智能客服、知识问答\",\n",
    "        \"优点\": \"理解语义，召回相关内容\",\n",
    "        \"缺点\": \"计算资源需求大\"\n",
    "    },\n",
    "    \"综合搜索系统\": {\n",
    "        \"推荐方法\": \"混合搜索 (最佳Alpha值)\",\n",
    "        \"适用场景\": \"通用搜索引擎、内容推荐\",\n",
    "        \"优点\": \"平衡精确性和语义理解\",\n",
    "        \"缺点\": \"参数调优复杂\"\n",
    "    },\n",
    "    \"大规模推荐\": {\n",
    "        \"推荐方法\": \"FAISS + 用户画像\",\n",
    "        \"适用场景\": \"电商推荐、内容推荐\",\n",
    "        \"优点\": \"可扩展、个性化\",\n",
    "        \"缺点\": \"冷启动问题\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for scenario, details in scenarios.items():\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\\n技术选型决策树:\")\n",
    "print(\"-\" * 40)\n",
    "decision_tree = [\n",
    "    \"1. 数据规模 < 10K？ → 使用基础向量搜索\",\n",
    "    \"2. 需要精确关键词匹配？ → 使用TF-IDF\",\n",
    "    \"3. 需要语义理解？ → 使用Sentence Transformers\",\n",
    "    \"4. 实时性要求高？ → 使用FAISS加速\",\n",
    "    \"5. 个性化需求？ → 构建用户画像系统\",\n",
    "    \"6. 效果要求最高？ → 使用混合搜索并调优参数\"\n",
    "]\n",
    "\n",
    "for decision in decision_tree:\n",
    "    print(f\"  {decision}\")\n",
    "\n",
    "print(\"\\n实验完成！向量搜索技术已成功应用于多个实际场景。\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
