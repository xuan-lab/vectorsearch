#!/usr/bin/env python3
"""
æµ‹è¯• NLTK å¢å¼ºçš„å‘é‡æœç´¢åŠŸèƒ½
Test NLTK-Enhanced Vector Search Functionality
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from text_vectorizer import TextVectorizer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def test_tfidf_with_nltk_preprocessing():
    """æµ‹è¯• TF-IDF ä¸ NLTK é¢„å¤„ç†çš„é›†æˆ"""
    print("ğŸ” æµ‹è¯• TF-IDF ä¸ NLTK é¢„å¤„ç†é›†æˆ")
    print("=" * 50)
    
    vectorizer = TextVectorizer()
    
    # æµ‹è¯•æ–‡æ¡£é›†åˆï¼ˆåŒ…å«ä¸­è‹±æ–‡ï¼‰
    documents = [
        "äººå·¥æ™ºèƒ½ (AI) æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œæ™ºèƒ½ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "Machine learning is a subset of artificial intelligence that enables computers to learn from data.",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å’Œåˆ†æå¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚",
        "Natural language processing helps computers understand and generate human language.",
        "æ•°æ®ç§‘å­¦ç»“åˆäº†ç»Ÿè®¡å­¦ã€ç¼–ç¨‹å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†æ¥ä»æ•°æ®ä¸­æå–æ´å¯Ÿã€‚",
        "Computer vision enables machines to interpret and understand visual information from images.",
        "Python æ˜¯æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸæœ€å—æ¬¢è¿çš„ç¼–ç¨‹è¯­è¨€ä¹‹ä¸€ã€‚",
        "Deep learning algorithms require large amounts of data and computational resources to train effectively."
    ]
    
    print(f"ğŸ“š æµ‹è¯•æ–‡æ¡£æ•°é‡: {len(documents)}")
    print("\næ–‡æ¡£å†…å®¹:")
    for i, doc in enumerate(documents, 1):
        print(f"{i:2d}. {doc}")
    
    # è¿›è¡Œ TF-IDF å‘é‡åŒ–
    print("\nğŸ”„ æ­£åœ¨è¿›è¡Œ TF-IDF å‘é‡åŒ–...")
    try:
        tfidf_matrix = vectorizer.tfidf_vectorize(documents, max_features=500)
        print(f"âœ… å‘é‡åŒ–æˆåŠŸ: {tfidf_matrix.shape}")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "äººå·¥æ™ºèƒ½",               # ä¸­æ–‡æŸ¥è¯¢
            "machine learning",       # è‹±æ–‡æŸ¥è¯¢  
            "æ·±åº¦å­¦ä¹  neural networks", # ä¸­è‹±æ··åˆæŸ¥è¯¢
            "æ•°æ®ç§‘å­¦ Python",        # ä¸­è‹±æ··åˆæŸ¥è¯¢
            "computer vision algorithms" # è‹±æ–‡æŸ¥è¯¢
        ]
        
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ ({len(test_queries)} ä¸ª):")
        
        for i, query in enumerate(test_queries, 1):
            print(f"\næŸ¥è¯¢ {i}: '{query}'")
            print("-" * 40)
            
            try:
                # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
                query_vector = vectorizer.tfidf_transform_query(query)
                print(f"âœ… æŸ¥è¯¢å‘é‡åŒ–æˆåŠŸ: ç‰¹å¾æ•° {len(query_vector)}")
                
                # è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦
                similarities = cosine_similarity([query_vector], tfidf_matrix)[0]
                
                # è·å–å‰3ä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£
                top_indices = np.argsort(similarities)[::-1][:3]
                
                print("ğŸ“Š æœ€ç›¸å…³çš„æ–‡æ¡£:")
                for rank, idx in enumerate(top_indices, 1):
                    sim_score = similarities[idx]
                    doc_preview = documents[idx][:60] + "..." if len(documents[idx]) > 60 else documents[idx]
                    print(f"  {rank}. ç›¸ä¼¼åº¦: {sim_score:.3f} | {doc_preview}")
                    
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
        return False

def test_preprocessing_quality():
    """æµ‹è¯•é¢„å¤„ç†è´¨é‡"""
    print("\n\nğŸ§¹ æµ‹è¯•é¢„å¤„ç†è´¨é‡")
    print("=" * 50)
    
    vectorizer = TextVectorizer()
    
    test_cases = [
        {
            "type": "ä¸­æ–‡æ–‡æœ¬",
            "text": "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æ˜¯ç°ä»£ç§‘æŠ€çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼å®ƒä»¬æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»ã€‚",
            "expected_features": ["ä¿ç•™ä¸­æ–‡å­—ç¬¦", "ç§»é™¤æ ‡ç‚¹ç¬¦å·"]
        },
        {
            "type": "è‹±æ–‡æ–‡æœ¬", 
            "text": "Machine learning algorithms are powerful tools for data analysis and prediction.",
            "expected_features": ["ç§»é™¤åœç”¨è¯", "è¯å¹²æå–", "ä¿ç•™å…³é”®è¯"]
        },
        {
            "type": "ä¸­è‹±æ··åˆ",
            "text": "Pythonæ˜¯ä¸€ç§æµè¡Œçš„programming languageï¼Œç‰¹åˆ«é€‚åˆdata scienceå’ŒAIå¼€å‘ã€‚",
            "expected_features": ["ä¿ç•™ä¸­è‹±æ–‡", "æ··åˆè¯­è¨€å¤„ç†"]
        },
        {
            "type": "æŠ€æœ¯æœ¯è¯­",
            "text": "TensorFlow, PyTorch, and scikit-learn are popular machine learning libraries.",
            "expected_features": ["ä¿ç•™æŠ€æœ¯æœ¯è¯­", "å¤„ç†ç‰¹æ®Šå­—ç¬¦"]
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}: {case['type']}")
        print(f"åŸæ–‡: {case['text']}")
        
        processed = vectorizer.preprocess_text(case['text'])
        print(f"å¤„ç†å: {processed}")
        
        # ç®€å•çš„è´¨é‡æ£€æŸ¥
        original_chinese_chars = len([c for c in case['text'] if '\u4e00' <= c <= '\u9fff'])
        processed_chinese_chars = len([c for c in processed if '\u4e00' <= c <= '\u9fff'])
        
        if original_chinese_chars > 0:
            if processed_chinese_chars >= original_chinese_chars * 0.8:  # å…è®¸ä¸€äº›æŸå¤±
                print("âœ… ä¸­æ–‡å­—ç¬¦ä¿ç•™è‰¯å¥½")
            else:
                print("âš ï¸  ä¸­æ–‡å­—ç¬¦å¯èƒ½ä¸¢å¤±è¿‡å¤š")
        
        if len(processed.strip()) > 0:
            print("âœ… å¤„ç†åæ–‡æœ¬éç©º")
        else:
            print("âŒ å¤„ç†åæ–‡æœ¬ä¸ºç©º!")
            
        print(f"æœŸæœ›ç‰¹æ€§: {', '.join(case['expected_features'])}")

def test_vocabulary_quality():
    """æµ‹è¯•è¯æ±‡è¡¨è´¨é‡"""
    print("\n\nğŸ“ æµ‹è¯•è¯æ±‡è¡¨è´¨é‡")
    print("=" * 50)
    
    vectorizer = TextVectorizer()
    
    # åˆ›å»ºä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–å†…å®¹çš„æ–‡æ¡£é›†
    diverse_documents = [
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯",
        "Machine learning and deep learning are core AI technologies", 
        "Pythonå’ŒRæ˜¯æ•°æ®ç§‘å­¦ä¸­æœ€å¸¸ç”¨çš„ç¼–ç¨‹è¯­è¨€",
        "Data scientists use Python and R for statistical analysis",
        "ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œåŸç†æ¥å¤„ç†ä¿¡æ¯",
        "Neural networks mimic the human brain to process information",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€",
        "Natural language processing helps computers understand human text"
    ]
    
    print(f"ğŸ“š ä½¿ç”¨ {len(diverse_documents)} ä¸ªæ–‡æ¡£å»ºç«‹è¯æ±‡è¡¨")
    
    try:
        # è¿›è¡Œå‘é‡åŒ–ä»¥å»ºç«‹è¯æ±‡è¡¨
        tfidf_matrix = vectorizer.tfidf_vectorize(diverse_documents, max_features=200)
        
        # è·å–ç‰¹å¾åç§°ï¼ˆè¯æ±‡è¡¨ï¼‰
        if hasattr(vectorizer.tfidf_vectorizer, 'get_feature_names_out'):
            vocabulary = vectorizer.tfidf_vectorizer.get_feature_names_out()
        else:
            vocabulary = vectorizer.tfidf_vectorizer.get_feature_names()
        
        print(f"âœ… è¯æ±‡è¡¨å¤§å°: {len(vocabulary)}")
        
        # åˆ†æè¯æ±‡è¡¨è´¨é‡
        chinese_terms = [term for term in vocabulary if any('\u4e00' <= c <= '\u9fff' for c in term)]
        english_terms = [term for term in vocabulary if term.isalpha() and not any('\u4e00' <= c <= '\u9fff' for c in term)]
        mixed_terms = [term for term in vocabulary if not term in chinese_terms and not term in english_terms]
        
        print(f"ğŸ“Š è¯æ±‡åˆ†å¸ƒ:")
        print(f"  ä¸­æ–‡è¯æ±‡: {len(chinese_terms)} ä¸ª")
        print(f"  è‹±æ–‡è¯æ±‡: {len(english_terms)} ä¸ª") 
        print(f"  å…¶ä»–è¯æ±‡: {len(mixed_terms)} ä¸ª")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹è¯æ±‡
        print(f"\nğŸ“ ä¸­æ–‡è¯æ±‡ç¤ºä¾‹ (å‰10ä¸ª):")
        for term in chinese_terms[:10]:
            print(f"  {term}")
            
        print(f"\nğŸ“ è‹±æ–‡è¯æ±‡ç¤ºä¾‹ (å‰10ä¸ª):")
        for term in english_terms[:10]:
            print(f"  {term}")
            
        if len(chinese_terms) > 0 and len(english_terms) > 0:
            print("âœ… è¯æ±‡è¡¨åŒæ—¶åŒ…å«ä¸­è‹±æ–‡è¯æ±‡ï¼Œå¤šè¯­è¨€æ”¯æŒè‰¯å¥½")
        else:
            print("âš ï¸  è¯æ±‡è¡¨å¯èƒ½ç¼ºå°‘æŸç§è¯­è¨€çš„è¯æ±‡")
            
        return True
        
    except Exception as e:
        print(f"âŒ è¯æ±‡è¡¨åˆ†æå¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ NLTK å¢å¼ºå‘é‡æœç´¢åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥ NLTK çŠ¶æ€
    try:
        import nltk
        from nltk.tokenize import word_tokenize
        from nltk.corpus import stopwords
        from nltk.stem import PorterStemmer
        print("âœ… NLTK åº“å·²å®‰è£…å¹¶å¯ç”¨")
    except ImportError as e:
        print(f"âŒ NLTK åº“ä¸å¯ç”¨: {e}")
        print("ğŸ’¡ å°†ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†åŠŸèƒ½")
    
    print()
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    results = []
    
    # æµ‹è¯•1: TF-IDF ä¸ NLTK é›†æˆ
    results.append(test_tfidf_with_nltk_preprocessing())
    
    # æµ‹è¯•2: é¢„å¤„ç†è´¨é‡
    test_preprocessing_quality()
    
    # æµ‹è¯•3: è¯æ±‡è¡¨è´¨é‡ 
    results.append(test_vocabulary_quality())
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    passed_tests = sum(results)
    total_tests = len(results)
    
    print(f"âœ… é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        print("ğŸ’¡ NLTK å¢å¼ºçš„å‘é‡æœç´¢åŠŸèƒ½å·²å‡†å¤‡å°±ç»ª")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
    
    print("\nğŸ”§ åŠŸèƒ½äº®ç‚¹:")
    print("â€¢ è¯­è¨€æ„ŸçŸ¥çš„æ–‡æœ¬é¢„å¤„ç†")
    print("â€¢ ä¸­è‹±æ–‡æ··åˆæ–‡æ¡£æ”¯æŒ") 
    print("â€¢ NLTK åœç”¨è¯è¿‡æ»¤å’Œè¯å¹²æå–")
    print("â€¢ é«˜è´¨é‡çš„ TF-IDF å‘é‡åŒ–")
    print("â€¢ å¤šè¯­è¨€è¯­ä¹‰æœç´¢èƒ½åŠ›")

if __name__ == "__main__":
    main()
