#!/usr/bin/env python3
"""
æµ‹è¯•è¯­è¨€æ„ŸçŸ¥çš„æ–‡æœ¬é¢„å¤„ç†åŠŸèƒ½
Test Language-Aware Text Preprocessing
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from text_vectorizer import TextVectorizer

def test_chinese_text_preprocessing():
    """æµ‹è¯•ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†"""
    print("=== æµ‹è¯•ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç† ===")
    
    vectorizer = TextVectorizer()
    
    # æµ‹è¯•ä¸­æ–‡æ–‡æ¡£
    chinese_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºæ…§çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ã€‚",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å’Œåˆ†ææ•°æ®ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
        "æ•°æ®ç§‘å­¦ç»“åˆäº†ç»Ÿè®¡å­¦ã€ç¼–ç¨‹å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚"
    ]
    
    print("åŸå§‹ä¸­æ–‡æ–‡æœ¬:")
    for i, text in enumerate(chinese_texts, 1):
        print(f"{i}. {text}")
    
    print("\né¢„å¤„ç†åçš„ä¸­æ–‡æ–‡æœ¬:")
    for i, text in enumerate(chinese_texts, 1):
        processed = vectorizer.preprocess_text(text)
        print(f"{i}. {processed}")
        
        # æ£€æŸ¥æ˜¯å¦ä¿ç•™äº†ä¸­æ–‡å­—ç¬¦
        chinese_chars_original = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        chinese_chars_processed = len([c for c in processed if '\u4e00' <= c <= '\u9fff'])
        
        print(f"   åŸå§‹ä¸­æ–‡å­—ç¬¦æ•°: {chinese_chars_original}, å¤„ç†å: {chinese_chars_processed}")
        
        if chinese_chars_processed == 0 and chinese_chars_original > 0:
            print(f"   âŒ é”™è¯¯: ä¸­æ–‡å­—ç¬¦è¢«å®Œå…¨ç§»é™¤!")
        elif chinese_chars_processed > 0:
            print(f"   âœ… æ­£ç¡®: ä¸­æ–‡å­—ç¬¦å¾—åˆ°ä¿ç•™")
    
    return chinese_texts

def test_english_text_preprocessing():
    """æµ‹è¯•è‹±æ–‡æ–‡æœ¬é¢„å¤„ç†"""
    print("\n=== æµ‹è¯•è‹±æ–‡æ–‡æœ¬é¢„å¤„ç† ===")
    
    vectorizer = TextVectorizer()
    
    # æµ‹è¯•è‹±æ–‡æ–‡æ¡£
    english_texts = [
        "Machine learning is a subset of artificial intelligence that focuses on algorithms.",
        "Deep learning uses neural networks with multiple layers to process data.",
        "Natural language processing helps computers understand human language.",
        "Computer vision enables machines to interpret and analyze visual information.",
        "Data science combines statistics, programming, and domain expertise."
    ]
    
    print("åŸå§‹è‹±æ–‡æ–‡æœ¬:")
    for i, text in enumerate(english_texts, 1):
        print(f"{i}. {text}")
    
    print("\né¢„å¤„ç†åçš„è‹±æ–‡æ–‡æœ¬:")
    for i, text in enumerate(english_texts, 1):
        processed = vectorizer.preprocess_text(text)
        print(f"{i}. {processed}")
        
        # æ£€æŸ¥æ˜¯å¦æ­£ç¡®å¤„ç†äº†åœç”¨è¯å’Œè¯å¹²æå–
        original_words = text.lower().split()
        processed_words = processed.split()
        
        print(f"   åŸå§‹å•è¯æ•°: {len(original_words)}, å¤„ç†å: {len(processed_words)}")
        
        if len(processed_words) < len(original_words):
            print(f"   âœ… æ­£ç¡®: åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·è¢«ç§»é™¤")
        else:
            print(f"   âš ï¸  æ³¨æ„: å¯èƒ½æœªæ­£ç¡®ç§»é™¤åœç”¨è¯")
    
    return english_texts

def test_mixed_text_preprocessing():
    """æµ‹è¯•ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬é¢„å¤„ç†"""
    print("\n=== æµ‹è¯•ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬é¢„å¤„ç† ===")
    
    vectorizer = TextVectorizer()
    
    # æµ‹è¯•ä¸­è‹±æ–‡æ··åˆæ–‡æ¡£
    mixed_texts = [
        "äººå·¥æ™ºèƒ½ (Artificial Intelligence, AI) æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸã€‚",
        "Machine Learning æœºå™¨å­¦ä¹ åœ¨å„ä¸ªè¡Œä¸šéƒ½æœ‰åº”ç”¨ã€‚",
        "Python æ˜¯æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ çš„çƒ­é—¨ç¼–ç¨‹è¯­è¨€ã€‚",
        "æ·±åº¦å­¦ä¹  Deep Learning ä½¿ç”¨ç¥ç»ç½‘ç»œ Neural Networksã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç† NLP å¤„ç†æ–‡æœ¬æ•°æ®ã€‚"
    ]
    
    print("åŸå§‹æ··åˆæ–‡æœ¬:")
    for i, text in enumerate(mixed_texts, 1):
        print(f"{i}. {text}")
    
    print("\né¢„å¤„ç†åçš„æ··åˆæ–‡æœ¬:")
    for i, text in enumerate(mixed_texts, 1):
        processed = vectorizer.preprocess_text(text)
        print(f"{i}. {processed}")
        
        # æ£€æŸ¥ä¸­è‹±æ–‡å­—ç¬¦ä¿ç•™æƒ…å†µ
        chinese_chars_original = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        chinese_chars_processed = len([c for c in processed if '\u4e00' <= c <= '\u9fff'])
        english_chars_original = len([c for c in text if c.isalpha() and not ('\u4e00' <= c <= '\u9fff')])
        english_chars_processed = len([c for c in processed if c.isalpha() and not ('\u4e00' <= c <= '\u9fff')])
        
        print(f"   ä¸­æ–‡å­—ç¬¦: {chinese_chars_original} â†’ {chinese_chars_processed}")
        print(f"   è‹±æ–‡å­—ç¬¦: {english_chars_original} â†’ {english_chars_processed}")
        
        if chinese_chars_processed > 0 and english_chars_processed > 0:
            print(f"   âœ… æ­£ç¡®: ä¸­è‹±æ–‡å­—ç¬¦éƒ½å¾—åˆ°ä¿ç•™")
        else:
            print(f"   âš ï¸  æ³¨æ„: æŸäº›å­—ç¬¦å¯èƒ½ä¸¢å¤±")
    
    return mixed_texts

def test_vectorization_with_new_preprocessing():
    """æµ‹è¯•æ–°é¢„å¤„ç†é€»è¾‘çš„å‘é‡åŒ–"""
    print("\n=== æµ‹è¯•å‘é‡åŒ–åŠŸèƒ½ ===")
    
    vectorizer = TextVectorizer()
    
    # ä½¿ç”¨ä¹‹å‰æµ‹è¯•çš„æ–‡æœ¬
    test_documents = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
        "Machine learning focuses on algorithms and statistical models.",
        "æ·±åº¦å­¦ä¹  Deep Learning ä½¿ç”¨ç¥ç»ç½‘ç»œã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€ã€‚",
        "Data science combines statistics and programming."
    ]
    
    print("æµ‹è¯•æ–‡æ¡£:")
    for i, doc in enumerate(test_documents, 1):
        print(f"{i}. {doc}")
    
    try:
        print("\næ­£åœ¨è¿›è¡Œ TF-IDF å‘é‡åŒ–...")
        tfidf_vectors = vectorizer.tfidf_vectorize(test_documents)
        print(f"âœ… TF-IDF å‘é‡åŒ–æˆåŠŸ: {tfidf_vectors.shape}")
        
        # æµ‹è¯•æŸ¥è¯¢è½¬æ¢
        test_queries = [
            "äººå·¥æ™ºèƒ½",
            "machine learning",
            "æ·±åº¦å­¦ä¹  neural networks"
        ]
        
        print("\næµ‹è¯•æŸ¥è¯¢å‘é‡åŒ–:")
        for query in test_queries:
            try:
                query_vector = vectorizer.tfidf_transform_query(query)
                print(f"âœ… æŸ¥è¯¢ '{query}' å‘é‡åŒ–æˆåŠŸ: {query_vector.shape}")
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢ '{query}' å‘é‡åŒ–å¤±è´¥: {e}")
                
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
    
    return test_documents

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ” NLTK è¯­è¨€æ„ŸçŸ¥é¢„å¤„ç†æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥ NLTK å¯ç”¨æ€§
    try:
        import nltk
        from nltk.tokenize import word_tokenize
        print("âœ… NLTK å·²å®‰è£…å¹¶å¯ç”¨")
    except ImportError:
        print("âŒ NLTK æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€é¢„å¤„ç†")
    
    try:
        # æµ‹è¯•å„ç§æ–‡æœ¬ç±»å‹çš„é¢„å¤„ç†
        chinese_texts = test_chinese_text_preprocessing()
        english_texts = test_english_text_preprocessing() 
        mixed_texts = test_mixed_text_preprocessing()
        
        # æµ‹è¯•å‘é‡åŒ–åŠŸèƒ½
        test_documents = test_vectorization_with_new_preprocessing()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"- ä¸­æ–‡æ–‡æ¡£æ•°é‡: {len(chinese_texts)}")
        print(f"- è‹±æ–‡æ–‡æ¡£æ•°é‡: {len(english_texts)}")
        print(f"- æ··åˆæ–‡æ¡£æ•°é‡: {len(mixed_texts)}")
        print(f"- å‘é‡åŒ–æµ‹è¯•æ–‡æ¡£æ•°é‡: {len(test_documents)}")
        
        print("\nâœ¨ è¯­è¨€æ„ŸçŸ¥é¢„å¤„ç†åŠŸèƒ½å·²å‡†å¤‡å°±ç»ª!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
